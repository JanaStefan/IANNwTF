{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64, 784), dtype=float32, numpy=\n",
      "array([[-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       ...,\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.]], dtype=float32)>, <tf.Tensor: shape=(64, 10), dtype=float32, numpy=\n",
      "array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.]], dtype=float32)>)\n",
      "(<tf.Tensor: shape=(64, 784), dtype=float32, numpy=\n",
      "array([[-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       ...,\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.]], dtype=float32)>, <tf.Tensor: shape=(64, 10), dtype=float32, numpy=\n",
      "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 12:48:25.466352: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n",
      "2023-11-21 12:48:25.505396: W tensorflow/core/kernels/data/cache_dataset_ops.cc:858] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1,\n",
      " Train Loss: 0.3582116663455963, Train Accuracy: 0.8881666666666667,\n",
      " Test Loss: 0.1517125517129898, Test Accuracy: 0.9541\n",
      "Epoch 2,\n",
      " Train Loss: 0.1385299563407898, Train Accuracy: 0.95675,\n",
      " Test Loss: 0.14895199239253998, Test Accuracy: 0.954\n",
      "Epoch 3,\n",
      " Train Loss: 0.10114321112632751, Train Accuracy: 0.9688333333333333,\n",
      " Test Loss: 0.19664208590984344, Test Accuracy: 0.9322\n",
      "Epoch 4,\n",
      " Train Loss: 0.08119477331638336, Train Accuracy: 0.97435,\n",
      " Test Loss: 0.13763003051280975, Test Accuracy: 0.9551\n",
      "Epoch 5,\n",
      " Train Loss: 0.06527698040008545, Train Accuracy: 0.9793666666666667,\n",
      " Test Loss: 0.13002650439739227, Test Accuracy: 0.9556\n",
      "Epoch 6,\n",
      " Train Loss: 0.054415423423051834, Train Accuracy: 0.9826666666666667,\n",
      " Test Loss: 0.08227454870939255, Test Accuracy: 0.9739\n",
      "Epoch 7,\n",
      " Train Loss: 0.04550603777170181, Train Accuracy: 0.9858333333333333,\n",
      " Test Loss: 0.07622519880533218, Test Accuracy: 0.976\n",
      "Epoch 8,\n",
      " Train Loss: 0.039359696209430695, Train Accuracy: 0.9875333333333334,\n",
      " Test Loss: 0.0661594346165657, Test Accuracy: 0.9792\n",
      "Epoch 9,\n",
      " Train Loss: 0.03266343101859093, Train Accuracy: 0.9893,\n",
      " Test Loss: 0.07158228754997253, Test Accuracy: 0.9773\n",
      "Epoch 10,\n",
      " Train Loss: 0.027311254292726517, Train Accuracy: 0.9910333333333333,\n",
      " Test Loss: 0.06959369778633118, Test Accuracy: 0.9787\n"
     ]
    }
   ],
   "source": [
    "#2.1 Loading the MNIST dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "#tfds.show_examples(train_ds , ds_info)\n",
    "\n",
    "# 2.2 Setting up the data pipeline\n",
    "\n",
    "def data_pipeline(input, batch_size=64, prefetch_buffer_size=4):\n",
    "    # Map the dataset to extract images and labels\n",
    "    input =input.map(lambda image, label: (image, label))\n",
    "    # Reshape each image to a flat vector\n",
    "    input = input.map(lambda image, label: (tf.reshape(image, (-1,)), label))\n",
    "    # Normalize(Scale) image values to be in the range [-1, 1]\n",
    "    input = input.map(lambda image, label: ((tf.cast(image, tf.float32) / 128) - 1, label))\n",
    "    # One-hot encode the labels\n",
    "    input = input.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "    # Shuffle the dataset and create batches of size 4\n",
    "    input = input.shuffle(1024).batch(batch_size)\n",
    "    # Prefetch the dataset to improve pipeline performance\n",
    "    input = input.prefetch(prefetch_buffer_size)\n",
    "    return input\n",
    "\n",
    "# Save the datasets after applying the data pipeline\n",
    "train_dataset = data_pipeline(train_ds)\n",
    "test_dataset = data_pipeline(test_ds)\n",
    "\n",
    "for elem in train_dataset.take(1):\n",
    "    print(elem)\n",
    "\n",
    "for elem in test_dataset.take(1):\n",
    "    print(elem)\n",
    "\n",
    "\n",
    "# 2.3 Building a deep neural network with TensorFlow\n",
    "class MLPModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, output_size=10):\n",
    "        super().__init__()\n",
    "        self.mlp_layers = []  # create a list to store hidden layers\n",
    "\n",
    "        # Create hidden layers with ReLU activation\n",
    "        for layer_size in layer_sizes:\n",
    "            new_layer = layers.Dense(units=layer_size, activation='relu')\n",
    "            self.mlp_layers.append(new_layer)\n",
    "        # Output layer with softmax activation for classification\n",
    "        self.output_layer = layers.Dense(units=output_size, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Forward pass through hidden layers\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            x = mlp_layer(x)\n",
    "        # Forward pass through the output layer\n",
    "        y = self.output_layer(x)\n",
    "        return y\n",
    "\n",
    "# 2.4 Training the network\n",
    "\"\"\"\n",
    "Define a training loop function which receives\n",
    "• The number of epochs\n",
    "• The model object\n",
    "• The training dataset\n",
    "• The test dataset\n",
    "• The loss function\n",
    "• The optimizer\n",
    "• Different arrays for the different values you want to track for visualization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_model(num_epochs, model, train_dataset, test_dataset, loss_function, optimizer):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []  # Track training accuracy\n",
    "    test_accuracies = []   # Track testing accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_losses = []\n",
    "        correct_train_predictions = 0  # Counter for correct training predictions\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        for x_train, target_train in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                pred_train = model(x_train)\n",
    "                # Calculate the training loss\n",
    "                loss_train = loss_function(target_train, pred_train)\n",
    "\n",
    "            # Calculate gradients\n",
    "            gradients_train = tape.gradient(loss_train, model.trainable_variables)\n",
    "\n",
    "            # Update weights using optimizer\n",
    "            optimizer.apply_gradients(zip(gradients_train, model.trainable_variables))\n",
    "\n",
    "            # Append the training loss to the list\n",
    "            epoch_train_losses.append(loss_train.numpy())\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            correct_train_predictions += np.sum(np.argmax(target_train, axis=1) == np.argmax(pred_train, axis=1))\n",
    "            total_train_samples += len(x_train)\n",
    "\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Testing phase\n",
    "        epoch_test_losses = []\n",
    "        correct_test_predictions = 0  # Counter for correct testing predictions\n",
    "        total_test_samples = 0\n",
    "\n",
    "        for x_test, target_test in test_dataset:\n",
    "            # Forward pass\n",
    "            pred_test = model(x_test)\n",
    "            # Calculate the testing loss\n",
    "            loss_test = loss_function(target_test, pred_test)\n",
    "            epoch_test_losses.append(loss_test.numpy())\n",
    "\n",
    "            # Calculate testing accuracy\n",
    "            correct_test_predictions += np.sum(np.argmax(target_test, axis=1) == np.argmax(pred_test, axis=1))\n",
    "            total_test_samples += len(x_test)\n",
    "\n",
    "        test_accuracy = correct_test_predictions / total_test_samples\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Print the mean training and testing loss and accuracy for the epoch\n",
    "        mean_train_loss = np.mean(epoch_train_losses)\n",
    "        mean_test_loss = np.mean(epoch_test_losses)\n",
    "        print(f\"Epoch {epoch + 1},\\n Train Loss: {mean_train_loss}, Train Accuracy: {train_accuracy},\\n Test Loss: {mean_test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "        # Append losses to the lists for visualization\n",
    "        train_losses.append(mean_train_loss)\n",
    "        test_losses.append(mean_test_loss)\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Example usage\n",
    "EPOCHS = 10\n",
    "model = MLPModel(layer_sizes=[256, 256], output_size=10)\n",
    "\n",
    "# Loss function and optimizer\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "sgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n",
    "\n",
    "# Assuming train_dataset and test_dataset are your TensorFlow datasets\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    EPOCHS, model, train_dataset, test_dataset, cce, sgd\n",
    ")\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T11:49:05.746023Z",
     "start_time": "2023-11-21T11:48:22.549554Z"
    }
   },
   "id": "dcad1c6457494b32"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.36190342903137207, Train Accuracy: 0.89085,\n",
      " Test Loss: 0.2055802047252655, Test Accuracy: 0.9387\n",
      "Epoch 2,\n",
      " Train Loss: 0.1822432577610016, Train Accuracy: 0.9464,\n",
      " Test Loss: 0.15766504406929016, Test Accuracy: 0.9541\n",
      "Epoch 3,\n",
      " Train Loss: 0.13292251527309418, Train Accuracy: 0.96085,\n",
      " Test Loss: 0.11811594665050507, Test Accuracy: 0.9633\n",
      "Epoch 4,\n",
      " Train Loss: 0.10496339201927185, Train Accuracy: 0.9696333333333333,\n",
      " Test Loss: 0.11345448344945908, Test Accuracy: 0.9645\n",
      "Epoch 5,\n",
      " Train Loss: 0.0873931497335434, Train Accuracy: 0.9742833333333333,\n",
      " Test Loss: 0.09564381837844849, Test Accuracy: 0.9702\n",
      "Epoch 6,\n",
      " Train Loss: 0.07416035234928131, Train Accuracy: 0.9782833333333333,\n",
      " Test Loss: 0.09924016147851944, Test Accuracy: 0.9681\n",
      "Epoch 7,\n",
      " Train Loss: 0.06378782540559769, Train Accuracy: 0.9817666666666667,\n",
      " Test Loss: 0.0811547264456749, Test Accuracy: 0.976\n",
      "Epoch 8,\n",
      " Train Loss: 0.055704616010189056, Train Accuracy: 0.9835333333333334,\n",
      " Test Loss: 0.07960943132638931, Test Accuracy: 0.9757\n",
      "Epoch 9,\n",
      " Train Loss: 0.04853159934282303, Train Accuracy: 0.9852166666666666,\n",
      " Test Loss: 0.08047161251306534, Test Accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.04289787635207176, Train Accuracy: 0.9874333333333334,\n",
      " Test Loss: 0.07407867908477783, Test Accuracy: 0.9783\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5128767490386963, Train Accuracy: 0.85665,\n",
      " Test Loss: 0.36383867263793945, Test Accuracy: 0.8926\n",
      "Epoch 2,\n",
      " Train Loss: 0.37632766366004944, Train Accuracy: 0.8922666666666667,\n",
      " Test Loss: 0.3644242286682129, Test Accuracy: 0.8982\n",
      "Epoch 3,\n",
      " Train Loss: 0.35122498869895935, Train Accuracy: 0.9015333333333333,\n",
      " Test Loss: 0.34770435094833374, Test Accuracy: 0.9066\n",
      "Epoch 4,\n",
      " Train Loss: 0.34440866112709045, Train Accuracy: 0.9038333333333334,\n",
      " Test Loss: 0.35633325576782227, Test Accuracy: 0.9044\n",
      "Epoch 5,\n",
      " Train Loss: 0.33175280690193176, Train Accuracy: 0.9091833333333333,\n",
      " Test Loss: 0.4286923110485077, Test Accuracy: 0.8898\n",
      "Epoch 6,\n",
      " Train Loss: 0.32617518305778503, Train Accuracy: 0.9113166666666667,\n",
      " Test Loss: 0.3941231071949005, Test Accuracy: 0.8989\n",
      "Epoch 7,\n",
      " Train Loss: 0.3240015208721161, Train Accuracy: 0.9119,\n",
      " Test Loss: 0.34087392687797546, Test Accuracy: 0.9096\n",
      "Epoch 8,\n",
      " Train Loss: 0.3156742751598358, Train Accuracy: 0.9148166666666666,\n",
      " Test Loss: 0.3098873198032379, Test Accuracy: 0.9208\n",
      "Epoch 9,\n",
      " Train Loss: 0.3135245144367218, Train Accuracy: 0.9155,\n",
      " Test Loss: 0.41739436984062195, Test Accuracy: 0.8882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.31371307373046875, Train Accuracy: 0.9153666666666667,\n",
      " Test Loss: 0.3535541296005249, Test Accuracy: 0.9069\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.839856743812561, Train Accuracy: 0.81815,\n",
      " Test Loss: 0.5456848740577698, Test Accuracy: 0.8622\n",
      "Epoch 2,\n",
      " Train Loss: 0.5995309948921204, Train Accuracy: 0.8678833333333333,\n",
      " Test Loss: 0.4834959805011749, Test Accuracy: 0.848\n",
      "Epoch 3,\n",
      " Train Loss: 0.6941777467727661, Train Accuracy: 0.8585166666666667,\n",
      " Test Loss: 0.6500893831253052, Test Accuracy: 0.8338\n",
      "Epoch 4,\n",
      " Train Loss: 0.8363321423530579, Train Accuracy: 0.8351666666666666,\n",
      " Test Loss: 1.5736606121063232, Test Accuracy: 0.8421\n",
      "Epoch 5,\n",
      " Train Loss: 0.8984742760658264, Train Accuracy: 0.8186,\n",
      " Test Loss: 0.8145985007286072, Test Accuracy: 0.7924\n",
      "Epoch 6,\n",
      " Train Loss: 0.9574118256568909, Train Accuracy: 0.8063166666666667,\n",
      " Test Loss: 1.3525080680847168, Test Accuracy: 0.7979\n",
      "Epoch 7,\n",
      " Train Loss: 1.0280206203460693, Train Accuracy: 0.79045,\n",
      " Test Loss: 1.376787781715393, Test Accuracy: 0.6517\n",
      "Epoch 8,\n",
      " Train Loss: 1.1263864040374756, Train Accuracy: 0.7689166666666667,\n",
      " Test Loss: 1.0801560878753662, Test Accuracy: 0.816\n",
      "Epoch 9,\n",
      " Train Loss: 1.1438568830490112, Train Accuracy: 0.7749666666666667,\n",
      " Test Loss: 1.1606378555297852, Test Accuracy: 0.8021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 1.242733359336853, Train Accuracy: 0.7244833333333334,\n",
      " Test Loss: 1.0085724592208862, Test Accuracy: 0.76\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.37604066729545593, Train Accuracy: 0.889,\n",
      " Test Loss: 0.21723979711532593, Test Accuracy: 0.9335\n",
      "Epoch 2,\n",
      " Train Loss: 0.19279305636882782, Train Accuracy: 0.94295,\n",
      " Test Loss: 0.16644276678562164, Test Accuracy: 0.948\n",
      "Epoch 3,\n",
      " Train Loss: 0.14531132578849792, Train Accuracy: 0.9575666666666667,\n",
      " Test Loss: 0.12917372584342957, Test Accuracy: 0.9604\n",
      "Epoch 4,\n",
      " Train Loss: 0.1174907311797142, Train Accuracy: 0.9651666666666666,\n",
      " Test Loss: 0.12020473927259445, Test Accuracy: 0.9628\n",
      "Epoch 5,\n",
      " Train Loss: 0.09853367507457733, Train Accuracy: 0.9709,\n",
      " Test Loss: 0.11110759526491165, Test Accuracy: 0.9655\n",
      "Epoch 6,\n",
      " Train Loss: 0.08599970489740372, Train Accuracy: 0.9739333333333333,\n",
      " Test Loss: 0.09212449938058853, Test Accuracy: 0.9708\n",
      "Epoch 7,\n",
      " Train Loss: 0.07536996901035309, Train Accuracy: 0.9776333333333334,\n",
      " Test Loss: 0.08895513415336609, Test Accuracy: 0.9704\n",
      "Epoch 8,\n",
      " Train Loss: 0.06685994565486908, Train Accuracy: 0.9801833333333333,\n",
      " Test Loss: 0.091347336769104, Test Accuracy: 0.9707\n",
      "Epoch 9,\n",
      " Train Loss: 0.05892591178417206, Train Accuracy: 0.9820333333333333,\n",
      " Test Loss: 0.08702675998210907, Test Accuracy: 0.9716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.05346805974841118, Train Accuracy: 0.9836666666666667,\n",
      " Test Loss: 0.08075536042451859, Test Accuracy: 0.9741\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.4861673414707184, Train Accuracy: 0.8561,\n",
      " Test Loss: 0.43545016646385193, Test Accuracy: 0.8683\n",
      "Epoch 2,\n",
      " Train Loss: 0.3670577108860016, Train Accuracy: 0.8941,\n",
      " Test Loss: 0.3997190296649933, Test Accuracy: 0.8882\n",
      "Epoch 3,\n",
      " Train Loss: 0.34555333852767944, Train Accuracy: 0.9031333333333333,\n",
      " Test Loss: 0.3720121383666992, Test Accuracy: 0.8947\n",
      "Epoch 4,\n",
      " Train Loss: 0.3362255394458771, Train Accuracy: 0.9062333333333333,\n",
      " Test Loss: 0.30277183651924133, Test Accuracy: 0.9117\n",
      "Epoch 5,\n",
      " Train Loss: 0.3311600387096405, Train Accuracy: 0.9071333333333333,\n",
      " Test Loss: 0.34256982803344727, Test Accuracy: 0.9042\n",
      "Epoch 6,\n",
      " Train Loss: 0.32150694727897644, Train Accuracy: 0.9112333333333333,\n",
      " Test Loss: 0.2748708724975586, Test Accuracy: 0.9248\n",
      "Epoch 7,\n",
      " Train Loss: 0.3167375326156616, Train Accuracy: 0.91275,\n",
      " Test Loss: 0.3318602740764618, Test Accuracy: 0.9155\n",
      "Epoch 8,\n",
      " Train Loss: 0.3152470886707306, Train Accuracy: 0.9132666666666667,\n",
      " Test Loss: 0.30997928977012634, Test Accuracy: 0.922\n",
      "Epoch 9,\n",
      " Train Loss: 0.3146984279155731, Train Accuracy: 0.9146166666666666,\n",
      " Test Loss: 0.2960651218891144, Test Accuracy: 0.9203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.3090272843837738, Train Accuracy: 0.9162666666666667,\n",
      " Test Loss: 0.3433346450328827, Test Accuracy: 0.9148\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.7455576062202454, Train Accuracy: 0.8165666666666667,\n",
      " Test Loss: 0.5606235861778259, Test Accuracy: 0.859\n",
      "Epoch 2,\n",
      " Train Loss: 0.5744231939315796, Train Accuracy: 0.86545,\n",
      " Test Loss: 0.5147086381912231, Test Accuracy: 0.8753\n",
      "Epoch 3,\n",
      " Train Loss: 0.6255579590797424, Train Accuracy: 0.8597166666666667,\n",
      " Test Loss: 0.6020618677139282, Test Accuracy: 0.8738\n",
      "Epoch 4,\n",
      " Train Loss: 0.6642473340034485, Train Accuracy: 0.8559833333333333,\n",
      " Test Loss: 1.281700611114502, Test Accuracy: 0.8454\n",
      "Epoch 5,\n",
      " Train Loss: 0.7455027103424072, Train Accuracy: 0.8439,\n",
      " Test Loss: 0.6784137487411499, Test Accuracy: 0.8351\n",
      "Epoch 6,\n",
      " Train Loss: 0.8425865173339844, Train Accuracy: 0.8245333333333333,\n",
      " Test Loss: 0.6964503526687622, Test Accuracy: 0.8195\n",
      "Epoch 7,\n",
      " Train Loss: 0.8977206349372864, Train Accuracy: 0.8110166666666667,\n",
      " Test Loss: 0.7463230490684509, Test Accuracy: 0.8172\n",
      "Epoch 8,\n",
      " Train Loss: 0.9917797446250916, Train Accuracy: 0.7992333333333334,\n",
      " Test Loss: 1.328758955001831, Test Accuracy: 0.8145\n",
      "Epoch 9,\n",
      " Train Loss: 1.0486820936203003, Train Accuracy: 0.7776166666666666,\n",
      " Test Loss: 0.9130948781967163, Test Accuracy: 0.7772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 1.1734747886657715, Train Accuracy: 0.76555,\n",
      " Test Loss: 0.9625411629676819, Test Accuracy: 0.7095\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.33971327543258667, Train Accuracy: 0.8966333333333333,\n",
      " Test Loss: 0.17352738976478577, Test Accuracy: 0.9485\n",
      "Epoch 2,\n",
      " Train Loss: 0.15743376314640045, Train Accuracy: 0.9524166666666667,\n",
      " Test Loss: 0.1437116414308548, Test Accuracy: 0.9565\n",
      "Epoch 3,\n",
      " Train Loss: 0.11319207400083542, Train Accuracy: 0.9654666666666667,\n",
      " Test Loss: 0.10540533810853958, Test Accuracy: 0.9683\n",
      "Epoch 4,\n",
      " Train Loss: 0.08733879029750824, Train Accuracy: 0.9736333333333334,\n",
      " Test Loss: 0.0978005900979042, Test Accuracy: 0.9694\n",
      "Epoch 5,\n",
      " Train Loss: 0.07042960077524185, Train Accuracy: 0.9787,\n",
      " Test Loss: 0.08840829133987427, Test Accuracy: 0.9725\n",
      "Epoch 6,\n",
      " Train Loss: 0.05897832661867142, Train Accuracy: 0.9818166666666667,\n",
      " Test Loss: 0.08602237701416016, Test Accuracy: 0.972\n",
      "Epoch 7,\n",
      " Train Loss: 0.048502709716558456, Train Accuracy: 0.9850833333333333,\n",
      " Test Loss: 0.07664754986763, Test Accuracy: 0.9766\n",
      "Epoch 8,\n",
      " Train Loss: 0.04046694561839104, Train Accuracy: 0.98795,\n",
      " Test Loss: 0.07555913925170898, Test Accuracy: 0.9759\n",
      "Epoch 9,\n",
      " Train Loss: 0.03404613211750984, Train Accuracy: 0.9899166666666667,\n",
      " Test Loss: 0.07143789529800415, Test Accuracy: 0.9775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.028052274137735367, Train Accuracy: 0.9917166666666667,\n",
      " Test Loss: 0.07013555616140366, Test Accuracy: 0.9781\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.4805423617362976, Train Accuracy: 0.8657666666666667,\n",
      " Test Loss: 0.5150047540664673, Test Accuracy: 0.8706\n",
      "Epoch 2,\n",
      " Train Loss: 0.3416652977466583, Train Accuracy: 0.9091833333333333,\n",
      " Test Loss: 0.31105363368988037, Test Accuracy: 0.9095\n",
      "Epoch 3,\n",
      " Train Loss: 0.32861995697021484, Train Accuracy: 0.9154166666666667,\n",
      " Test Loss: 0.30628517270088196, Test Accuracy: 0.917\n",
      "Epoch 4,\n",
      " Train Loss: 0.30440038442611694, Train Accuracy: 0.9228333333333333,\n",
      " Test Loss: 0.2806815207004547, Test Accuracy: 0.9287\n",
      "Epoch 5,\n",
      " Train Loss: 0.2990841269493103, Train Accuracy: 0.9256666666666666,\n",
      " Test Loss: 0.2814965844154358, Test Accuracy: 0.9308\n",
      "Epoch 6,\n",
      " Train Loss: 0.2789822220802307, Train Accuracy: 0.9311166666666667,\n",
      " Test Loss: 0.36181822419166565, Test Accuracy: 0.9226\n",
      "Epoch 7,\n",
      " Train Loss: 0.28333649039268494, Train Accuracy: 0.93195,\n",
      " Test Loss: 0.285332053899765, Test Accuracy: 0.9303\n",
      "Epoch 8,\n",
      " Train Loss: 0.28100404143333435, Train Accuracy: 0.9311166666666667,\n",
      " Test Loss: 0.2572689950466156, Test Accuracy: 0.9393\n",
      "Epoch 9,\n",
      " Train Loss: 0.2523553669452667, Train Accuracy: 0.9382333333333334,\n",
      " Test Loss: 0.28332439064979553, Test Accuracy: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.282926470041275, Train Accuracy: 0.9318666666666666,\n",
      " Test Loss: 0.2613462209701538, Test Accuracy: 0.9398\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.9726371169090271, Train Accuracy: 0.80095,\n",
      " Test Loss: 0.4861316680908203, Test Accuracy: 0.8871\n",
      "Epoch 2,\n",
      " Train Loss: 0.7754066586494446, Train Accuracy: 0.8291333333333334,\n",
      " Test Loss: 0.9299142956733704, Test Accuracy: 0.7533\n",
      "Epoch 3,\n",
      " Train Loss: 1.2271705865859985, Train Accuracy: 0.743,\n",
      " Test Loss: 1.040286898612976, Test Accuracy: 0.7273\n",
      "Epoch 4,\n",
      " Train Loss: 1.7621943950653076, Train Accuracy: 0.5779833333333333,\n",
      " Test Loss: 1.569638967514038, Test Accuracy: 0.5076\n",
      "Epoch 5,\n",
      " Train Loss: 2.6893837451934814, Train Accuracy: 0.4494,\n",
      " Test Loss: 1.746724009513855, Test Accuracy: 0.3717\n",
      "Epoch 6,\n",
      " Train Loss: 3.0177183151245117, Train Accuracy: 0.4093833333333333,\n",
      " Test Loss: 1.9215084314346313, Test Accuracy: 0.3973\n",
      "Epoch 7,\n",
      " Train Loss: 4.475308418273926, Train Accuracy: 0.3408333333333333,\n",
      " Test Loss: 2.241987466812134, Test Accuracy: 0.2945\n",
      "Epoch 8,\n",
      " Train Loss: 5.363742828369141, Train Accuracy: 0.26188333333333336,\n",
      " Test Loss: 2.229020595550537, Test Accuracy: 0.2539\n",
      "Epoch 9,\n",
      " Train Loss: 7.057121753692627, Train Accuracy: 0.21188333333333334,\n",
      " Test Loss: 2.275329351425171, Test Accuracy: 0.1252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 6.295046806335449, Train Accuracy: 0.15895,\n",
      " Test Loss: 2.1698999404907227, Test Accuracy: 0.2247\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.45205408334732056, Train Accuracy: 0.8688833333333333,\n",
      " Test Loss: 0.26903975009918213, Test Accuracy: 0.9206\n",
      "Epoch 2,\n",
      " Train Loss: 0.24352777004241943, Train Accuracy: 0.9279833333333334,\n",
      " Test Loss: 0.19741764664649963, Test Accuracy: 0.9432\n",
      "Epoch 3,\n",
      " Train Loss: 0.1900893896818161, Train Accuracy: 0.94465,\n",
      " Test Loss: 0.16885846853256226, Test Accuracy: 0.9507\n",
      "Epoch 4,\n",
      " Train Loss: 0.1557254046201706, Train Accuracy: 0.9546666666666667,\n",
      " Test Loss: 0.1419430673122406, Test Accuracy: 0.9578\n",
      "Epoch 5,\n",
      " Train Loss: 0.13215427100658417, Train Accuracy: 0.9614,\n",
      " Test Loss: 0.13235226273536682, Test Accuracy: 0.9601\n",
      "Epoch 6,\n",
      " Train Loss: 0.1149367168545723, Train Accuracy: 0.96655,\n",
      " Test Loss: 0.11736521124839783, Test Accuracy: 0.9669\n",
      "Epoch 7,\n",
      " Train Loss: 0.10179615020751953, Train Accuracy: 0.9705833333333334,\n",
      " Test Loss: 0.11192925274372101, Test Accuracy: 0.9656\n",
      "Epoch 8,\n",
      " Train Loss: 0.09091322869062424, Train Accuracy: 0.9734166666666667,\n",
      " Test Loss: 0.10182536393404007, Test Accuracy: 0.9687\n",
      "Epoch 9,\n",
      " Train Loss: 0.08263847231864929, Train Accuracy: 0.97575,\n",
      " Test Loss: 0.10084176808595657, Test Accuracy: 0.9687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.0750209391117096, Train Accuracy: 0.978,\n",
      " Test Loss: 0.09470189362764359, Test Accuracy: 0.9709\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.49449682235717773, Train Accuracy: 0.8623,\n",
      " Test Loss: 0.33622586727142334, Test Accuracy: 0.8987\n",
      "Epoch 2,\n",
      " Train Loss: 0.32690539956092834, Train Accuracy: 0.90175,\n",
      " Test Loss: 0.3766188323497772, Test Accuracy: 0.8839\n",
      "Epoch 3,\n",
      " Train Loss: 0.3022042214870453, Train Accuracy: 0.9114,\n",
      " Test Loss: 0.26297226548194885, Test Accuracy: 0.9229\n",
      "Epoch 4,\n",
      " Train Loss: 0.2841942012310028, Train Accuracy: 0.9172666666666667,\n",
      " Test Loss: 0.3406897187232971, Test Accuracy: 0.9036\n",
      "Epoch 5,\n",
      " Train Loss: 0.290629506111145, Train Accuracy: 0.9167,\n",
      " Test Loss: 0.26700499653816223, Test Accuracy: 0.9222\n",
      "Epoch 6,\n",
      " Train Loss: 0.2718823254108429, Train Accuracy: 0.9207666666666666,\n",
      " Test Loss: 0.27236756682395935, Test Accuracy: 0.9235\n",
      "Epoch 7,\n",
      " Train Loss: 0.27252522110939026, Train Accuracy: 0.92065,\n",
      " Test Loss: 0.256172239780426, Test Accuracy: 0.9283\n",
      "Epoch 8,\n",
      " Train Loss: 0.27763280272483826, Train Accuracy: 0.9208833333333334,\n",
      " Test Loss: 0.27322155237197876, Test Accuracy: 0.9246\n",
      "Epoch 9,\n",
      " Train Loss: 0.2626837491989136, Train Accuracy: 0.9245333333333333,\n",
      " Test Loss: 0.2956813871860504, Test Accuracy: 0.92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.26404252648353577, Train Accuracy: 0.92445,\n",
      " Test Loss: 0.38948309421539307, Test Accuracy: 0.8858\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.01694917678833, Train Accuracy: 0.8123666666666667,\n",
      " Test Loss: 0.7812280654907227, Test Accuracy: 0.8325\n",
      "Epoch 2,\n",
      " Train Loss: 0.4337003231048584, Train Accuracy: 0.8895833333333333,\n",
      " Test Loss: 0.3411356508731842, Test Accuracy: 0.9054\n",
      "Epoch 3,\n",
      " Train Loss: 0.432691752910614, Train Accuracy: 0.8963333333333333,\n",
      " Test Loss: 0.4639309346675873, Test Accuracy: 0.8991\n",
      "Epoch 4,\n",
      " Train Loss: 0.4107125401496887, Train Accuracy: 0.90235,\n",
      " Test Loss: 0.4529634714126587, Test Accuracy: 0.8934\n",
      "Epoch 5,\n",
      " Train Loss: 0.4230164587497711, Train Accuracy: 0.9045166666666666,\n",
      " Test Loss: 0.5278235077857971, Test Accuracy: 0.86\n",
      "Epoch 6,\n",
      " Train Loss: 0.4249080717563629, Train Accuracy: 0.9010666666666667,\n",
      " Test Loss: 0.3554205000400543, Test Accuracy: 0.9128\n",
      "Epoch 7,\n",
      " Train Loss: 0.4445227384567261, Train Accuracy: 0.8988666666666667,\n",
      " Test Loss: 0.48572424054145813, Test Accuracy: 0.883\n",
      "Epoch 8,\n",
      " Train Loss: 0.4690500497817993, Train Accuracy: 0.89535,\n",
      " Test Loss: 0.7088534832000732, Test Accuracy: 0.8965\n",
      "Epoch 9,\n",
      " Train Loss: 0.49024730920791626, Train Accuracy: 0.8925166666666666,\n",
      " Test Loss: 0.7292876243591309, Test Accuracy: 0.8614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.5451158285140991, Train Accuracy: 0.8846666666666667,\n",
      " Test Loss: 0.6365649104118347, Test Accuracy: 0.8924\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.45835080742836, Train Accuracy: 0.8668833333333333,\n",
      " Test Loss: 0.27498164772987366, Test Accuracy: 0.9229\n",
      "Epoch 2,\n",
      " Train Loss: 0.2543190121650696, Train Accuracy: 0.9243333333333333,\n",
      " Test Loss: 0.21879956126213074, Test Accuracy: 0.9359\n",
      "Epoch 3,\n",
      " Train Loss: 0.20342153310775757, Train Accuracy: 0.9399666666666666,\n",
      " Test Loss: 0.19191324710845947, Test Accuracy: 0.9424\n",
      "Epoch 4,\n",
      " Train Loss: 0.17159698903560638, Train Accuracy: 0.9491333333333334,\n",
      " Test Loss: 0.15732263028621674, Test Accuracy: 0.9534\n",
      "Epoch 5,\n",
      " Train Loss: 0.14733454585075378, Train Accuracy: 0.9553333333333334,\n",
      " Test Loss: 0.14162157475948334, Test Accuracy: 0.9561\n",
      "Epoch 6,\n",
      " Train Loss: 0.1301814466714859, Train Accuracy: 0.96165,\n",
      " Test Loss: 0.1328956037759781, Test Accuracy: 0.9595\n",
      "Epoch 7,\n",
      " Train Loss: 0.11586350202560425, Train Accuracy: 0.96595,\n",
      " Test Loss: 0.12851591408252716, Test Accuracy: 0.9591\n",
      "Epoch 8,\n",
      " Train Loss: 0.10541718453168869, Train Accuracy: 0.9687333333333333,\n",
      " Test Loss: 0.11561856418848038, Test Accuracy: 0.9645\n",
      "Epoch 9,\n",
      " Train Loss: 0.09550023823976517, Train Accuracy: 0.9722166666666666,\n",
      " Test Loss: 0.11274569481611252, Test Accuracy: 0.9654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.08744298666715622, Train Accuracy: 0.9742833333333333,\n",
      " Test Loss: 0.10590321570634842, Test Accuracy: 0.9676\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.42416316270828247, Train Accuracy: 0.8779333333333333,\n",
      " Test Loss: 0.2557583153247833, Test Accuracy: 0.9208\n",
      "Epoch 2,\n",
      " Train Loss: 0.2737957835197449, Train Accuracy: 0.9194,\n",
      " Test Loss: 0.24098467826843262, Test Accuracy: 0.933\n",
      "Epoch 3,\n",
      " Train Loss: 0.2630580961704254, Train Accuracy: 0.9244833333333333,\n",
      " Test Loss: 0.27353358268737793, Test Accuracy: 0.9153\n",
      "Epoch 4,\n",
      " Train Loss: 0.2570752203464508, Train Accuracy: 0.9260833333333334,\n",
      " Test Loss: 0.24991931021213531, Test Accuracy: 0.9306\n",
      "Epoch 5,\n",
      " Train Loss: 0.25039705634117126, Train Accuracy: 0.92955,\n",
      " Test Loss: 0.26016533374786377, Test Accuracy: 0.9225\n",
      "Epoch 6,\n",
      " Train Loss: 0.24080558121204376, Train Accuracy: 0.9331333333333334,\n",
      " Test Loss: 0.2908198833465576, Test Accuracy: 0.9196\n",
      "Epoch 7,\n",
      " Train Loss: 0.23579071462154388, Train Accuracy: 0.93395,\n",
      " Test Loss: 0.23387765884399414, Test Accuracy: 0.9356\n",
      "Epoch 8,\n",
      " Train Loss: 0.23064309358596802, Train Accuracy: 0.93505,\n",
      " Test Loss: 0.23785465955734253, Test Accuracy: 0.933\n",
      "Epoch 9,\n",
      " Train Loss: 0.2356308102607727, Train Accuracy: 0.93485,\n",
      " Test Loss: 0.24931135773658752, Test Accuracy: 0.9381\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.2255742847919464, Train Accuracy: 0.9368333333333333,\n",
      " Test Loss: 0.23887313902378082, Test Accuracy: 0.9403\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.7265651226043701, Train Accuracy: 0.8250333333333333,\n",
      " Test Loss: 0.6402016878128052, Test Accuracy: 0.8273\n",
      "Epoch 2,\n",
      " Train Loss: 0.4180527627468109, Train Accuracy: 0.89665,\n",
      " Test Loss: 0.9179418087005615, Test Accuracy: 0.8082\n",
      "Epoch 3,\n",
      " Train Loss: 0.4146360754966736, Train Accuracy: 0.9008166666666667,\n",
      " Test Loss: 0.378533273935318, Test Accuracy: 0.9026\n",
      "Epoch 4,\n",
      " Train Loss: 0.4004126787185669, Train Accuracy: 0.9043833333333333,\n",
      " Test Loss: 0.29047438502311707, Test Accuracy: 0.9228\n",
      "Epoch 5,\n",
      " Train Loss: 0.4272466003894806, Train Accuracy: 0.9022833333333333,\n",
      " Test Loss: 0.4395327866077423, Test Accuracy: 0.9217\n",
      "Epoch 6,\n",
      " Train Loss: 0.41772422194480896, Train Accuracy: 0.90395,\n",
      " Test Loss: 0.5830891728401184, Test Accuracy: 0.8757\n",
      "Epoch 7,\n",
      " Train Loss: 0.4275740683078766, Train Accuracy: 0.9012833333333333,\n",
      " Test Loss: 0.7146276831626892, Test Accuracy: 0.8798\n",
      "Epoch 8,\n",
      " Train Loss: 0.4441588222980499, Train Accuracy: 0.9005166666666666,\n",
      " Test Loss: 0.7498300075531006, Test Accuracy: 0.846\n",
      "Epoch 9,\n",
      " Train Loss: 0.4618602395057678, Train Accuracy: 0.8961333333333333,\n",
      " Test Loss: 0.9992594122886658, Test Accuracy: 0.8197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.4717327952384949, Train Accuracy: 0.8985166666666666,\n",
      " Test Loss: 0.5923338532447815, Test Accuracy: 0.8936\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.42062488198280334, Train Accuracy: 0.87845,\n",
      " Test Loss: 0.22152529656887054, Test Accuracy: 0.9346\n",
      "Epoch 2,\n",
      " Train Loss: 0.20861175656318665, Train Accuracy: 0.9368666666666666,\n",
      " Test Loss: 0.16423426568508148, Test Accuracy: 0.9535\n",
      "Epoch 3,\n",
      " Train Loss: 0.1549316942691803, Train Accuracy: 0.9543333333333334,\n",
      " Test Loss: 0.15492995083332062, Test Accuracy: 0.9516\n",
      "Epoch 4,\n",
      " Train Loss: 0.12397599965333939, Train Accuracy: 0.9635166666666667,\n",
      " Test Loss: 0.12619532644748688, Test Accuracy: 0.9608\n",
      "Epoch 5,\n",
      " Train Loss: 0.10284994542598724, Train Accuracy: 0.97015,\n",
      " Test Loss: 0.10759338736534119, Test Accuracy: 0.966\n",
      "Epoch 6,\n",
      " Train Loss: 0.0867980420589447, Train Accuracy: 0.9745,\n",
      " Test Loss: 0.10906525701284409, Test Accuracy: 0.968\n",
      "Epoch 7,\n",
      " Train Loss: 0.07538767904043198, Train Accuracy: 0.9781666666666666,\n",
      " Test Loss: 0.09220417588949203, Test Accuracy: 0.9719\n",
      "Epoch 8,\n",
      " Train Loss: 0.06537169963121414, Train Accuracy: 0.9809833333333333,\n",
      " Test Loss: 0.08637472242116928, Test Accuracy: 0.973\n",
      "Epoch 9,\n",
      " Train Loss: 0.057504091411828995, Train Accuracy: 0.9830666666666666,\n",
      " Test Loss: 0.08241800963878632, Test Accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.051008570939302444, Train Accuracy: 0.9849,\n",
      " Test Loss: 0.08875386416912079, Test Accuracy: 0.9722\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5178348422050476, Train Accuracy: 0.8647,\n",
      " Test Loss: 0.2582460045814514, Test Accuracy: 0.9259\n",
      "Epoch 2,\n",
      " Train Loss: 0.27102163434028625, Train Accuracy: 0.9212833333333333,\n",
      " Test Loss: 0.24207186698913574, Test Accuracy: 0.9297\n",
      "Epoch 3,\n",
      " Train Loss: 0.25378331542015076, Train Accuracy: 0.9291666666666667,\n",
      " Test Loss: 0.3323366641998291, Test Accuracy: 0.9143\n",
      "Epoch 4,\n",
      " Train Loss: 0.25492218136787415, Train Accuracy: 0.9302833333333334,\n",
      " Test Loss: 0.21802428364753723, Test Accuracy: 0.9406\n",
      "Epoch 5,\n",
      " Train Loss: 0.24003499746322632, Train Accuracy: 0.9344833333333333,\n",
      " Test Loss: 0.2914714515209198, Test Accuracy: 0.9231\n",
      "Epoch 6,\n",
      " Train Loss: 0.24339327216148376, Train Accuracy: 0.93545,\n",
      " Test Loss: 0.22455820441246033, Test Accuracy: 0.9442\n",
      "Epoch 7,\n",
      " Train Loss: 0.2341633439064026, Train Accuracy: 0.9391,\n",
      " Test Loss: 0.2118164300918579, Test Accuracy: 0.947\n",
      "Epoch 8,\n",
      " Train Loss: 0.23510423302650452, Train Accuracy: 0.9391333333333334,\n",
      " Test Loss: 0.24782095849514008, Test Accuracy: 0.9355\n",
      "Epoch 9,\n",
      " Train Loss: 0.2342415452003479, Train Accuracy: 0.9391166666666667,\n",
      " Test Loss: 0.2639676332473755, Test Accuracy: 0.9375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.22410430014133453, Train Accuracy: 0.9411333333333334,\n",
      " Test Loss: 0.19970062375068665, Test Accuracy: 0.95\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.1762278079986572, Train Accuracy: 0.7975833333333333,\n",
      " Test Loss: 0.5973672270774841, Test Accuracy: 0.8579\n",
      "Epoch 2,\n",
      " Train Loss: 0.4651339054107666, Train Accuracy: 0.88825,\n",
      " Test Loss: 0.4047101140022278, Test Accuracy: 0.9035\n",
      "Epoch 3,\n",
      " Train Loss: 0.4587474465370178, Train Accuracy: 0.8966,\n",
      " Test Loss: 0.5813605189323425, Test Accuracy: 0.8921\n",
      "Epoch 4,\n",
      " Train Loss: 0.5312994718551636, Train Accuracy: 0.89065,\n",
      " Test Loss: 0.5531131029129028, Test Accuracy: 0.8882\n",
      "Epoch 5,\n",
      " Train Loss: 0.5990433096885681, Train Accuracy: 0.8707166666666667,\n",
      " Test Loss: 0.6126871109008789, Test Accuracy: 0.8472\n",
      "Epoch 6,\n",
      " Train Loss: 0.7349058389663696, Train Accuracy: 0.8423,\n",
      " Test Loss: 0.9143067598342896, Test Accuracy: 0.8663\n",
      "Epoch 7,\n",
      " Train Loss: 0.8624733090400696, Train Accuracy: 0.80135,\n",
      " Test Loss: 1.0772595405578613, Test Accuracy: 0.7878\n",
      "Epoch 8,\n",
      " Train Loss: 1.063124418258667, Train Accuracy: 0.7688166666666667,\n",
      " Test Loss: 2.3269026279449463, Test Accuracy: 0.7905\n",
      "Epoch 9,\n",
      " Train Loss: 1.217419981956482, Train Accuracy: 0.7364333333333334,\n",
      " Test Loss: 5.0155816078186035, Test Accuracy: 0.6681\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 1.5903410911560059, Train Accuracy: 0.7003833333333334,\n",
      " Test Loss: 2.29620099067688, Test Accuracy: 0.6806\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.5541693568229675, Train Accuracy: 0.8423333333333334,\n",
      " Test Loss: 0.31501710414886475, Test Accuracy: 0.9089\n",
      "Epoch 2,\n",
      " Train Loss: 0.2995542287826538, Train Accuracy: 0.9125833333333333,\n",
      " Test Loss: 0.25622767210006714, Test Accuracy: 0.9271\n",
      "Epoch 3,\n",
      " Train Loss: 0.2521199584007263, Train Accuracy: 0.92595,\n",
      " Test Loss: 0.2241670936346054, Test Accuracy: 0.9355\n",
      "Epoch 4,\n",
      " Train Loss: 0.2176239788532257, Train Accuracy: 0.9374166666666667,\n",
      " Test Loss: 0.1976730078458786, Test Accuracy: 0.9424\n",
      "Epoch 5,\n",
      " Train Loss: 0.1916159987449646, Train Accuracy: 0.94455,\n",
      " Test Loss: 0.18120311200618744, Test Accuracy: 0.9467\n",
      "Epoch 6,\n",
      " Train Loss: 0.1704007238149643, Train Accuracy: 0.9506,\n",
      " Test Loss: 0.16252201795578003, Test Accuracy: 0.953\n",
      "Epoch 7,\n",
      " Train Loss: 0.15346789360046387, Train Accuracy: 0.9564333333333334,\n",
      " Test Loss: 0.1513698250055313, Test Accuracy: 0.9548\n",
      "Epoch 8,\n",
      " Train Loss: 0.13974031805992126, Train Accuracy: 0.9601333333333333,\n",
      " Test Loss: 0.13808229565620422, Test Accuracy: 0.9585\n",
      "Epoch 9,\n",
      " Train Loss: 0.12817654013633728, Train Accuracy: 0.9632333333333334,\n",
      " Test Loss: 0.1321800947189331, Test Accuracy: 0.9599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.11816462129354477, Train Accuracy: 0.96615,\n",
      " Test Loss: 0.12342911958694458, Test Accuracy: 0.9619\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5322698354721069, Train Accuracy: 0.8626333333333334,\n",
      " Test Loss: 0.30167195200920105, Test Accuracy: 0.9039\n",
      "Epoch 2,\n",
      " Train Loss: 0.27322232723236084, Train Accuracy: 0.9168833333333334,\n",
      " Test Loss: 0.2586810886859894, Test Accuracy: 0.9187\n",
      "Epoch 3,\n",
      " Train Loss: 0.24809080362319946, Train Accuracy: 0.9243666666666667,\n",
      " Test Loss: 0.2860657274723053, Test Accuracy: 0.9154\n",
      "Epoch 4,\n",
      " Train Loss: 0.24672463536262512, Train Accuracy: 0.92675,\n",
      " Test Loss: 0.2132331281900406, Test Accuracy: 0.9367\n",
      "Epoch 5,\n",
      " Train Loss: 0.23401311039924622, Train Accuracy: 0.9311666666666667,\n",
      " Test Loss: 0.2417137324810028, Test Accuracy: 0.9264\n",
      "Epoch 6,\n",
      " Train Loss: 0.24206779897212982, Train Accuracy: 0.9286666666666666,\n",
      " Test Loss: 0.2787972092628479, Test Accuracy: 0.9178\n",
      "Epoch 7,\n",
      " Train Loss: 0.22836421430110931, Train Accuracy: 0.9339666666666666,\n",
      " Test Loss: 0.24088500440120697, Test Accuracy: 0.9312\n",
      "Epoch 8,\n",
      " Train Loss: 0.23199574649333954, Train Accuracy: 0.9319333333333333,\n",
      " Test Loss: 0.22852566838264465, Test Accuracy: 0.9347\n",
      "Epoch 9,\n",
      " Train Loss: 0.22517047822475433, Train Accuracy: 0.9338833333333333,\n",
      " Test Loss: 0.24880032241344452, Test Accuracy: 0.9303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.22793622314929962, Train Accuracy: 0.93385,\n",
      " Test Loss: 0.20406684279441833, Test Accuracy: 0.9402\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.1426995992660522, Train Accuracy: 0.79255,\n",
      " Test Loss: 0.2951207160949707, Test Accuracy: 0.9134\n",
      "Epoch 2,\n",
      " Train Loss: 0.3743778467178345, Train Accuracy: 0.8983666666666666,\n",
      " Test Loss: 0.23379695415496826, Test Accuracy: 0.9355\n",
      "Epoch 3,\n",
      " Train Loss: 0.334916353225708, Train Accuracy: 0.911,\n",
      " Test Loss: 0.3090975880622864, Test Accuracy: 0.914\n",
      "Epoch 4,\n",
      " Train Loss: 0.32280024886131287, Train Accuracy: 0.9177833333333333,\n",
      " Test Loss: 0.3187783360481262, Test Accuracy: 0.91\n",
      "Epoch 5,\n",
      " Train Loss: 0.32230037450790405, Train Accuracy: 0.91955,\n",
      " Test Loss: 0.3822087347507477, Test Accuracy: 0.9185\n",
      "Epoch 6,\n",
      " Train Loss: 0.31991857290267944, Train Accuracy: 0.9231833333333334,\n",
      " Test Loss: 0.4118133783340454, Test Accuracy: 0.8977\n",
      "Epoch 7,\n",
      " Train Loss: 0.3152559697628021, Train Accuracy: 0.9250333333333334,\n",
      " Test Loss: 0.30686259269714355, Test Accuracy: 0.9318\n",
      "Epoch 8,\n",
      " Train Loss: 0.32130709290504456, Train Accuracy: 0.9240166666666667,\n",
      " Test Loss: 0.7002437710762024, Test Accuracy: 0.8325\n",
      "Epoch 9,\n",
      " Train Loss: 0.32009589672088623, Train Accuracy: 0.9242,\n",
      " Test Loss: 0.36832496523857117, Test Accuracy: 0.9018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.3084287941455841, Train Accuracy: 0.9273166666666667,\n",
      " Test Loss: 0.2554841637611389, Test Accuracy: 0.9338\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.5837955474853516, Train Accuracy: 0.8334833333333334,\n",
      " Test Loss: 0.3172970712184906, Test Accuracy: 0.9093\n",
      "Epoch 2,\n",
      " Train Loss: 0.3050764799118042, Train Accuracy: 0.91105,\n",
      " Test Loss: 0.2636534869670868, Test Accuracy: 0.9238\n",
      "Epoch 3,\n",
      " Train Loss: 0.25851041078567505, Train Accuracy: 0.9243,\n",
      " Test Loss: 0.2397751361131668, Test Accuracy: 0.9339\n",
      "Epoch 4,\n",
      " Train Loss: 0.22611495852470398, Train Accuracy: 0.9337833333333333,\n",
      " Test Loss: 0.2188507467508316, Test Accuracy: 0.9343\n",
      "Epoch 5,\n",
      " Train Loss: 0.20178288221359253, Train Accuracy: 0.9416666666666667,\n",
      " Test Loss: 0.18632057309150696, Test Accuracy: 0.946\n",
      "Epoch 6,\n",
      " Train Loss: 0.18178151547908783, Train Accuracy: 0.9475166666666667,\n",
      " Test Loss: 0.17782019078731537, Test Accuracy: 0.9493\n",
      "Epoch 7,\n",
      " Train Loss: 0.16608168184757233, Train Accuracy: 0.9518833333333333,\n",
      " Test Loss: 0.16477523744106293, Test Accuracy: 0.9534\n",
      "Epoch 8,\n",
      " Train Loss: 0.15241046249866486, Train Accuracy: 0.9562166666666667,\n",
      " Test Loss: 0.15361496806144714, Test Accuracy: 0.954\n",
      "Epoch 9,\n",
      " Train Loss: 0.14100998640060425, Train Accuracy: 0.9593,\n",
      " Test Loss: 0.14627274870872498, Test Accuracy: 0.9582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.13116012513637543, Train Accuracy: 0.9616666666666667,\n",
      " Test Loss: 0.14118367433547974, Test Accuracy: 0.9581\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.48328569531440735, Train Accuracy: 0.86585,\n",
      " Test Loss: 0.2380288988351822, Test Accuracy: 0.9301\n",
      "Epoch 2,\n",
      " Train Loss: 0.239725723862648, Train Accuracy: 0.92805,\n",
      " Test Loss: 0.21644502878189087, Test Accuracy: 0.936\n",
      "Epoch 3,\n",
      " Train Loss: 0.21723830699920654, Train Accuracy: 0.9353166666666667,\n",
      " Test Loss: 0.2104821503162384, Test Accuracy: 0.9356\n",
      "Epoch 4,\n",
      " Train Loss: 0.20598623156547546, Train Accuracy: 0.9388166666666666,\n",
      " Test Loss: 0.2002270668745041, Test Accuracy: 0.9424\n",
      "Epoch 5,\n",
      " Train Loss: 0.19895491003990173, Train Accuracy: 0.9412166666666667,\n",
      " Test Loss: 0.20742732286453247, Test Accuracy: 0.935\n",
      "Epoch 6,\n",
      " Train Loss: 0.20042194426059723, Train Accuracy: 0.9412666666666667,\n",
      " Test Loss: 0.22779126465320587, Test Accuracy: 0.9359\n",
      "Epoch 7,\n",
      " Train Loss: 0.19334441423416138, Train Accuracy: 0.9441333333333334,\n",
      " Test Loss: 0.1922036111354828, Test Accuracy: 0.9454\n",
      "Epoch 8,\n",
      " Train Loss: 0.19662141799926758, Train Accuracy: 0.9429,\n",
      " Test Loss: 0.21860858798027039, Test Accuracy: 0.9396\n",
      "Epoch 9,\n",
      " Train Loss: 0.18748295307159424, Train Accuracy: 0.94585,\n",
      " Test Loss: 0.22934135794639587, Test Accuracy: 0.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.18847164511680603, Train Accuracy: 0.94575,\n",
      " Test Loss: 0.2295951545238495, Test Accuracy: 0.9407\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.9363395571708679, Train Accuracy: 0.8003833333333333,\n",
      " Test Loss: 0.37131887674331665, Test Accuracy: 0.889\n",
      "Epoch 2,\n",
      " Train Loss: 0.355430543422699, Train Accuracy: 0.9002,\n",
      " Test Loss: 0.3943839371204376, Test Accuracy: 0.8868\n",
      "Epoch 3,\n",
      " Train Loss: 0.3167189657688141, Train Accuracy: 0.91475,\n",
      " Test Loss: 0.2649686634540558, Test Accuracy: 0.9212\n",
      "Epoch 4,\n",
      " Train Loss: 0.3059852421283722, Train Accuracy: 0.9207833333333333,\n",
      " Test Loss: 0.2738775312900543, Test Accuracy: 0.9306\n",
      "Epoch 5,\n",
      " Train Loss: 0.3097604513168335, Train Accuracy: 0.9228,\n",
      " Test Loss: 0.3182990550994873, Test Accuracy: 0.9284\n",
      "Epoch 6,\n",
      " Train Loss: 0.28940215706825256, Train Accuracy: 0.9271833333333334,\n",
      " Test Loss: 0.41168129444122314, Test Accuracy: 0.892\n",
      "Epoch 7,\n",
      " Train Loss: 0.30255547165870667, Train Accuracy: 0.9263666666666667,\n",
      " Test Loss: 0.4661247730255127, Test Accuracy: 0.8973\n",
      "Epoch 8,\n",
      " Train Loss: 0.3043879568576813, Train Accuracy: 0.927,\n",
      " Test Loss: 0.3084072768688202, Test Accuracy: 0.9391\n",
      "Epoch 9,\n",
      " Train Loss: 0.3114337623119354, Train Accuracy: 0.9271166666666667,\n",
      " Test Loss: 0.4362366199493408, Test Accuracy: 0.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.3190978169441223, Train Accuracy: 0.9288166666666666,\n",
      " Test Loss: 0.2756028473377228, Test Accuracy: 0.933\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.5140765905380249, Train Accuracy: 0.85465,\n",
      " Test Loss: 0.3042922616004944, Test Accuracy: 0.9108\n",
      "Epoch 2,\n",
      " Train Loss: 0.26199769973754883, Train Accuracy: 0.9231166666666667,\n",
      " Test Loss: 0.2196965515613556, Test Accuracy: 0.9357\n",
      "Epoch 3,\n",
      " Train Loss: 0.20832687616348267, Train Accuracy: 0.9388666666666666,\n",
      " Test Loss: 0.193017840385437, Test Accuracy: 0.946\n",
      "Epoch 4,\n",
      " Train Loss: 0.1738159954547882, Train Accuracy: 0.9497333333333333,\n",
      " Test Loss: 0.1597682386636734, Test Accuracy: 0.9534\n",
      "Epoch 5,\n",
      " Train Loss: 0.14881719648838043, Train Accuracy: 0.95665,\n",
      " Test Loss: 0.14319463074207306, Test Accuracy: 0.9575\n",
      "Epoch 6,\n",
      " Train Loss: 0.13057981431484222, Train Accuracy: 0.9619166666666666,\n",
      " Test Loss: 0.12826219201087952, Test Accuracy: 0.9603\n",
      "Epoch 7,\n",
      " Train Loss: 0.11564508080482483, Train Accuracy: 0.9665833333333333,\n",
      " Test Loss: 0.12754686176776886, Test Accuracy: 0.9605\n",
      "Epoch 8,\n",
      " Train Loss: 0.10395710170269012, Train Accuracy: 0.9701333333333333,\n",
      " Test Loss: 0.11174724996089935, Test Accuracy: 0.9656\n",
      "Epoch 9,\n",
      " Train Loss: 0.0939522534608841, Train Accuracy: 0.9729333333333333,\n",
      " Test Loss: 0.1056116595864296, Test Accuracy: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.08554096519947052, Train Accuracy: 0.9754833333333334,\n",
      " Test Loss: 0.10340040922164917, Test Accuracy: 0.9683\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.6784155964851379, Train Accuracy: 0.8524333333333334,\n",
      " Test Loss: 0.2382020652294159, Test Accuracy: 0.9291\n",
      "Epoch 2,\n",
      " Train Loss: 0.2494654655456543, Train Accuracy: 0.9255666666666666,\n",
      " Test Loss: 0.22965185344219208, Test Accuracy: 0.9332\n",
      "Epoch 3,\n",
      " Train Loss: 0.2139102965593338, Train Accuracy: 0.9373,\n",
      " Test Loss: 0.2546934485435486, Test Accuracy: 0.9251\n",
      "Epoch 4,\n",
      " Train Loss: 0.2036992758512497, Train Accuracy: 0.9397,\n",
      " Test Loss: 0.19529274106025696, Test Accuracy: 0.9452\n",
      "Epoch 5,\n",
      " Train Loss: 0.19526737928390503, Train Accuracy: 0.94225,\n",
      " Test Loss: 0.19941021502017975, Test Accuracy: 0.9463\n",
      "Epoch 6,\n",
      " Train Loss: 0.195087268948555, Train Accuracy: 0.9435833333333333,\n",
      " Test Loss: 0.19710852205753326, Test Accuracy: 0.9452\n",
      "Epoch 7,\n",
      " Train Loss: 0.18830569088459015, Train Accuracy: 0.9461833333333334,\n",
      " Test Loss: 0.2067136913537979, Test Accuracy: 0.9435\n",
      "Epoch 8,\n",
      " Train Loss: 0.1833653599023819, Train Accuracy: 0.9479,\n",
      " Test Loss: 0.20762097835540771, Test Accuracy: 0.9434\n",
      "Epoch 9,\n",
      " Train Loss: 0.1872878074645996, Train Accuracy: 0.9491666666666667,\n",
      " Test Loss: 0.22055435180664062, Test Accuracy: 0.9426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.18032757937908173, Train Accuracy: 0.9504666666666667,\n",
      " Test Loss: 0.19840343296527863, Test Accuracy: 0.9517\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.5850201845169067, Train Accuracy: 0.7546666666666667,\n",
      " Test Loss: 0.2789803445339203, Test Accuracy: 0.9256\n",
      "Epoch 2,\n",
      " Train Loss: 0.3820890784263611, Train Accuracy: 0.8961833333333333,\n",
      " Test Loss: 0.31084245443344116, Test Accuracy: 0.915\n",
      "Epoch 3,\n",
      " Train Loss: 0.34105488657951355, Train Accuracy: 0.9113833333333333,\n",
      " Test Loss: 0.42079031467437744, Test Accuracy: 0.9096\n",
      "Epoch 4,\n",
      " Train Loss: 0.33962592482566833, Train Accuracy: 0.9182333333333333,\n",
      " Test Loss: 0.42299503087997437, Test Accuracy: 0.9206\n",
      "Epoch 5,\n",
      " Train Loss: 0.32890239357948303, Train Accuracy: 0.92105,\n",
      " Test Loss: 0.3353120982646942, Test Accuracy: 0.9126\n",
      "Epoch 6,\n",
      " Train Loss: 0.3097460865974426, Train Accuracy: 0.9266333333333333,\n",
      " Test Loss: 0.25498589873313904, Test Accuracy: 0.9439\n",
      "Epoch 7,\n",
      " Train Loss: 0.32415106892585754, Train Accuracy: 0.9268,\n",
      " Test Loss: 0.2787708640098572, Test Accuracy: 0.9396\n",
      "Epoch 8,\n",
      " Train Loss: 0.3136627972126007, Train Accuracy: 0.9285,\n",
      " Test Loss: 0.30088573694229126, Test Accuracy: 0.9483\n",
      "Epoch 9,\n",
      " Train Loss: 0.3477703034877777, Train Accuracy: 0.9227166666666666,\n",
      " Test Loss: 0.27571240067481995, Test Accuracy: 0.9409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.36786195635795593, Train Accuracy: 0.92085,\n",
      " Test Loss: 0.5205202698707581, Test Accuracy: 0.858\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.72721928358078, Train Accuracy: 0.804,\n",
      " Test Loss: 0.39511290192604065, Test Accuracy: 0.8878\n",
      "Epoch 2,\n",
      " Train Loss: 0.35856369137763977, Train Accuracy: 0.8962666666666667,\n",
      " Test Loss: 0.3375670313835144, Test Accuracy: 0.8999\n",
      "Epoch 3,\n",
      " Train Loss: 0.3048311173915863, Train Accuracy: 0.9119166666666667,\n",
      " Test Loss: 0.28512829542160034, Test Accuracy: 0.9155\n",
      "Epoch 4,\n",
      " Train Loss: 0.27361658215522766, Train Accuracy: 0.9207833333333333,\n",
      " Test Loss: 0.26578086614608765, Test Accuracy: 0.917\n",
      "Epoch 5,\n",
      " Train Loss: 0.25143158435821533, Train Accuracy: 0.9268666666666666,\n",
      " Test Loss: 0.23800909519195557, Test Accuracy: 0.9311\n",
      "Epoch 6,\n",
      " Train Loss: 0.23286759853363037, Train Accuracy: 0.9326333333333333,\n",
      " Test Loss: 0.2169760763645172, Test Accuracy: 0.9369\n",
      "Epoch 7,\n",
      " Train Loss: 0.21737505495548248, Train Accuracy: 0.9373666666666667,\n",
      " Test Loss: 0.21680998802185059, Test Accuracy: 0.9347\n",
      "Epoch 8,\n",
      " Train Loss: 0.20402851700782776, Train Accuracy: 0.9408666666666666,\n",
      " Test Loss: 0.19592761993408203, Test Accuracy: 0.9428\n",
      "Epoch 9,\n",
      " Train Loss: 0.19241227209568024, Train Accuracy: 0.944,\n",
      " Test Loss: 0.18461892008781433, Test Accuracy: 0.9467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.1813201904296875, Train Accuracy: 0.9478666666666666,\n",
      " Test Loss: 0.1748461276292801, Test Accuracy: 0.949\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.7093170881271362, Train Accuracy: 0.8474333333333334,\n",
      " Test Loss: 0.21256884932518005, Test Accuracy: 0.9337\n",
      "Epoch 2,\n",
      " Train Loss: 0.20183296501636505, Train Accuracy: 0.9382666666666667,\n",
      " Test Loss: 0.21243155002593994, Test Accuracy: 0.9341\n",
      "Epoch 3,\n",
      " Train Loss: 0.18800292909145355, Train Accuracy: 0.94315,\n",
      " Test Loss: 0.15496443212032318, Test Accuracy: 0.9495\n",
      "Epoch 4,\n",
      " Train Loss: 0.16082130372524261, Train Accuracy: 0.95095,\n",
      " Test Loss: 0.15591445565223694, Test Accuracy: 0.9544\n",
      "Epoch 5,\n",
      " Train Loss: 0.15711428225040436, Train Accuracy: 0.9520333333333333,\n",
      " Test Loss: 0.17750826478004456, Test Accuracy: 0.9482\n",
      "Epoch 6,\n",
      " Train Loss: 0.15193159878253937, Train Accuracy: 0.9534166666666667,\n",
      " Test Loss: 0.14242330193519592, Test Accuracy: 0.9582\n",
      "Epoch 7,\n",
      " Train Loss: 0.14036166667938232, Train Accuracy: 0.95735,\n",
      " Test Loss: 0.17662982642650604, Test Accuracy: 0.9522\n",
      "Epoch 8,\n",
      " Train Loss: 0.1300608217716217, Train Accuracy: 0.9600833333333333,\n",
      " Test Loss: 0.1579032838344574, Test Accuracy: 0.9568\n",
      "Epoch 9,\n",
      " Train Loss: 0.1346549540758133, Train Accuracy: 0.9595,\n",
      " Test Loss: 0.16869604587554932, Test Accuracy: 0.9519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.12892749905586243, Train Accuracy: 0.96085,\n",
      " Test Loss: 0.16970603168010712, Test Accuracy: 0.9532\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.9578819274902344, Train Accuracy: 0.7263666666666667,\n",
      " Test Loss: 0.46597105264663696, Test Accuracy: 0.8601\n",
      "Epoch 2,\n",
      " Train Loss: 0.3893474042415619, Train Accuracy: 0.8868833333333334,\n",
      " Test Loss: 0.5238251686096191, Test Accuracy: 0.8566\n",
      "Epoch 3,\n",
      " Train Loss: 0.31454890966415405, Train Accuracy: 0.9118166666666667,\n",
      " Test Loss: 0.26547911763191223, Test Accuracy: 0.9231\n",
      "Epoch 4,\n",
      " Train Loss: 0.28640222549438477, Train Accuracy: 0.92375,\n",
      " Test Loss: 0.2651086747646332, Test Accuracy: 0.9336\n",
      "Epoch 5,\n",
      " Train Loss: 0.264165997505188, Train Accuracy: 0.9313666666666667,\n",
      " Test Loss: 0.24513807892799377, Test Accuracy: 0.9364\n",
      "Epoch 6,\n",
      " Train Loss: 0.2493525892496109, Train Accuracy: 0.9349833333333334,\n",
      " Test Loss: 0.35216420888900757, Test Accuracy: 0.9151\n",
      "Epoch 7,\n",
      " Train Loss: 0.2522735297679901, Train Accuracy: 0.9361333333333334,\n",
      " Test Loss: 0.3253270983695984, Test Accuracy: 0.9244\n",
      "Epoch 8,\n",
      " Train Loss: 0.24636487662792206, Train Accuracy: 0.93765,\n",
      " Test Loss: 0.2756894528865814, Test Accuracy: 0.9374\n",
      "Epoch 9,\n",
      " Train Loss: 0.23861008882522583, Train Accuracy: 0.9400166666666666,\n",
      " Test Loss: 0.26642894744873047, Test Accuracy: 0.9334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.23448051512241364, Train Accuracy: 0.94245,\n",
      " Test Loss: 0.440024197101593, Test Accuracy: 0.9135\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.7663000226020813, Train Accuracy: 0.7916166666666666,\n",
      " Test Loss: 0.3953933119773865, Test Accuracy: 0.8886\n",
      "Epoch 2,\n",
      " Train Loss: 0.373279869556427, Train Accuracy: 0.89075,\n",
      " Test Loss: 0.3212815821170807, Test Accuracy: 0.9073\n",
      "Epoch 3,\n",
      " Train Loss: 0.3192899227142334, Train Accuracy: 0.9057333333333333,\n",
      " Test Loss: 0.2917693257331848, Test Accuracy: 0.9125\n",
      "Epoch 4,\n",
      " Train Loss: 0.28805118799209595, Train Accuracy: 0.9158,\n",
      " Test Loss: 0.27460893988609314, Test Accuracy: 0.9225\n",
      "Epoch 5,\n",
      " Train Loss: 0.2651546001434326, Train Accuracy: 0.92285,\n",
      " Test Loss: 0.25102880597114563, Test Accuracy: 0.9248\n",
      "Epoch 6,\n",
      " Train Loss: 0.2465520054101944, Train Accuracy: 0.9279166666666666,\n",
      " Test Loss: 0.23649001121520996, Test Accuracy: 0.9293\n",
      "Epoch 7,\n",
      " Train Loss: 0.2301839292049408, Train Accuracy: 0.9326166666666666,\n",
      " Test Loss: 0.21912197768688202, Test Accuracy: 0.9351\n",
      "Epoch 8,\n",
      " Train Loss: 0.21638306975364685, Train Accuracy: 0.9371333333333334,\n",
      " Test Loss: 0.21023979783058167, Test Accuracy: 0.9396\n",
      "Epoch 9,\n",
      " Train Loss: 0.20366178452968597, Train Accuracy: 0.9408333333333333,\n",
      " Test Loss: 0.19302412867546082, Test Accuracy: 0.9419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.19278515875339508, Train Accuracy: 0.9444,\n",
      " Test Loss: 0.18852844834327698, Test Accuracy: 0.944\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5345009565353394, Train Accuracy: 0.8531,\n",
      " Test Loss: 0.22737523913383484, Test Accuracy: 0.929\n",
      "Epoch 2,\n",
      " Train Loss: 0.2247326523065567, Train Accuracy: 0.93205,\n",
      " Test Loss: 0.2116985321044922, Test Accuracy: 0.934\n",
      "Epoch 3,\n",
      " Train Loss: 0.19370593130588531, Train Accuracy: 0.94015,\n",
      " Test Loss: 0.1756000965833664, Test Accuracy: 0.9453\n",
      "Epoch 4,\n",
      " Train Loss: 0.18170204758644104, Train Accuracy: 0.9436333333333333,\n",
      " Test Loss: 0.17047938704490662, Test Accuracy: 0.9479\n",
      "Epoch 5,\n",
      " Train Loss: 0.16619093716144562, Train Accuracy: 0.9486166666666667,\n",
      " Test Loss: 0.1582580804824829, Test Accuracy: 0.9527\n",
      "Epoch 6,\n",
      " Train Loss: 0.16020728647708893, Train Accuracy: 0.9501333333333334,\n",
      " Test Loss: 0.17569969594478607, Test Accuracy: 0.9483\n",
      "Epoch 7,\n",
      " Train Loss: 0.1625388264656067, Train Accuracy: 0.9489,\n",
      " Test Loss: 0.1758093237876892, Test Accuracy: 0.9463\n",
      "Epoch 8,\n",
      " Train Loss: 0.15417596697807312, Train Accuracy: 0.95275,\n",
      " Test Loss: 0.1544354408979416, Test Accuracy: 0.955\n",
      "Epoch 9,\n",
      " Train Loss: 0.15405744314193726, Train Accuracy: 0.9531166666666666,\n",
      " Test Loss: 0.2420869767665863, Test Accuracy: 0.9244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.15023621916770935, Train Accuracy: 0.9543,\n",
      " Test Loss: 0.19783389568328857, Test Accuracy: 0.9439\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.3119279146194458, Train Accuracy: 0.7394333333333334,\n",
      " Test Loss: 0.36318355798721313, Test Accuracy: 0.8907\n",
      "Epoch 2,\n",
      " Train Loss: 0.3600253164768219, Train Accuracy: 0.8949833333333334,\n",
      " Test Loss: 0.29152363538742065, Test Accuracy: 0.9085\n",
      "Epoch 3,\n",
      " Train Loss: 0.2816779911518097, Train Accuracy: 0.9196,\n",
      " Test Loss: 0.9751049876213074, Test Accuracy: 0.7691\n",
      "Epoch 4,\n",
      " Train Loss: 0.2598005533218384, Train Accuracy: 0.9271166666666667,\n",
      " Test Loss: 0.5175173282623291, Test Accuracy: 0.8715\n",
      "Epoch 5,\n",
      " Train Loss: 0.23775380849838257, Train Accuracy: 0.93475,\n",
      " Test Loss: 0.3384956419467926, Test Accuracy: 0.8985\n",
      "Epoch 6,\n",
      " Train Loss: 0.2282402217388153, Train Accuracy: 0.9375,\n",
      " Test Loss: 0.3351843059062958, Test Accuracy: 0.9201\n",
      "Epoch 7,\n",
      " Train Loss: 0.22111567854881287, Train Accuracy: 0.9402,\n",
      " Test Loss: 0.6609203219413757, Test Accuracy: 0.8878\n",
      "Epoch 8,\n",
      " Train Loss: 0.22341668605804443, Train Accuracy: 0.9411833333333334,\n",
      " Test Loss: 0.2774120271205902, Test Accuracy: 0.935\n",
      "Epoch 9,\n",
      " Train Loss: 0.21563541889190674, Train Accuracy: 0.9438166666666666,\n",
      " Test Loss: 0.3221169412136078, Test Accuracy: 0.9257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.21765461564064026, Train Accuracy: 0.9438333333333333,\n",
      " Test Loss: 0.30525094270706177, Test Accuracy: 0.9397\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.6750710606575012, Train Accuracy: 0.8186833333333333,\n",
      " Test Loss: 0.33627620339393616, Test Accuracy: 0.9045\n",
      "Epoch 2,\n",
      " Train Loss: 0.31896960735321045, Train Accuracy: 0.9066833333333333,\n",
      " Test Loss: 0.2908762991428375, Test Accuracy: 0.9141\n",
      "Epoch 3,\n",
      " Train Loss: 0.2670247256755829, Train Accuracy: 0.9218666666666666,\n",
      " Test Loss: 0.276119202375412, Test Accuracy: 0.9183\n",
      "Epoch 4,\n",
      " Train Loss: 0.23548699915409088, Train Accuracy: 0.9311,\n",
      " Test Loss: 0.2241857349872589, Test Accuracy: 0.9343\n",
      "Epoch 5,\n",
      " Train Loss: 0.21087941527366638, Train Accuracy: 0.9378333333333333,\n",
      " Test Loss: 0.2065882682800293, Test Accuracy: 0.9401\n",
      "Epoch 6,\n",
      " Train Loss: 0.19196008145809174, Train Accuracy: 0.9435,\n",
      " Test Loss: 0.18711499869823456, Test Accuracy: 0.9435\n",
      "Epoch 7,\n",
      " Train Loss: 0.17677874863147736, Train Accuracy: 0.9486666666666667,\n",
      " Test Loss: 0.17212513089179993, Test Accuracy: 0.9473\n",
      "Epoch 8,\n",
      " Train Loss: 0.16273236274719238, Train Accuracy: 0.9524,\n",
      " Test Loss: 0.18174512684345245, Test Accuracy: 0.9459\n",
      "Epoch 9,\n",
      " Train Loss: 0.1515180915594101, Train Accuracy: 0.9561666666666667,\n",
      " Test Loss: 0.15859682857990265, Test Accuracy: 0.9514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.1407018005847931, Train Accuracy: 0.9595,\n",
      " Test Loss: 0.15010115504264832, Test Accuracy: 0.9547\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.6781414151191711, Train Accuracy: 0.8393333333333334,\n",
      " Test Loss: 0.21460957825183868, Test Accuracy: 0.9333\n",
      "Epoch 2,\n",
      " Train Loss: 0.19550058245658875, Train Accuracy: 0.9400833333333334,\n",
      " Test Loss: 0.17800958454608917, Test Accuracy: 0.9469\n",
      "Epoch 3,\n",
      " Train Loss: 0.15939445793628693, Train Accuracy: 0.9515833333333333,\n",
      " Test Loss: 0.16875125467777252, Test Accuracy: 0.9473\n",
      "Epoch 4,\n",
      " Train Loss: 0.14258745312690735, Train Accuracy: 0.9562166666666667,\n",
      " Test Loss: 0.14563751220703125, Test Accuracy: 0.9573\n",
      "Epoch 5,\n",
      " Train Loss: 0.1279086321592331, Train Accuracy: 0.96065,\n",
      " Test Loss: 0.15126562118530273, Test Accuracy: 0.9555\n",
      "Epoch 6,\n",
      " Train Loss: 0.12170754373073578, Train Accuracy: 0.9629166666666666,\n",
      " Test Loss: 0.14830121397972107, Test Accuracy: 0.958\n",
      "Epoch 7,\n",
      " Train Loss: 0.12063194066286087, Train Accuracy: 0.9634833333333334,\n",
      " Test Loss: 0.13941681385040283, Test Accuracy: 0.9598\n",
      "Epoch 8,\n",
      " Train Loss: 0.11483438313007355, Train Accuracy: 0.9660166666666666,\n",
      " Test Loss: 0.13187146186828613, Test Accuracy: 0.9655\n",
      "Epoch 9,\n",
      " Train Loss: 0.11038268357515335, Train Accuracy: 0.9671333333333333,\n",
      " Test Loss: 0.12880602478981018, Test Accuracy: 0.9624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.0993478000164032, Train Accuracy: 0.9698333333333333,\n",
      " Test Loss: 0.1527359038591385, Test Accuracy: 0.9592\n",
      "Running experiment with LR: 0.01, Momentum: 0.5, Batch Size: 256, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 2.7107274532318115, Train Accuracy: 0.6777166666666666,\n",
      " Test Loss: 0.6008403897285461, Test Accuracy: 0.8148\n",
      "Epoch 2,\n",
      " Train Loss: 0.42529720067977905, Train Accuracy: 0.8781833333333333,\n",
      " Test Loss: 0.48227277398109436, Test Accuracy: 0.8799\n",
      "Epoch 3,\n",
      " Train Loss: 0.30969884991645813, Train Accuracy: 0.9141166666666667,\n",
      " Test Loss: 0.2634570896625519, Test Accuracy: 0.9249\n",
      "Epoch 4,\n",
      " Train Loss: 0.2774716019630432, Train Accuracy: 0.9259166666666667,\n",
      " Test Loss: 0.27421385049819946, Test Accuracy: 0.9306\n",
      "Epoch 5,\n",
      " Train Loss: 0.26211321353912354, Train Accuracy: 0.9325166666666667,\n",
      " Test Loss: 0.3775169253349304, Test Accuracy: 0.9065\n",
      "Epoch 6,\n",
      " Train Loss: 0.24870707094669342, Train Accuracy: 0.9364833333333333,\n",
      " Test Loss: 0.469226211309433, Test Accuracy: 0.9045\n",
      "Epoch 7,\n",
      " Train Loss: 0.23269520699977875, Train Accuracy: 0.9420333333333333,\n",
      " Test Loss: 0.2736852169036865, Test Accuracy: 0.9294\n",
      "Epoch 8,\n",
      " Train Loss: 0.22172163426876068, Train Accuracy: 0.9467666666666666,\n",
      " Test Loss: 0.29887229204177856, Test Accuracy: 0.9344\n",
      "Epoch 9,\n",
      " Train Loss: 0.21185214817523956, Train Accuracy: 0.9491166666666667,\n",
      " Test Loss: 0.43583011627197266, Test Accuracy: 0.941\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.2056431919336319, Train Accuracy: 0.9511,\n",
      " Test Loss: 0.3318691849708557, Test Accuracy: 0.9296\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.2817288339138031, Train Accuracy: 0.91205,\n",
      " Test Loss: 0.13315331935882568, Test Accuracy: 0.9569\n",
      "Epoch 2,\n",
      " Train Loss: 0.13100847601890564, Train Accuracy: 0.9593,\n",
      " Test Loss: 0.11166063696146011, Test Accuracy: 0.9659\n",
      "Epoch 3,\n",
      " Train Loss: 0.0958791971206665, Train Accuracy: 0.96955,\n",
      " Test Loss: 0.10642749071121216, Test Accuracy: 0.965\n",
      "Epoch 4,\n",
      " Train Loss: 0.07418001443147659, Train Accuracy: 0.9762166666666666,\n",
      " Test Loss: 0.0860985592007637, Test Accuracy: 0.974\n",
      "Epoch 5,\n",
      " Train Loss: 0.06006402149796486, Train Accuracy: 0.9803166666666666,\n",
      " Test Loss: 0.09143757075071335, Test Accuracy: 0.974\n",
      "Epoch 6,\n",
      " Train Loss: 0.04873259365558624, Train Accuracy: 0.9842833333333333,\n",
      " Test Loss: 0.0762849897146225, Test Accuracy: 0.9782\n",
      "Epoch 7,\n",
      " Train Loss: 0.041412804275751114, Train Accuracy: 0.9863666666666666,\n",
      " Test Loss: 0.07849809527397156, Test Accuracy: 0.9783\n",
      "Epoch 8,\n",
      " Train Loss: 0.035949695855379105, Train Accuracy: 0.9879833333333333,\n",
      " Test Loss: 0.08038284629583359, Test Accuracy: 0.9768\n",
      "Epoch 9,\n",
      " Train Loss: 0.027834808453917503, Train Accuracy: 0.9908833333333333,\n",
      " Test Loss: 0.08537648618221283, Test Accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.025276530534029007, Train Accuracy: 0.9911833333333333,\n",
      " Test Loss: 0.0817851573228836, Test Accuracy: 0.9757\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.6047191023826599, Train Accuracy: 0.8256666666666667,\n",
      " Test Loss: 0.4240809381008148, Test Accuracy: 0.8747\n",
      "Epoch 2,\n",
      " Train Loss: 0.42938899993896484, Train Accuracy: 0.8735833333333334,\n",
      " Test Loss: 0.45299863815307617, Test Accuracy: 0.8648\n",
      "Epoch 3,\n",
      " Train Loss: 0.4161868691444397, Train Accuracy: 0.8782333333333333,\n",
      " Test Loss: 0.4221278131008148, Test Accuracy: 0.8787\n",
      "Epoch 4,\n",
      " Train Loss: 0.4109305441379547, Train Accuracy: 0.88135,\n",
      " Test Loss: 0.40457025170326233, Test Accuracy: 0.8782\n",
      "Epoch 5,\n",
      " Train Loss: 0.4049265682697296, Train Accuracy: 0.8824833333333333,\n",
      " Test Loss: 0.37552741169929504, Test Accuracy: 0.8896\n",
      "Epoch 6,\n",
      " Train Loss: 0.3987635374069214, Train Accuracy: 0.8844666666666666,\n",
      " Test Loss: 0.3792259097099304, Test Accuracy: 0.8889\n",
      "Epoch 7,\n",
      " Train Loss: 0.3927556872367859, Train Accuracy: 0.88885,\n",
      " Test Loss: 0.34968164563179016, Test Accuracy: 0.8945\n",
      "Epoch 8,\n",
      " Train Loss: 0.3935627341270447, Train Accuracy: 0.88735,\n",
      " Test Loss: 0.41530323028564453, Test Accuracy: 0.8849\n",
      "Epoch 9,\n",
      " Train Loss: 0.3947438895702362, Train Accuracy: 0.88775,\n",
      " Test Loss: 0.3607831299304962, Test Accuracy: 0.8962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.39019402861595154, Train Accuracy: 0.89045,\n",
      " Test Loss: 0.3974846303462982, Test Accuracy: 0.876\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.828326940536499, Train Accuracy: 0.81105,\n",
      " Test Loss: 0.6048246622085571, Test Accuracy: 0.8583\n",
      "Epoch 2,\n",
      " Train Loss: 0.5848543047904968, Train Accuracy: 0.8574166666666667,\n",
      " Test Loss: 0.5275062322616577, Test Accuracy: 0.8748\n",
      "Epoch 3,\n",
      " Train Loss: 0.6267716884613037, Train Accuracy: 0.8554,\n",
      " Test Loss: 0.5501915216445923, Test Accuracy: 0.8515\n",
      "Epoch 4,\n",
      " Train Loss: 0.6936255693435669, Train Accuracy: 0.8455,\n",
      " Test Loss: 0.5695291757583618, Test Accuracy: 0.8818\n",
      "Epoch 5,\n",
      " Train Loss: 0.7394645810127258, Train Accuracy: 0.8419166666666666,\n",
      " Test Loss: 0.6318132877349854, Test Accuracy: 0.8668\n",
      "Epoch 6,\n",
      " Train Loss: 0.801569938659668, Train Accuracy: 0.8225833333333333,\n",
      " Test Loss: 0.9866711497306824, Test Accuracy: 0.6951\n",
      "Epoch 7,\n",
      " Train Loss: 0.8978652358055115, Train Accuracy: 0.8054833333333333,\n",
      " Test Loss: 0.7991138696670532, Test Accuracy: 0.7844\n",
      "Epoch 8,\n",
      " Train Loss: 0.9798571467399597, Train Accuracy: 0.7872,\n",
      " Test Loss: 1.8347604274749756, Test Accuracy: 0.7179\n",
      "Epoch 9,\n",
      " Train Loss: 0.9961033463478088, Train Accuracy: 0.7793833333333333,\n",
      " Test Loss: 1.0010085105895996, Test Accuracy: 0.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 1.0689791440963745, Train Accuracy: 0.7737666666666667,\n",
      " Test Loss: 1.1888004541397095, Test Accuracy: 0.684\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.2983179986476898, Train Accuracy: 0.9069666666666667,\n",
      " Test Loss: 0.14436590671539307, Test Accuracy: 0.9555\n",
      "Epoch 2,\n",
      " Train Loss: 0.14709241688251495, Train Accuracy: 0.9555833333333333,\n",
      " Test Loss: 0.11645227670669556, Test Accuracy: 0.9648\n",
      "Epoch 3,\n",
      " Train Loss: 0.10987187176942825, Train Accuracy: 0.9657666666666667,\n",
      " Test Loss: 0.1176171824336052, Test Accuracy: 0.9622\n",
      "Epoch 4,\n",
      " Train Loss: 0.0937085971236229, Train Accuracy: 0.9699666666666666,\n",
      " Test Loss: 0.1067609116435051, Test Accuracy: 0.9688\n",
      "Epoch 5,\n",
      " Train Loss: 0.07654693722724915, Train Accuracy: 0.9751166666666666,\n",
      " Test Loss: 0.08428166806697845, Test Accuracy: 0.9739\n",
      "Epoch 6,\n",
      " Train Loss: 0.06709953397512436, Train Accuracy: 0.9782666666666666,\n",
      " Test Loss: 0.08757786452770233, Test Accuracy: 0.9747\n",
      "Epoch 7,\n",
      " Train Loss: 0.05868416279554367, Train Accuracy: 0.9807333333333333,\n",
      " Test Loss: 0.11256658285856247, Test Accuracy: 0.9681\n",
      "Epoch 8,\n",
      " Train Loss: 0.052888721227645874, Train Accuracy: 0.98235,\n",
      " Test Loss: 0.09518971294164658, Test Accuracy: 0.9744\n",
      "Epoch 9,\n",
      " Train Loss: 0.045063480734825134, Train Accuracy: 0.9848,\n",
      " Test Loss: 0.08672232925891876, Test Accuracy: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.04333239793777466, Train Accuracy: 0.9856333333333334,\n",
      " Test Loss: 0.1045236587524414, Test Accuracy: 0.9724\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5034651160240173, Train Accuracy: 0.8517833333333333,\n",
      " Test Loss: 0.38004276156425476, Test Accuracy: 0.8929\n",
      "Epoch 2,\n",
      " Train Loss: 0.38158586621284485, Train Accuracy: 0.8914333333333333,\n",
      " Test Loss: 0.3304156959056854, Test Accuracy: 0.9053\n",
      "Epoch 3,\n",
      " Train Loss: 0.35429662466049194, Train Accuracy: 0.90135,\n",
      " Test Loss: 0.39851492643356323, Test Accuracy: 0.89\n",
      "Epoch 4,\n",
      " Train Loss: 0.346658855676651, Train Accuracy: 0.9042666666666667,\n",
      " Test Loss: 0.29944756627082825, Test Accuracy: 0.9178\n",
      "Epoch 5,\n",
      " Train Loss: 0.3413245379924774, Train Accuracy: 0.9067333333333333,\n",
      " Test Loss: 0.2914056181907654, Test Accuracy: 0.9225\n",
      "Epoch 6,\n",
      " Train Loss: 0.3365970849990845, Train Accuracy: 0.90915,\n",
      " Test Loss: 0.3158620595932007, Test Accuracy: 0.9132\n",
      "Epoch 7,\n",
      " Train Loss: 0.3271220028400421, Train Accuracy: 0.9103833333333333,\n",
      " Test Loss: 0.3164776563644409, Test Accuracy: 0.9163\n",
      "Epoch 8,\n",
      " Train Loss: 0.3313893973827362, Train Accuracy: 0.9118666666666667,\n",
      " Test Loss: 0.28702032566070557, Test Accuracy: 0.9236\n",
      "Epoch 9,\n",
      " Train Loss: 0.3237309157848358, Train Accuracy: 0.9123166666666667,\n",
      " Test Loss: 0.35075223445892334, Test Accuracy: 0.9044\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.3233571946620941, Train Accuracy: 0.9131833333333333,\n",
      " Test Loss: 0.29758578538894653, Test Accuracy: 0.9214\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.7493504285812378, Train Accuracy: 0.8206333333333333,\n",
      " Test Loss: 0.3631921410560608, Test Accuracy: 0.8923\n",
      "Epoch 2,\n",
      " Train Loss: 0.5664629936218262, Train Accuracy: 0.8695666666666667,\n",
      " Test Loss: 0.36117643117904663, Test Accuracy: 0.9023\n",
      "Epoch 3,\n",
      " Train Loss: 0.6250429749488831, Train Accuracy: 0.8647833333333333,\n",
      " Test Loss: 0.49323368072509766, Test Accuracy: 0.8842\n",
      "Epoch 4,\n",
      " Train Loss: 0.6692805886268616, Train Accuracy: 0.8547333333333333,\n",
      " Test Loss: 0.9491513967514038, Test Accuracy: 0.8712\n",
      "Epoch 5,\n",
      " Train Loss: 0.7755541205406189, Train Accuracy: 0.8327333333333333,\n",
      " Test Loss: 0.8904207348823547, Test Accuracy: 0.8489\n",
      "Epoch 6,\n",
      " Train Loss: 0.9324385523796082, Train Accuracy: 0.8080833333333334,\n",
      " Test Loss: 1.277247667312622, Test Accuracy: 0.6814\n",
      "Epoch 7,\n",
      " Train Loss: 0.9660155177116394, Train Accuracy: 0.8133833333333333,\n",
      " Test Loss: 1.1909444332122803, Test Accuracy: 0.848\n",
      "Epoch 8,\n",
      " Train Loss: 0.9939056634902954, Train Accuracy: 0.8001166666666667,\n",
      " Test Loss: 0.9082902073860168, Test Accuracy: 0.807\n",
      "Epoch 9,\n",
      " Train Loss: 1.0916168689727783, Train Accuracy: 0.7762666666666667,\n",
      " Test Loss: 1.392467975616455, Test Accuracy: 0.7886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 1.1424587965011597, Train Accuracy: 0.75395,\n",
      " Test Loss: 1.0902292728424072, Test Accuracy: 0.804\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.27700507640838623, Train Accuracy: 0.9118333333333334,\n",
      " Test Loss: 0.12622107565402985, Test Accuracy: 0.9598\n",
      "Epoch 2,\n",
      " Train Loss: 0.13195520639419556, Train Accuracy: 0.9587833333333333,\n",
      " Test Loss: 0.12326878309249878, Test Accuracy: 0.9578\n",
      "Epoch 3,\n",
      " Train Loss: 0.09523695707321167, Train Accuracy: 0.9690666666666666,\n",
      " Test Loss: 0.10112128406763077, Test Accuracy: 0.9691\n",
      "Epoch 4,\n",
      " Train Loss: 0.07577317208051682, Train Accuracy: 0.97585,\n",
      " Test Loss: 0.09684755653142929, Test Accuracy: 0.97\n",
      "Epoch 5,\n",
      " Train Loss: 0.06303402781486511, Train Accuracy: 0.9792666666666666,\n",
      " Test Loss: 0.07360079139471054, Test Accuracy: 0.9783\n",
      "Epoch 6,\n",
      " Train Loss: 0.05019427090883255, Train Accuracy: 0.9835833333333334,\n",
      " Test Loss: 0.08180774003267288, Test Accuracy: 0.9766\n",
      "Epoch 7,\n",
      " Train Loss: 0.04329370707273483, Train Accuracy: 0.9857333333333334,\n",
      " Test Loss: 0.10740182548761368, Test Accuracy: 0.9685\n",
      "Epoch 8,\n",
      " Train Loss: 0.03560139983892441, Train Accuracy: 0.9880666666666666,\n",
      " Test Loss: 0.08563167601823807, Test Accuracy: 0.9748\n",
      "Epoch 9,\n",
      " Train Loss: 0.03267174959182739, Train Accuracy: 0.9890333333333333,\n",
      " Test Loss: 0.08221503347158432, Test Accuracy: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.02713114395737648, Train Accuracy: 0.99115,\n",
      " Test Loss: 0.07725093513727188, Test Accuracy: 0.9808\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5016807317733765, Train Accuracy: 0.8609666666666667,\n",
      " Test Loss: 0.28551816940307617, Test Accuracy: 0.918\n",
      "Epoch 2,\n",
      " Train Loss: 0.3732762336730957, Train Accuracy: 0.8985333333333333,\n",
      " Test Loss: 0.35394585132598877, Test Accuracy: 0.9014\n",
      "Epoch 3,\n",
      " Train Loss: 0.31919342279434204, Train Accuracy: 0.91425,\n",
      " Test Loss: 0.29951056838035583, Test Accuracy: 0.9215\n",
      "Epoch 4,\n",
      " Train Loss: 0.2978907823562622, Train Accuracy: 0.9236333333333333,\n",
      " Test Loss: 0.2697223424911499, Test Accuracy: 0.9307\n",
      "Epoch 5,\n",
      " Train Loss: 0.2983187437057495, Train Accuracy: 0.92435,\n",
      " Test Loss: 0.2741621732711792, Test Accuracy: 0.9293\n",
      "Epoch 6,\n",
      " Train Loss: 0.3019579350948334, Train Accuracy: 0.9242666666666667,\n",
      " Test Loss: 0.29651156067848206, Test Accuracy: 0.9233\n",
      "Epoch 7,\n",
      " Train Loss: 0.3364061415195465, Train Accuracy: 0.9145333333333333,\n",
      " Test Loss: 0.2606428265571594, Test Accuracy: 0.9331\n",
      "Epoch 8,\n",
      " Train Loss: 0.27997416257858276, Train Accuracy: 0.9287333333333333,\n",
      " Test Loss: 0.33112475275993347, Test Accuracy: 0.9172\n",
      "Epoch 9,\n",
      " Train Loss: 0.2784411907196045, Train Accuracy: 0.9295666666666667,\n",
      " Test Loss: 0.2589735686779022, Test Accuracy: 0.9303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.2784717381000519, Train Accuracy: 0.9301166666666667,\n",
      " Test Loss: 0.27324017882347107, Test Accuracy: 0.928\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.939466655254364, Train Accuracy: 0.8043666666666667,\n",
      " Test Loss: 0.8232309818267822, Test Accuracy: 0.8163\n",
      "Epoch 2,\n",
      " Train Loss: 0.659234881401062, Train Accuracy: 0.8525666666666667,\n",
      " Test Loss: 0.8488693237304688, Test Accuracy: 0.8231\n",
      "Epoch 3,\n",
      " Train Loss: 0.8965036273002625, Train Accuracy: 0.8022166666666667,\n",
      " Test Loss: 0.8710495829582214, Test Accuracy: 0.8225\n",
      "Epoch 4,\n",
      " Train Loss: 1.3869010210037231, Train Accuracy: 0.69355,\n",
      " Test Loss: 1.178870677947998, Test Accuracy: 0.575\n",
      "Epoch 5,\n",
      " Train Loss: 2.0998427867889404, Train Accuracy: 0.5306333333333333,\n",
      " Test Loss: 1.700285792350769, Test Accuracy: 0.4615\n",
      "Epoch 6,\n",
      " Train Loss: 2.4180188179016113, Train Accuracy: 0.44615,\n",
      " Test Loss: 1.5429909229278564, Test Accuracy: 0.4902\n",
      "Epoch 7,\n",
      " Train Loss: 2.9681177139282227, Train Accuracy: 0.39421666666666666,\n",
      " Test Loss: 1.9025005102157593, Test Accuracy: 0.3543\n",
      "Epoch 8,\n",
      " Train Loss: 4.80002498626709, Train Accuracy: 0.29446666666666665,\n",
      " Test Loss: 4.2914557456970215, Test Accuracy: 0.2963\n",
      "Epoch 9,\n",
      " Train Loss: 7.705565929412842, Train Accuracy: 0.2462,\n",
      " Test Loss: 2.7782840728759766, Test Accuracy: 0.3102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 5.407295227050781, Train Accuracy: 0.2038,\n",
      " Test Loss: 2.237807512283325, Test Accuracy: 0.1401\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.30178534984588623, Train Accuracy: 0.9061333333333333,\n",
      " Test Loss: 0.16441649198532104, Test Accuracy: 0.9488\n",
      "Epoch 2,\n",
      " Train Loss: 0.14035853743553162, Train Accuracy: 0.9574166666666667,\n",
      " Test Loss: 0.11586853861808777, Test Accuracy: 0.9644\n",
      "Epoch 3,\n",
      " Train Loss: 0.10023131221532822, Train Accuracy: 0.9689666666666666,\n",
      " Test Loss: 0.09184530377388, Test Accuracy: 0.9716\n",
      "Epoch 4,\n",
      " Train Loss: 0.07686209678649902, Train Accuracy: 0.9763166666666667,\n",
      " Test Loss: 0.08182302117347717, Test Accuracy: 0.9749\n",
      "Epoch 5,\n",
      " Train Loss: 0.0649152472615242, Train Accuracy: 0.9796333333333334,\n",
      " Test Loss: 0.08145271241664886, Test Accuracy: 0.9749\n",
      "Epoch 6,\n",
      " Train Loss: 0.05276321992278099, Train Accuracy: 0.9835,\n",
      " Test Loss: 0.07488231360912323, Test Accuracy: 0.9775\n",
      "Epoch 7,\n",
      " Train Loss: 0.04547756165266037, Train Accuracy: 0.9856333333333334,\n",
      " Test Loss: 0.08524186164140701, Test Accuracy: 0.9745\n",
      "Epoch 8,\n",
      " Train Loss: 0.03756953403353691, Train Accuracy: 0.98785,\n",
      " Test Loss: 0.0750378742814064, Test Accuracy: 0.9786\n",
      "Epoch 9,\n",
      " Train Loss: 0.030021045356988907, Train Accuracy: 0.9906666666666667,\n",
      " Test Loss: 0.07471539825201035, Test Accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.027617856860160828, Train Accuracy: 0.99105,\n",
      " Test Loss: 0.07139070332050323, Test Accuracy: 0.9789\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.4661058485507965, Train Accuracy: 0.8784833333333333,\n",
      " Test Loss: 0.29210084676742554, Test Accuracy: 0.9087\n",
      "Epoch 2,\n",
      " Train Loss: 0.26851099729537964, Train Accuracy: 0.9216833333333333,\n",
      " Test Loss: 0.25327757000923157, Test Accuracy: 0.9298\n",
      "Epoch 3,\n",
      " Train Loss: 0.25339022278785706, Train Accuracy: 0.9276166666666666,\n",
      " Test Loss: 0.2862270176410675, Test Accuracy: 0.922\n",
      "Epoch 4,\n",
      " Train Loss: 0.24808305501937866, Train Accuracy: 0.9309666666666667,\n",
      " Test Loss: 0.2533503770828247, Test Accuracy: 0.9343\n",
      "Epoch 5,\n",
      " Train Loss: 0.240597203373909, Train Accuracy: 0.9333166666666667,\n",
      " Test Loss: 0.23120953142642975, Test Accuracy: 0.9365\n",
      "Epoch 6,\n",
      " Train Loss: 0.233598992228508, Train Accuracy: 0.936,\n",
      " Test Loss: 0.28201451897621155, Test Accuracy: 0.9286\n",
      "Epoch 7,\n",
      " Train Loss: 0.23513615131378174, Train Accuracy: 0.9365666666666667,\n",
      " Test Loss: 0.23797695338726044, Test Accuracy: 0.939\n",
      "Epoch 8,\n",
      " Train Loss: 0.22847914695739746, Train Accuracy: 0.9383166666666667,\n",
      " Test Loss: 0.2506987452507019, Test Accuracy: 0.9325\n",
      "Epoch 9,\n",
      " Train Loss: 0.22771351039409637, Train Accuracy: 0.9395166666666667,\n",
      " Test Loss: 0.2313464730978012, Test Accuracy: 0.942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.2243083119392395, Train Accuracy: 0.9394833333333333,\n",
      " Test Loss: 0.24990995228290558, Test Accuracy: 0.9361\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.9610865116119385, Train Accuracy: 0.8077,\n",
      " Test Loss: 1.000645637512207, Test Accuracy: 0.7746\n",
      "Epoch 2,\n",
      " Train Loss: 0.4610976278781891, Train Accuracy: 0.8926333333333333,\n",
      " Test Loss: 0.5379407405853271, Test Accuracy: 0.8684\n",
      "Epoch 3,\n",
      " Train Loss: 0.4346182644367218, Train Accuracy: 0.9043166666666667,\n",
      " Test Loss: 0.4934428930282593, Test Accuracy: 0.8854\n",
      "Epoch 4,\n",
      " Train Loss: 0.44104400277137756, Train Accuracy: 0.90715,\n",
      " Test Loss: 0.41088050603866577, Test Accuracy: 0.9011\n",
      "Epoch 5,\n",
      " Train Loss: 0.4428965449333191, Train Accuracy: 0.9038,\n",
      " Test Loss: 0.5401468276977539, Test Accuracy: 0.9088\n",
      "Epoch 6,\n",
      " Train Loss: 0.4626641869544983, Train Accuracy: 0.9008166666666667,\n",
      " Test Loss: 0.4394032657146454, Test Accuracy: 0.9209\n",
      "Epoch 7,\n",
      " Train Loss: 0.46191179752349854, Train Accuracy: 0.9015666666666666,\n",
      " Test Loss: 1.0867395401000977, Test Accuracy: 0.8731\n",
      "Epoch 8,\n",
      " Train Loss: 0.49003443121910095, Train Accuracy: 0.8934666666666666,\n",
      " Test Loss: 0.9525143504142761, Test Accuracy: 0.8386\n",
      "Epoch 9,\n",
      " Train Loss: 0.5448065400123596, Train Accuracy: 0.8882833333333333,\n",
      " Test Loss: 0.4064948558807373, Test Accuracy: 0.8983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.5672844648361206, Train Accuracy: 0.8888833333333334,\n",
      " Test Loss: 0.812785267829895, Test Accuracy: 0.8894\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.32803118228912354, Train Accuracy: 0.8971,\n",
      " Test Loss: 0.16833598911762238, Test Accuracy: 0.9475\n",
      "Epoch 2,\n",
      " Train Loss: 0.1535012125968933, Train Accuracy: 0.9533833333333334,\n",
      " Test Loss: 0.1409536749124527, Test Accuracy: 0.9552\n",
      "Epoch 3,\n",
      " Train Loss: 0.11591652780771255, Train Accuracy: 0.9647,\n",
      " Test Loss: 0.10193382203578949, Test Accuracy: 0.9677\n",
      "Epoch 4,\n",
      " Train Loss: 0.09371347725391388, Train Accuracy: 0.9709,\n",
      " Test Loss: 0.10566137731075287, Test Accuracy: 0.9652\n",
      "Epoch 5,\n",
      " Train Loss: 0.0778837651014328, Train Accuracy: 0.9760166666666666,\n",
      " Test Loss: 0.10225458443164825, Test Accuracy: 0.969\n",
      "Epoch 6,\n",
      " Train Loss: 0.06664211302995682, Train Accuracy: 0.97995,\n",
      " Test Loss: 0.10586290061473846, Test Accuracy: 0.9672\n",
      "Epoch 7,\n",
      " Train Loss: 0.05777575448155403, Train Accuracy: 0.9818,\n",
      " Test Loss: 0.09584739804267883, Test Accuracy: 0.9715\n",
      "Epoch 8,\n",
      " Train Loss: 0.05021943151950836, Train Accuracy: 0.9840666666666666,\n",
      " Test Loss: 0.09070724248886108, Test Accuracy: 0.9734\n",
      "Epoch 9,\n",
      " Train Loss: 0.043442752212285995, Train Accuracy: 0.9861166666666666,\n",
      " Test Loss: 0.08527836948633194, Test Accuracy: 0.9753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.04057373106479645, Train Accuracy: 0.9866333333333334,\n",
      " Test Loss: 0.08967064321041107, Test Accuracy: 0.9737\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.39944809675216675, Train Accuracy: 0.8812666666666666,\n",
      " Test Loss: 0.35797151923179626, Test Accuracy: 0.8919\n",
      "Epoch 2,\n",
      " Train Loss: 0.24674548208713531, Train Accuracy: 0.9284166666666667,\n",
      " Test Loss: 0.2570433020591736, Test Accuracy: 0.9224\n",
      "Epoch 3,\n",
      " Train Loss: 0.23352757096290588, Train Accuracy: 0.9348166666666666,\n",
      " Test Loss: 0.2364083081483841, Test Accuracy: 0.9327\n",
      "Epoch 4,\n",
      " Train Loss: 0.23267033696174622, Train Accuracy: 0.9357333333333333,\n",
      " Test Loss: 0.20383375883102417, Test Accuracy: 0.9416\n",
      "Epoch 5,\n",
      " Train Loss: 0.222186878323555, Train Accuracy: 0.9397333333333333,\n",
      " Test Loss: 0.20381462574005127, Test Accuracy: 0.9506\n",
      "Epoch 6,\n",
      " Train Loss: 0.21404413878917694, Train Accuracy: 0.9434,\n",
      " Test Loss: 0.20125116407871246, Test Accuracy: 0.9503\n",
      "Epoch 7,\n",
      " Train Loss: 0.20717290043830872, Train Accuracy: 0.9453666666666667,\n",
      " Test Loss: 0.1925911009311676, Test Accuracy: 0.9508\n",
      "Epoch 8,\n",
      " Train Loss: 0.20601044595241547, Train Accuracy: 0.946,\n",
      " Test Loss: 0.1953076869249344, Test Accuracy: 0.9458\n",
      "Epoch 9,\n",
      " Train Loss: 0.20733006298542023, Train Accuracy: 0.9461,\n",
      " Test Loss: 0.2494538426399231, Test Accuracy: 0.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.19635191559791565, Train Accuracy: 0.9494,\n",
      " Test Loss: 0.22717435657978058, Test Accuracy: 0.9427\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.7270243167877197, Train Accuracy: 0.8270833333333333,\n",
      " Test Loss: 0.35711508989334106, Test Accuracy: 0.9053\n",
      "Epoch 2,\n",
      " Train Loss: 0.41351884603500366, Train Accuracy: 0.89375,\n",
      " Test Loss: 0.5064796209335327, Test Accuracy: 0.8814\n",
      "Epoch 3,\n",
      " Train Loss: 0.41158586740493774, Train Accuracy: 0.8990166666666667,\n",
      " Test Loss: 0.8148283958435059, Test Accuracy: 0.8408\n",
      "Epoch 4,\n",
      " Train Loss: 0.3996581435203552, Train Accuracy: 0.9052333333333333,\n",
      " Test Loss: 0.4128948152065277, Test Accuracy: 0.9074\n",
      "Epoch 5,\n",
      " Train Loss: 0.4077828526496887, Train Accuracy: 0.9050333333333334,\n",
      " Test Loss: 0.41476455330848694, Test Accuracy: 0.8999\n",
      "Epoch 6,\n",
      " Train Loss: 0.42501798272132874, Train Accuracy: 0.9063333333333333,\n",
      " Test Loss: 0.5361843109130859, Test Accuracy: 0.8898\n",
      "Epoch 7,\n",
      " Train Loss: 0.4547037184238434, Train Accuracy: 0.9033333333333333,\n",
      " Test Loss: 0.5152549743652344, Test Accuracy: 0.8997\n",
      "Epoch 8,\n",
      " Train Loss: 0.4628199636936188, Train Accuracy: 0.9011166666666667,\n",
      " Test Loss: 0.9916402101516724, Test Accuracy: 0.8326\n",
      "Epoch 9,\n",
      " Train Loss: 0.48214447498321533, Train Accuracy: 0.89855,\n",
      " Test Loss: 0.4746578633785248, Test Accuracy: 0.9103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.4975024461746216, Train Accuracy: 0.8997833333333334,\n",
      " Test Loss: 0.6794858574867249, Test Accuracy: 0.875\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.3055296540260315, Train Accuracy: 0.9036,\n",
      " Test Loss: 0.1576644480228424, Test Accuracy: 0.9491\n",
      "Epoch 2,\n",
      " Train Loss: 0.13089926540851593, Train Accuracy: 0.95915,\n",
      " Test Loss: 0.12907609343528748, Test Accuracy: 0.9602\n",
      "Epoch 3,\n",
      " Train Loss: 0.09369281679391861, Train Accuracy: 0.97075,\n",
      " Test Loss: 0.0864562839269638, Test Accuracy: 0.9727\n",
      "Epoch 4,\n",
      " Train Loss: 0.07199514657258987, Train Accuracy: 0.9781,\n",
      " Test Loss: 0.07345684617757797, Test Accuracy: 0.977\n",
      "Epoch 5,\n",
      " Train Loss: 0.05845145881175995, Train Accuracy: 0.9812333333333333,\n",
      " Test Loss: 0.07185151427984238, Test Accuracy: 0.9779\n",
      "Epoch 6,\n",
      " Train Loss: 0.046240393072366714, Train Accuracy: 0.98515,\n",
      " Test Loss: 0.07689348608255386, Test Accuracy: 0.9763\n",
      "Epoch 7,\n",
      " Train Loss: 0.03763037919998169, Train Accuracy: 0.9880166666666667,\n",
      " Test Loss: 0.07887453585863113, Test Accuracy: 0.9771\n",
      "Epoch 8,\n",
      " Train Loss: 0.030245153233408928, Train Accuracy: 0.9902666666666666,\n",
      " Test Loss: 0.07180967181921005, Test Accuracy: 0.9803\n",
      "Epoch 9,\n",
      " Train Loss: 0.02936667762696743, Train Accuracy: 0.9903,\n",
      " Test Loss: 0.07349127531051636, Test Accuracy: 0.9788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.023071398958563805, Train Accuracy: 0.9923333333333333,\n",
      " Test Loss: 0.06795634329319, Test Accuracy: 0.9804\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.505145788192749, Train Accuracy: 0.8728,\n",
      " Test Loss: 0.3075242340564728, Test Accuracy: 0.9115\n",
      "Epoch 2,\n",
      " Train Loss: 0.2622958719730377, Train Accuracy: 0.9244666666666667,\n",
      " Test Loss: 0.2513831555843353, Test Accuracy: 0.9346\n",
      "Epoch 3,\n",
      " Train Loss: 0.26005157828330994, Train Accuracy: 0.92765,\n",
      " Test Loss: 0.26428937911987305, Test Accuracy: 0.9274\n",
      "Epoch 4,\n",
      " Train Loss: 0.2474309653043747, Train Accuracy: 0.93145,\n",
      " Test Loss: 0.22319462895393372, Test Accuracy: 0.9409\n",
      "Epoch 5,\n",
      " Train Loss: 0.23075321316719055, Train Accuracy: 0.9375666666666667,\n",
      " Test Loss: 0.256751149892807, Test Accuracy: 0.9324\n",
      "Epoch 6,\n",
      " Train Loss: 0.22581800818443298, Train Accuracy: 0.9388166666666666,\n",
      " Test Loss: 0.2848097085952759, Test Accuracy: 0.9273\n",
      "Epoch 7,\n",
      " Train Loss: 0.22018110752105713, Train Accuracy: 0.9433166666666667,\n",
      " Test Loss: 0.21125763654708862, Test Accuracy: 0.9466\n",
      "Epoch 8,\n",
      " Train Loss: 0.2200823277235031, Train Accuracy: 0.9430666666666667,\n",
      " Test Loss: 0.2683941125869751, Test Accuracy: 0.9354\n",
      "Epoch 9,\n",
      " Train Loss: 0.21815010905265808, Train Accuracy: 0.94395,\n",
      " Test Loss: 0.2362968772649765, Test Accuracy: 0.9406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.20834916830062866, Train Accuracy: 0.94675,\n",
      " Test Loss: 0.24965599179267883, Test Accuracy: 0.9418\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.1358355283737183, Train Accuracy: 0.7934333333333333,\n",
      " Test Loss: 0.6792376041412354, Test Accuracy: 0.8385\n",
      "Epoch 2,\n",
      " Train Loss: 0.4430651366710663, Train Accuracy: 0.8904166666666666,\n",
      " Test Loss: 0.3422173857688904, Test Accuracy: 0.9258\n",
      "Epoch 3,\n",
      " Train Loss: 0.45142894983291626, Train Accuracy: 0.89465,\n",
      " Test Loss: 0.7227350473403931, Test Accuracy: 0.8612\n",
      "Epoch 4,\n",
      " Train Loss: 0.48306795954704285, Train Accuracy: 0.89135,\n",
      " Test Loss: 1.3715380430221558, Test Accuracy: 0.8538\n",
      "Epoch 5,\n",
      " Train Loss: 0.5140798687934875, Train Accuracy: 0.8833666666666666,\n",
      " Test Loss: 0.49902811646461487, Test Accuracy: 0.8944\n",
      "Epoch 6,\n",
      " Train Loss: 0.5917367339134216, Train Accuracy: 0.8713,\n",
      " Test Loss: 0.7947571277618408, Test Accuracy: 0.8258\n",
      "Epoch 7,\n",
      " Train Loss: 0.6620243787765503, Train Accuracy: 0.8582833333333333,\n",
      " Test Loss: 1.5945289134979248, Test Accuracy: 0.86\n",
      "Epoch 8,\n",
      " Train Loss: 0.8116645216941833, Train Accuracy: 0.8312166666666667,\n",
      " Test Loss: 0.7744687795639038, Test Accuracy: 0.8014\n",
      "Epoch 9,\n",
      " Train Loss: 0.9261418581008911, Train Accuracy: 0.8124833333333333,\n",
      " Test Loss: 0.8577374219894409, Test Accuracy: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 1.0375224351882935, Train Accuracy: 0.77465,\n",
      " Test Loss: 0.9081137180328369, Test Accuracy: 0.7668\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.36201563477516174, Train Accuracy: 0.8904333333333333,\n",
      " Test Loss: 0.20630447566509247, Test Accuracy: 0.9381\n",
      "Epoch 2,\n",
      " Train Loss: 0.1694849282503128, Train Accuracy: 0.9496333333333333,\n",
      " Test Loss: 0.1427374929189682, Test Accuracy: 0.9564\n",
      "Epoch 3,\n",
      " Train Loss: 0.12312377244234085, Train Accuracy: 0.9622,\n",
      " Test Loss: 0.11185438185930252, Test Accuracy: 0.9663\n",
      "Epoch 4,\n",
      " Train Loss: 0.09900487959384918, Train Accuracy: 0.9702833333333334,\n",
      " Test Loss: 0.10060208290815353, Test Accuracy: 0.9684\n",
      "Epoch 5,\n",
      " Train Loss: 0.08207227289676666, Train Accuracy: 0.9751166666666666,\n",
      " Test Loss: 0.09248470515012741, Test Accuracy: 0.9722\n",
      "Epoch 6,\n",
      " Train Loss: 0.0686502531170845, Train Accuracy: 0.9791833333333333,\n",
      " Test Loss: 0.10459421575069427, Test Accuracy: 0.9696\n",
      "Epoch 7,\n",
      " Train Loss: 0.05983851104974747, Train Accuracy: 0.9817,\n",
      " Test Loss: 0.07909557223320007, Test Accuracy: 0.9744\n",
      "Epoch 8,\n",
      " Train Loss: 0.05198277533054352, Train Accuracy: 0.9842166666666666,\n",
      " Test Loss: 0.07923883199691772, Test Accuracy: 0.9751\n",
      "Epoch 9,\n",
      " Train Loss: 0.044784314930438995, Train Accuracy: 0.9864166666666667,\n",
      " Test Loss: 0.07674197852611542, Test Accuracy: 0.9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.039350539445877075, Train Accuracy: 0.9880833333333333,\n",
      " Test Loss: 0.06878592818975449, Test Accuracy: 0.9796\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5159640908241272, Train Accuracy: 0.8723833333333333,\n",
      " Test Loss: 0.27027732133865356, Test Accuracy: 0.9146\n",
      "Epoch 2,\n",
      " Train Loss: 0.23107808828353882, Train Accuracy: 0.9312166666666667,\n",
      " Test Loss: 0.2053767293691635, Test Accuracy: 0.937\n",
      "Epoch 3,\n",
      " Train Loss: 0.20677579939365387, Train Accuracy: 0.9393833333333333,\n",
      " Test Loss: 0.2058625966310501, Test Accuracy: 0.9391\n",
      "Epoch 4,\n",
      " Train Loss: 0.2041596621274948, Train Accuracy: 0.9392,\n",
      " Test Loss: 0.1909223198890686, Test Accuracy: 0.9431\n",
      "Epoch 5,\n",
      " Train Loss: 0.19658422470092773, Train Accuracy: 0.9421833333333334,\n",
      " Test Loss: 0.17240223288536072, Test Accuracy: 0.9506\n",
      "Epoch 6,\n",
      " Train Loss: 0.19351014494895935, Train Accuracy: 0.9449166666666666,\n",
      " Test Loss: 0.19581103324890137, Test Accuracy: 0.9439\n",
      "Epoch 7,\n",
      " Train Loss: 0.19164590537548065, Train Accuracy: 0.9453666666666667,\n",
      " Test Loss: 0.19731487333774567, Test Accuracy: 0.9462\n",
      "Epoch 8,\n",
      " Train Loss: 0.1828528642654419, Train Accuracy: 0.9478166666666666,\n",
      " Test Loss: 0.19385237991809845, Test Accuracy: 0.9456\n",
      "Epoch 9,\n",
      " Train Loss: 0.17965508997440338, Train Accuracy: 0.9492833333333334,\n",
      " Test Loss: 0.18291659653186798, Test Accuracy: 0.951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.18200267851352692, Train Accuracy: 0.9478833333333333,\n",
      " Test Loss: 0.20064234733581543, Test Accuracy: 0.9427\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.2268177270889282, Train Accuracy: 0.7922833333333333,\n",
      " Test Loss: 0.28623753786087036, Test Accuracy: 0.9183\n",
      "Epoch 2,\n",
      " Train Loss: 0.3595096170902252, Train Accuracy: 0.90075,\n",
      " Test Loss: 0.7259517312049866, Test Accuracy: 0.8223\n",
      "Epoch 3,\n",
      " Train Loss: 0.3375553786754608, Train Accuracy: 0.91455,\n",
      " Test Loss: 0.22850656509399414, Test Accuracy: 0.9377\n",
      "Epoch 4,\n",
      " Train Loss: 0.32085251808166504, Train Accuracy: 0.9185833333333333,\n",
      " Test Loss: 0.2554496228694916, Test Accuracy: 0.9389\n",
      "Epoch 5,\n",
      " Train Loss: 0.30928823351860046, Train Accuracy: 0.9247,\n",
      " Test Loss: 0.27130812406539917, Test Accuracy: 0.9332\n",
      "Epoch 6,\n",
      " Train Loss: 0.3050631582736969, Train Accuracy: 0.9257666666666666,\n",
      " Test Loss: 0.35656970739364624, Test Accuracy: 0.9238\n",
      "Epoch 7,\n",
      " Train Loss: 0.3003222644329071, Train Accuracy: 0.92765,\n",
      " Test Loss: 0.7153280377388, Test Accuracy: 0.8772\n",
      "Epoch 8,\n",
      " Train Loss: 0.29407307505607605, Train Accuracy: 0.9290333333333334,\n",
      " Test Loss: 0.392469197511673, Test Accuracy: 0.8939\n",
      "Epoch 9,\n",
      " Train Loss: 0.28626808524131775, Train Accuracy: 0.9298166666666666,\n",
      " Test Loss: 0.3064603805541992, Test Accuracy: 0.9354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.2812475562095642, Train Accuracy: 0.9332833333333334,\n",
      " Test Loss: 0.29593372344970703, Test Accuracy: 0.9231\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.41374698281288147, Train Accuracy: 0.8734666666666666,\n",
      " Test Loss: 0.2122299075126648, Test Accuracy: 0.9382\n",
      "Epoch 2,\n",
      " Train Loss: 0.19141477346420288, Train Accuracy: 0.9432,\n",
      " Test Loss: 0.16798920929431915, Test Accuracy: 0.9499\n",
      "Epoch 3,\n",
      " Train Loss: 0.14231336116790771, Train Accuracy: 0.9571666666666667,\n",
      " Test Loss: 0.14211373031139374, Test Accuracy: 0.9576\n",
      "Epoch 4,\n",
      " Train Loss: 0.1156894788146019, Train Accuracy: 0.9660666666666666,\n",
      " Test Loss: 0.12216451019048691, Test Accuracy: 0.963\n",
      "Epoch 5,\n",
      " Train Loss: 0.09700740873813629, Train Accuracy: 0.9713,\n",
      " Test Loss: 0.11448881775140762, Test Accuracy: 0.9632\n",
      "Epoch 6,\n",
      " Train Loss: 0.08394722640514374, Train Accuracy: 0.9746666666666667,\n",
      " Test Loss: 0.1013495996594429, Test Accuracy: 0.9706\n",
      "Epoch 7,\n",
      " Train Loss: 0.07450111955404282, Train Accuracy: 0.9769333333333333,\n",
      " Test Loss: 0.09578825533390045, Test Accuracy: 0.9689\n",
      "Epoch 8,\n",
      " Train Loss: 0.06474392116069794, Train Accuracy: 0.97975,\n",
      " Test Loss: 0.08975974470376968, Test Accuracy: 0.9726\n",
      "Epoch 9,\n",
      " Train Loss: 0.059188272804021835, Train Accuracy: 0.9815333333333334,\n",
      " Test Loss: 0.0856972485780716, Test Accuracy: 0.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.05274936556816101, Train Accuracy: 0.9830833333333333,\n",
      " Test Loss: 0.08773025870323181, Test Accuracy: 0.9725\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.4474853575229645, Train Accuracy: 0.87095,\n",
      " Test Loss: 0.21076412498950958, Test Accuracy: 0.9366\n",
      "Epoch 2,\n",
      " Train Loss: 0.22217167913913727, Train Accuracy: 0.9326666666666666,\n",
      " Test Loss: 0.18824154138565063, Test Accuracy: 0.9415\n",
      "Epoch 3,\n",
      " Train Loss: 0.1954953521490097, Train Accuracy: 0.9408333333333333,\n",
      " Test Loss: 0.178031325340271, Test Accuracy: 0.9459\n",
      "Epoch 4,\n",
      " Train Loss: 0.18749693036079407, Train Accuracy: 0.9439,\n",
      " Test Loss: 0.18819619715213776, Test Accuracy: 0.9473\n",
      "Epoch 5,\n",
      " Train Loss: 0.1796865612268448, Train Accuracy: 0.9467,\n",
      " Test Loss: 0.1918649822473526, Test Accuracy: 0.9461\n",
      "Epoch 6,\n",
      " Train Loss: 0.17227071523666382, Train Accuracy: 0.949,\n",
      " Test Loss: 0.16965042054653168, Test Accuracy: 0.9506\n",
      "Epoch 7,\n",
      " Train Loss: 0.17015551030635834, Train Accuracy: 0.9509,\n",
      " Test Loss: 0.20578540861606598, Test Accuracy: 0.9439\n",
      "Epoch 8,\n",
      " Train Loss: 0.16916410624980927, Train Accuracy: 0.9513333333333334,\n",
      " Test Loss: 0.17519141733646393, Test Accuracy: 0.9482\n",
      "Epoch 9,\n",
      " Train Loss: 0.1721118539571762, Train Accuracy: 0.9501166666666667,\n",
      " Test Loss: 0.22169141471385956, Test Accuracy: 0.9424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.17097888886928558, Train Accuracy: 0.9509666666666666,\n",
      " Test Loss: 0.21183761954307556, Test Accuracy: 0.9462\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 0.9439462423324585, Train Accuracy: 0.7980333333333334,\n",
      " Test Loss: 0.6299634575843811, Test Accuracy: 0.8311\n",
      "Epoch 2,\n",
      " Train Loss: 0.3608575463294983, Train Accuracy: 0.89815,\n",
      " Test Loss: 0.5609660148620605, Test Accuracy: 0.8643\n",
      "Epoch 3,\n",
      " Train Loss: 0.32134416699409485, Train Accuracy: 0.9133,\n",
      " Test Loss: 0.4404594302177429, Test Accuracy: 0.8517\n",
      "Epoch 4,\n",
      " Train Loss: 0.313638836145401, Train Accuracy: 0.9177833333333333,\n",
      " Test Loss: 0.2839573919773102, Test Accuracy: 0.9254\n",
      "Epoch 5,\n",
      " Train Loss: 0.31086671352386475, Train Accuracy: 0.9209,\n",
      " Test Loss: 0.3332287073135376, Test Accuracy: 0.9124\n",
      "Epoch 6,\n",
      " Train Loss: 0.3086329996585846, Train Accuracy: 0.9239,\n",
      " Test Loss: 0.30883339047431946, Test Accuracy: 0.9257\n",
      "Epoch 7,\n",
      " Train Loss: 0.31185948848724365, Train Accuracy: 0.92635,\n",
      " Test Loss: 0.39422985911369324, Test Accuracy: 0.9083\n",
      "Epoch 8,\n",
      " Train Loss: 0.30765417218208313, Train Accuracy: 0.9259833333333334,\n",
      " Test Loss: 0.277333527803421, Test Accuracy: 0.9336\n",
      "Epoch 9,\n",
      " Train Loss: 0.3244032561779022, Train Accuracy: 0.9247,\n",
      " Test Loss: 0.35672318935394287, Test Accuracy: 0.9202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.30749940872192383, Train Accuracy: 0.9287333333333333,\n",
      " Test Loss: 0.5230944752693176, Test Accuracy: 0.9097\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.36512628197669983, Train Accuracy: 0.8890833333333333,\n",
      " Test Loss: 0.17442701756954193, Test Accuracy: 0.9455\n",
      "Epoch 2,\n",
      " Train Loss: 0.1522221714258194, Train Accuracy: 0.9539,\n",
      " Test Loss: 0.12954957783222198, Test Accuracy: 0.9599\n",
      "Epoch 3,\n",
      " Train Loss: 0.10851826518774033, Train Accuracy: 0.967,\n",
      " Test Loss: 0.10912389308214188, Test Accuracy: 0.9661\n",
      "Epoch 4,\n",
      " Train Loss: 0.08263477683067322, Train Accuracy: 0.9743833333333334,\n",
      " Test Loss: 0.0855574831366539, Test Accuracy: 0.9711\n",
      "Epoch 5,\n",
      " Train Loss: 0.06596162170171738, Train Accuracy: 0.98005,\n",
      " Test Loss: 0.08347388356924057, Test Accuracy: 0.9747\n",
      "Epoch 6,\n",
      " Train Loss: 0.05387643352150917, Train Accuracy: 0.9828833333333333,\n",
      " Test Loss: 0.08386247605085373, Test Accuracy: 0.974\n",
      "Epoch 7,\n",
      " Train Loss: 0.04475836455821991, Train Accuracy: 0.9866666666666667,\n",
      " Test Loss: 0.06858576834201813, Test Accuracy: 0.9774\n",
      "Epoch 8,\n",
      " Train Loss: 0.03695974126458168, Train Accuracy: 0.9885,\n",
      " Test Loss: 0.06647568196058273, Test Accuracy: 0.9782\n",
      "Epoch 9,\n",
      " Train Loss: 0.031961243599653244, Train Accuracy: 0.99,\n",
      " Test Loss: 0.06328532099723816, Test Accuracy: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.02831614762544632, Train Accuracy: 0.9909166666666667,\n",
      " Test Loss: 0.06699003279209137, Test Accuracy: 0.9787\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.6610254049301147, Train Accuracy: 0.8377333333333333,\n",
      " Test Loss: 0.22882770001888275, Test Accuracy: 0.9321\n",
      "Epoch 2,\n",
      " Train Loss: 0.2258703112602234, Train Accuracy: 0.9320166666666667,\n",
      " Test Loss: 0.23328691720962524, Test Accuracy: 0.9266\n",
      "Epoch 3,\n",
      " Train Loss: 0.19003164768218994, Train Accuracy: 0.9432,\n",
      " Test Loss: 0.17419014871120453, Test Accuracy: 0.9507\n",
      "Epoch 4,\n",
      " Train Loss: 0.1716768890619278, Train Accuracy: 0.9488166666666666,\n",
      " Test Loss: 0.19026823341846466, Test Accuracy: 0.9435\n",
      "Epoch 5,\n",
      " Train Loss: 0.1670950949192047, Train Accuracy: 0.9503666666666667,\n",
      " Test Loss: 0.16800305247306824, Test Accuracy: 0.9539\n",
      "Epoch 6,\n",
      " Train Loss: 0.15521760284900665, Train Accuracy: 0.9544833333333334,\n",
      " Test Loss: 0.16834527254104614, Test Accuracy: 0.9549\n",
      "Epoch 7,\n",
      " Train Loss: 0.1533738225698471, Train Accuracy: 0.9563166666666667,\n",
      " Test Loss: 0.16119341552257538, Test Accuracy: 0.9571\n",
      "Epoch 8,\n",
      " Train Loss: 0.1542518138885498, Train Accuracy: 0.9551666666666667,\n",
      " Test Loss: 0.20370642840862274, Test Accuracy: 0.9475\n",
      "Epoch 9,\n",
      " Train Loss: 0.15459977090358734, Train Accuracy: 0.9553833333333334,\n",
      " Test Loss: 0.193402498960495, Test Accuracy: 0.9518\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.14944471418857574, Train Accuracy: 0.9574166666666667,\n",
      " Test Loss: 0.17525996267795563, Test Accuracy: 0.9535\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.8328124284744263, Train Accuracy: 0.7577833333333334,\n",
      " Test Loss: 0.36279746890068054, Test Accuracy: 0.8977\n",
      "Epoch 2,\n",
      " Train Loss: 0.38151124119758606, Train Accuracy: 0.8994333333333333,\n",
      " Test Loss: 0.3429633378982544, Test Accuracy: 0.9213\n",
      "Epoch 3,\n",
      " Train Loss: 0.33991923928260803, Train Accuracy: 0.9160166666666667,\n",
      " Test Loss: 0.3369187116622925, Test Accuracy: 0.9191\n",
      "Epoch 4,\n",
      " Train Loss: 0.30657118558883667, Train Accuracy: 0.9240166666666667,\n",
      " Test Loss: 0.4995783567428589, Test Accuracy: 0.8974\n",
      "Epoch 5,\n",
      " Train Loss: 0.3018144369125366, Train Accuracy: 0.9283166666666667,\n",
      " Test Loss: 0.25756439566612244, Test Accuracy: 0.9416\n",
      "Epoch 6,\n",
      " Train Loss: 0.32606327533721924, Train Accuracy: 0.9298666666666666,\n",
      " Test Loss: 0.3409169018268585, Test Accuracy: 0.899\n",
      "Epoch 7,\n",
      " Train Loss: 0.3094014823436737, Train Accuracy: 0.9315833333333333,\n",
      " Test Loss: 0.3827437162399292, Test Accuracy: 0.9085\n",
      "Epoch 8,\n",
      " Train Loss: 0.3195131719112396, Train Accuracy: 0.9295166666666667,\n",
      " Test Loss: 0.3225264549255371, Test Accuracy: 0.9326\n",
      "Epoch 9,\n",
      " Train Loss: 0.3372441232204437, Train Accuracy: 0.92525,\n",
      " Test Loss: 0.42319685220718384, Test Accuracy: 0.9275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.37105873227119446, Train Accuracy: 0.9203333333333333,\n",
      " Test Loss: 0.2911459505558014, Test Accuracy: 0.9374\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.4592427611351013, Train Accuracy: 0.8616,\n",
      " Test Loss: 0.23647227883338928, Test Accuracy: 0.9326\n",
      "Epoch 2,\n",
      " Train Loss: 0.22277338802814484, Train Accuracy: 0.9353666666666667,\n",
      " Test Loss: 0.17900176346302032, Test Accuracy: 0.9499\n",
      "Epoch 3,\n",
      " Train Loss: 0.17057041823863983, Train Accuracy: 0.9501333333333334,\n",
      " Test Loss: 0.14992260932922363, Test Accuracy: 0.9585\n",
      "Epoch 4,\n",
      " Train Loss: 0.13874194025993347, Train Accuracy: 0.9594166666666667,\n",
      " Test Loss: 0.12674424052238464, Test Accuracy: 0.9634\n",
      "Epoch 5,\n",
      " Train Loss: 0.11516276746988297, Train Accuracy: 0.96635,\n",
      " Test Loss: 0.11148254573345184, Test Accuracy: 0.9678\n",
      "Epoch 6,\n",
      " Train Loss: 0.10069449245929718, Train Accuracy: 0.9714,\n",
      " Test Loss: 0.10718226432800293, Test Accuracy: 0.9686\n",
      "Epoch 7,\n",
      " Train Loss: 0.08814170956611633, Train Accuracy: 0.9744,\n",
      " Test Loss: 0.09397377818822861, Test Accuracy: 0.9713\n",
      "Epoch 8,\n",
      " Train Loss: 0.0769336149096489, Train Accuracy: 0.9778166666666667,\n",
      " Test Loss: 0.09281539171934128, Test Accuracy: 0.9728\n",
      "Epoch 9,\n",
      " Train Loss: 0.06946965306997299, Train Accuracy: 0.97995,\n",
      " Test Loss: 0.08596872538328171, Test Accuracy: 0.973\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.0625477209687233, Train Accuracy: 0.9821833333333333,\n",
      " Test Loss: 0.08404429256916046, Test Accuracy: 0.9736\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.7386024594306946, Train Accuracy: 0.8368166666666667,\n",
      " Test Loss: 0.21964767575263977, Test Accuracy: 0.9317\n",
      "Epoch 2,\n",
      " Train Loss: 0.20845957100391388, Train Accuracy: 0.9367666666666666,\n",
      " Test Loss: 0.18107977509498596, Test Accuracy: 0.9455\n",
      "Epoch 3,\n",
      " Train Loss: 0.1800435185432434, Train Accuracy: 0.9450166666666666,\n",
      " Test Loss: 0.2394542694091797, Test Accuracy: 0.9255\n",
      "Epoch 4,\n",
      " Train Loss: 0.16706016659736633, Train Accuracy: 0.9492833333333334,\n",
      " Test Loss: 0.21248745918273926, Test Accuracy: 0.9338\n",
      "Epoch 5,\n",
      " Train Loss: 0.15757666528224945, Train Accuracy: 0.9520166666666666,\n",
      " Test Loss: 0.18864387273788452, Test Accuracy: 0.9431\n",
      "Epoch 6,\n",
      " Train Loss: 0.1506710648536682, Train Accuracy: 0.9548,\n",
      " Test Loss: 0.17688389122486115, Test Accuracy: 0.9499\n",
      "Epoch 7,\n",
      " Train Loss: 0.14089688658714294, Train Accuracy: 0.95695,\n",
      " Test Loss: 0.19334149360656738, Test Accuracy: 0.9428\n",
      "Epoch 8,\n",
      " Train Loss: 0.1375747174024582, Train Accuracy: 0.95865,\n",
      " Test Loss: 0.13857083022594452, Test Accuracy: 0.9581\n",
      "Epoch 9,\n",
      " Train Loss: 0.1377364844083786, Train Accuracy: 0.9588666666666666,\n",
      " Test Loss: 0.19969390332698822, Test Accuracy: 0.9454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.1432073414325714, Train Accuracy: 0.9580166666666666,\n",
      " Test Loss: 0.17448458075523376, Test Accuracy: 0.9495\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.9037692546844482, Train Accuracy: 0.7235833333333334,\n",
      " Test Loss: 0.3243711292743683, Test Accuracy: 0.8941\n",
      "Epoch 2,\n",
      " Train Loss: 0.3690741956233978, Train Accuracy: 0.89005,\n",
      " Test Loss: 0.3646772801876068, Test Accuracy: 0.8882\n",
      "Epoch 3,\n",
      " Train Loss: 0.30372345447540283, Train Accuracy: 0.9121333333333334,\n",
      " Test Loss: 0.45647352933883667, Test Accuracy: 0.8552\n",
      "Epoch 4,\n",
      " Train Loss: 0.270668625831604, Train Accuracy: 0.92445,\n",
      " Test Loss: 0.3254310190677643, Test Accuracy: 0.9222\n",
      "Epoch 5,\n",
      " Train Loss: 0.2572050988674164, Train Accuracy: 0.9294666666666667,\n",
      " Test Loss: 0.43451976776123047, Test Accuracy: 0.8978\n",
      "Epoch 6,\n",
      " Train Loss: 0.24070337414741516, Train Accuracy: 0.9355333333333333,\n",
      " Test Loss: 0.3358534276485443, Test Accuracy: 0.9123\n",
      "Epoch 7,\n",
      " Train Loss: 0.2393166720867157, Train Accuracy: 0.9371,\n",
      " Test Loss: 0.3455657660961151, Test Accuracy: 0.8946\n",
      "Epoch 8,\n",
      " Train Loss: 0.22601652145385742, Train Accuracy: 0.9408666666666666,\n",
      " Test Loss: 0.34565168619155884, Test Accuracy: 0.914\n",
      "Epoch 9,\n",
      " Train Loss: 0.22315703332424164, Train Accuracy: 0.94355,\n",
      " Test Loss: 0.312936007976532, Test Accuracy: 0.9412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.22136899828910828, Train Accuracy: 0.9425166666666667,\n",
      " Test Loss: 0.24247288703918457, Test Accuracy: 0.9379\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.5161932706832886, Train Accuracy: 0.84535,\n",
      " Test Loss: 0.2568284571170807, Test Accuracy: 0.9266\n",
      "Epoch 2,\n",
      " Train Loss: 0.2421756535768509, Train Accuracy: 0.9270333333333334,\n",
      " Test Loss: 0.20609712600708008, Test Accuracy: 0.9377\n",
      "Epoch 3,\n",
      " Train Loss: 0.18729963898658752, Train Accuracy: 0.9447666666666666,\n",
      " Test Loss: 0.16916140913963318, Test Accuracy: 0.9497\n",
      "Epoch 4,\n",
      " Train Loss: 0.15609897673130035, Train Accuracy: 0.9542333333333334,\n",
      " Test Loss: 0.1472366601228714, Test Accuracy: 0.9536\n",
      "Epoch 5,\n",
      " Train Loss: 0.132052481174469, Train Accuracy: 0.9604833333333334,\n",
      " Test Loss: 0.13134504854679108, Test Accuracy: 0.9601\n",
      "Epoch 6,\n",
      " Train Loss: 0.11637407541275024, Train Accuracy: 0.9655666666666667,\n",
      " Test Loss: 0.11533449590206146, Test Accuracy: 0.9655\n",
      "Epoch 7,\n",
      " Train Loss: 0.10362157970666885, Train Accuracy: 0.9686166666666667,\n",
      " Test Loss: 0.11260352283716202, Test Accuracy: 0.9655\n",
      "Epoch 8,\n",
      " Train Loss: 0.09209176152944565, Train Accuracy: 0.9726833333333333,\n",
      " Test Loss: 0.10340169817209244, Test Accuracy: 0.9686\n",
      "Epoch 9,\n",
      " Train Loss: 0.08363809436559677, Train Accuracy: 0.9756166666666667,\n",
      " Test Loss: 0.10053254663944244, Test Accuracy: 0.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.07801000028848648, Train Accuracy: 0.9770666666666666,\n",
      " Test Loss: 0.10730723291635513, Test Accuracy: 0.9654\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.5761590600013733, Train Accuracy: 0.8436166666666667,\n",
      " Test Loss: 0.21729402244091034, Test Accuracy: 0.9362\n",
      "Epoch 2,\n",
      " Train Loss: 0.21413762867450714, Train Accuracy: 0.9352,\n",
      " Test Loss: 0.1795637309551239, Test Accuracy: 0.9481\n",
      "Epoch 3,\n",
      " Train Loss: 0.19134753942489624, Train Accuracy: 0.94225,\n",
      " Test Loss: 0.17499950528144836, Test Accuracy: 0.9463\n",
      "Epoch 4,\n",
      " Train Loss: 0.17516453564167023, Train Accuracy: 0.9465833333333333,\n",
      " Test Loss: 0.1773223578929901, Test Accuracy: 0.9455\n",
      "Epoch 5,\n",
      " Train Loss: 0.16023798286914825, Train Accuracy: 0.9513166666666667,\n",
      " Test Loss: 0.1700819730758667, Test Accuracy: 0.9486\n",
      "Epoch 6,\n",
      " Train Loss: 0.1534510850906372, Train Accuracy: 0.9537333333333333,\n",
      " Test Loss: 0.17318479716777802, Test Accuracy: 0.9469\n",
      "Epoch 7,\n",
      " Train Loss: 0.14962086081504822, Train Accuracy: 0.9550833333333333,\n",
      " Test Loss: 0.19090142846107483, Test Accuracy: 0.9405\n",
      "Epoch 8,\n",
      " Train Loss: 0.14204737544059753, Train Accuracy: 0.9582166666666667,\n",
      " Test Loss: 0.17377901077270508, Test Accuracy: 0.949\n",
      "Epoch 9,\n",
      " Train Loss: 0.14994561672210693, Train Accuracy: 0.9545333333333333,\n",
      " Test Loss: 0.1503399908542633, Test Accuracy: 0.9568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.13530489802360535, Train Accuracy: 0.9597666666666667,\n",
      " Test Loss: 0.1569858342409134, Test Accuracy: 0.957\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1.3449556827545166, Train Accuracy: 0.7362833333333333,\n",
      " Test Loss: 0.5476213097572327, Test Accuracy: 0.8321\n",
      "Epoch 2,\n",
      " Train Loss: 0.3801078498363495, Train Accuracy: 0.8861,\n",
      " Test Loss: 0.38809531927108765, Test Accuracy: 0.8795\n",
      "Epoch 3,\n",
      " Train Loss: 0.298642098903656, Train Accuracy: 0.9114,\n",
      " Test Loss: 0.7464389204978943, Test Accuracy: 0.8195\n",
      "Epoch 4,\n",
      " Train Loss: 0.26330098509788513, Train Accuracy: 0.9239833333333334,\n",
      " Test Loss: 0.307046115398407, Test Accuracy: 0.9192\n",
      "Epoch 5,\n",
      " Train Loss: 0.2501350939273834, Train Accuracy: 0.9298333333333333,\n",
      " Test Loss: 0.2179795205593109, Test Accuracy: 0.9388\n",
      "Epoch 6,\n",
      " Train Loss: 0.24493294954299927, Train Accuracy: 0.9331166666666667,\n",
      " Test Loss: 0.5856812596321106, Test Accuracy: 0.8648\n",
      "Epoch 7,\n",
      " Train Loss: 0.23401881754398346, Train Accuracy: 0.9372166666666667,\n",
      " Test Loss: 0.574705958366394, Test Accuracy: 0.8601\n",
      "Epoch 8,\n",
      " Train Loss: 0.22840751707553864, Train Accuracy: 0.9387,\n",
      " Test Loss: 0.23004034161567688, Test Accuracy: 0.942\n",
      "Epoch 9,\n",
      " Train Loss: 0.2237374484539032, Train Accuracy: 0.9410333333333334,\n",
      " Test Loss: 0.3168184161186218, Test Accuracy: 0.915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.22442083060741425, Train Accuracy: 0.9406833333333333,\n",
      " Test Loss: 0.24148544669151306, Test Accuracy: 0.9463\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.44944554567337036, Train Accuracy: 0.8629,\n",
      " Test Loss: 0.244773268699646, Test Accuracy: 0.9261\n",
      "Epoch 2,\n",
      " Train Loss: 0.20032192766666412, Train Accuracy: 0.9397666666666666,\n",
      " Test Loss: 0.15854127705097198, Test Accuracy: 0.9519\n",
      "Epoch 3,\n",
      " Train Loss: 0.1455157846212387, Train Accuracy: 0.9566333333333333,\n",
      " Test Loss: 0.1262180358171463, Test Accuracy: 0.9596\n",
      "Epoch 4,\n",
      " Train Loss: 0.1163199245929718, Train Accuracy: 0.9654,\n",
      " Test Loss: 0.11849156767129898, Test Accuracy: 0.9613\n",
      "Epoch 5,\n",
      " Train Loss: 0.09689541906118393, Train Accuracy: 0.9708833333333333,\n",
      " Test Loss: 0.10459005832672119, Test Accuracy: 0.9648\n",
      "Epoch 6,\n",
      " Train Loss: 0.08120918273925781, Train Accuracy: 0.9756666666666667,\n",
      " Test Loss: 0.09254114329814911, Test Accuracy: 0.9716\n",
      "Epoch 7,\n",
      " Train Loss: 0.06781848520040512, Train Accuracy: 0.9792,\n",
      " Test Loss: 0.08605726063251495, Test Accuracy: 0.9723\n",
      "Epoch 8,\n",
      " Train Loss: 0.05841350927948952, Train Accuracy: 0.9822166666666666,\n",
      " Test Loss: 0.0790528804063797, Test Accuracy: 0.9739\n",
      "Epoch 9,\n",
      " Train Loss: 0.05224793776869774, Train Accuracy: 0.9846333333333334,\n",
      " Test Loss: 0.07875634729862213, Test Accuracy: 0.9739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.044573552906513214, Train Accuracy: 0.9867333333333334,\n",
      " Test Loss: 0.07321055233478546, Test Accuracy: 0.9763\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 0.712449848651886, Train Accuracy: 0.83275,\n",
      " Test Loss: 0.22905607521533966, Test Accuracy: 0.9306\n",
      "Epoch 2,\n",
      " Train Loss: 0.22474631667137146, Train Accuracy: 0.9304166666666667,\n",
      " Test Loss: 0.2360207587480545, Test Accuracy: 0.9293\n",
      "Epoch 3,\n",
      " Train Loss: 0.18908122181892395, Train Accuracy: 0.9417,\n",
      " Test Loss: 0.1857258528470993, Test Accuracy: 0.9439\n",
      "Epoch 4,\n",
      " Train Loss: 0.17125870287418365, Train Accuracy: 0.9474333333333333,\n",
      " Test Loss: 0.18769307434558868, Test Accuracy: 0.9457\n",
      "Epoch 5,\n",
      " Train Loss: 0.15713010728359222, Train Accuracy: 0.9516833333333333,\n",
      " Test Loss: 0.17932140827178955, Test Accuracy: 0.9485\n",
      "Epoch 6,\n",
      " Train Loss: 0.1495867520570755, Train Accuracy: 0.95375,\n",
      " Test Loss: 0.18964987993240356, Test Accuracy: 0.9466\n",
      "Epoch 7,\n",
      " Train Loss: 0.1451667696237564, Train Accuracy: 0.95555,\n",
      " Test Loss: 0.14204856753349304, Test Accuracy: 0.9575\n",
      "Epoch 8,\n",
      " Train Loss: 0.13270512223243713, Train Accuracy: 0.9592833333333334,\n",
      " Test Loss: 0.17807558178901672, Test Accuracy: 0.9511\n",
      "Epoch 9,\n",
      " Train Loss: 0.13277576863765717, Train Accuracy: 0.95975,\n",
      " Test Loss: 0.15552161633968353, Test Accuracy: 0.9529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.13547146320343018, Train Accuracy: 0.958,\n",
      " Test Loss: 0.18106940388679504, Test Accuracy: 0.9477\n",
      "Running experiment with LR: 0.01, Momentum: 0.9, Batch Size: 256, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 2.586928129196167, Train Accuracy: 0.6671833333333334,\n",
      " Test Loss: 0.8298385739326477, Test Accuracy: 0.7655\n",
      "Epoch 2,\n",
      " Train Loss: 0.39305710792541504, Train Accuracy: 0.8837166666666667,\n",
      " Test Loss: 0.5463050603866577, Test Accuracy: 0.8825\n",
      "Epoch 3,\n",
      " Train Loss: 0.31871533393859863, Train Accuracy: 0.91225,\n",
      " Test Loss: 0.22823968529701233, Test Accuracy: 0.9372\n",
      "Epoch 4,\n",
      " Train Loss: 0.27132514119148254, Train Accuracy: 0.9260833333333334,\n",
      " Test Loss: 0.23002943396568298, Test Accuracy: 0.9415\n",
      "Epoch 5,\n",
      " Train Loss: 0.26176518201828003, Train Accuracy: 0.9321333333333334,\n",
      " Test Loss: 0.19792060554027557, Test Accuracy: 0.9403\n",
      "Epoch 6,\n",
      " Train Loss: 0.24631620943546295, Train Accuracy: 0.9354,\n",
      " Test Loss: 0.3267379403114319, Test Accuracy: 0.9206\n",
      "Epoch 7,\n",
      " Train Loss: 0.22796273231506348, Train Accuracy: 0.9405333333333333,\n",
      " Test Loss: 0.23778867721557617, Test Accuracy: 0.9462\n",
      "Epoch 8,\n",
      " Train Loss: 0.22996637225151062, Train Accuracy: 0.9407833333333333,\n",
      " Test Loss: 0.4311869144439697, Test Accuracy: 0.9012\n",
      "Epoch 9,\n",
      " Train Loss: 0.22413519024848938, Train Accuracy: 0.9439333333333333,\n",
      " Test Loss: 0.3153756260871887, Test Accuracy: 0.9323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.2273925095796585, Train Accuracy: 0.9449333333333333,\n",
      " Test Loss: 0.21729488670825958, Test Accuracy: 0.9554\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.3615465760231018, Train Accuracy: 0.8852666666666666,\n",
      " Test Loss: 0.1913190335035324, Test Accuracy: 0.94\n",
      "Epoch 2,\n",
      " Train Loss: 0.176478311419487, Train Accuracy: 0.9455,\n",
      " Test Loss: 0.1512252539396286, Test Accuracy: 0.9519\n",
      "Epoch 3,\n",
      " Train Loss: 0.13600076735019684, Train Accuracy: 0.9583166666666667,\n",
      " Test Loss: 0.14099793136119843, Test Accuracy: 0.9569\n",
      "Epoch 4,\n",
      " Train Loss: 0.12116958200931549, Train Accuracy: 0.9632166666666667,\n",
      " Test Loss: 0.13912498950958252, Test Accuracy: 0.9615\n",
      "Epoch 5,\n",
      " Train Loss: 0.10355380177497864, Train Accuracy: 0.9683,\n",
      " Test Loss: 0.13052505254745483, Test Accuracy: 0.9605\n",
      "Epoch 6,\n",
      " Train Loss: 0.095836341381073, Train Accuracy: 0.97065,\n",
      " Test Loss: 0.15211854875087738, Test Accuracy: 0.9558\n",
      "Epoch 7,\n",
      " Train Loss: 0.08926009386777878, Train Accuracy: 0.9723333333333334,\n",
      " Test Loss: 0.1296059638261795, Test Accuracy: 0.9651\n",
      "Epoch 8,\n",
      " Train Loss: 0.08624754101037979, Train Accuracy: 0.9735333333333334,\n",
      " Test Loss: 0.11256491392850876, Test Accuracy: 0.97\n",
      "Epoch 9,\n",
      " Train Loss: 0.08102428168058395, Train Accuracy: 0.9750833333333333,\n",
      " Test Loss: 0.14517302811145782, Test Accuracy: 0.9583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.0747680813074112, Train Accuracy: 0.9769833333333333,\n",
      " Test Loss: 0.15358349680900574, Test Accuracy: 0.9614\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 4.106475830078125, Train Accuracy: 0.10336666666666666,\n",
      " Test Loss: 2.314080238342285, Test Accuracy: 0.1009\n",
      "Epoch 2,\n",
      " Train Loss: 2.313538074493408, Train Accuracy: 0.1028,\n",
      " Test Loss: 2.3162763118743896, Test Accuracy: 0.0982\n",
      "Epoch 3,\n",
      " Train Loss: 2.3143084049224854, Train Accuracy: 0.10291666666666667,\n",
      " Test Loss: 2.3112521171569824, Test Accuracy: 0.1009\n",
      "Epoch 4,\n",
      " Train Loss: 2.3144891262054443, Train Accuracy: 0.10258333333333333,\n",
      " Test Loss: 2.312030792236328, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.3134212493896484, Train Accuracy: 0.10226666666666667,\n",
      " Test Loss: 2.316093921661377, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.3136940002441406, Train Accuracy: 0.10353333333333334,\n",
      " Test Loss: 2.3117895126342773, Test Accuracy: 0.0958\n",
      "Epoch 7,\n",
      " Train Loss: 2.314126968383789, Train Accuracy: 0.10353333333333334,\n",
      " Test Loss: 2.306142568588257, Test Accuracy: 0.1028\n",
      "Epoch 8,\n",
      " Train Loss: 2.3144781589508057, Train Accuracy: 0.10333333333333333,\n",
      " Test Loss: 2.3054661750793457, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3143908977508545, Train Accuracy: 0.10248333333333333,\n",
      " Test Loss: 2.3089911937713623, Test Accuracy: 0.0982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3144781589508057, Train Accuracy: 0.1025,\n",
      " Test Loss: 2.3117246627807617, Test Accuracy: 0.0958\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 60.508792877197266, Train Accuracy: 0.10545,\n",
      " Test Loss: 2.315972089767456, Test Accuracy: 0.101\n",
      "Epoch 2,\n",
      " Train Loss: 2.7631428241729736, Train Accuracy: 0.10296666666666666,\n",
      " Test Loss: 2.3273024559020996, Test Accuracy: 0.0982\n",
      "Epoch 3,\n",
      " Train Loss: 2.314756393432617, Train Accuracy: 0.1031,\n",
      " Test Loss: 2.316850423812866, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.3151280879974365, Train Accuracy: 0.10193333333333333,\n",
      " Test Loss: 2.3106799125671387, Test Accuracy: 0.101\n",
      "Epoch 5,\n",
      " Train Loss: 2.31447172164917, Train Accuracy: 0.10343333333333334,\n",
      " Test Loss: 2.3191142082214355, Test Accuracy: 0.0958\n",
      "Epoch 6,\n",
      " Train Loss: 2.3149852752685547, Train Accuracy: 0.10223333333333333,\n",
      " Test Loss: 2.3079657554626465, Test Accuracy: 0.1009\n",
      "Epoch 7,\n",
      " Train Loss: 2.314180612564087, Train Accuracy: 0.10403333333333334,\n",
      " Test Loss: 2.3144257068634033, Test Accuracy: 0.1032\n",
      "Epoch 8,\n",
      " Train Loss: 2.315012216567993, Train Accuracy: 0.1028,\n",
      " Test Loss: 2.3193278312683105, Test Accuracy: 0.1032\n",
      "Epoch 9,\n",
      " Train Loss: 2.315587043762207, Train Accuracy: 0.10105,\n",
      " Test Loss: 2.3098230361938477, Test Accuracy: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.314920425415039, Train Accuracy: 0.1039,\n",
      " Test Loss: 2.309756278991699, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.43539634346961975, Train Accuracy: 0.86265,\n",
      " Test Loss: 0.23300650715827942, Test Accuracy: 0.9284\n",
      "Epoch 2,\n",
      " Train Loss: 0.2320738583803177, Train Accuracy: 0.9305666666666667,\n",
      " Test Loss: 0.2279471457004547, Test Accuracy: 0.933\n",
      "Epoch 3,\n",
      " Train Loss: 0.2006547600030899, Train Accuracy: 0.9404333333333333,\n",
      " Test Loss: 0.2588333487510681, Test Accuracy: 0.9232\n",
      "Epoch 4,\n",
      " Train Loss: 0.18111254274845123, Train Accuracy: 0.9461166666666667,\n",
      " Test Loss: 0.2186993509531021, Test Accuracy: 0.9409\n",
      "Epoch 5,\n",
      " Train Loss: 0.16504469513893127, Train Accuracy: 0.9512333333333334,\n",
      " Test Loss: 0.1547541320323944, Test Accuracy: 0.955\n",
      "Epoch 6,\n",
      " Train Loss: 0.15681050717830658, Train Accuracy: 0.9534166666666667,\n",
      " Test Loss: 0.18178248405456543, Test Accuracy: 0.9513\n",
      "Epoch 7,\n",
      " Train Loss: 0.14710527658462524, Train Accuracy: 0.9563666666666667,\n",
      " Test Loss: 0.1379302740097046, Test Accuracy: 0.9606\n",
      "Epoch 8,\n",
      " Train Loss: 0.14164726436138153, Train Accuracy: 0.9576666666666667,\n",
      " Test Loss: 0.19766665995121002, Test Accuracy: 0.9494\n",
      "Epoch 9,\n",
      " Train Loss: 0.13517816364765167, Train Accuracy: 0.95965,\n",
      " Test Loss: 0.18586857616901398, Test Accuracy: 0.9497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.13565228879451752, Train Accuracy: 0.9601666666666666,\n",
      " Test Loss: 0.1737450361251831, Test Accuracy: 0.9582\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 2.9563231468200684, Train Accuracy: 0.10255,\n",
      " Test Loss: 2.3177967071533203, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.3139588832855225, Train Accuracy: 0.1016,\n",
      " Test Loss: 2.307955026626587, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.3142855167388916, Train Accuracy: 0.10326666666666667,\n",
      " Test Loss: 2.3126654624938965, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.31449556350708, Train Accuracy: 0.10211666666666666,\n",
      " Test Loss: 2.314718723297119, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.3144326210021973, Train Accuracy: 0.10313333333333333,\n",
      " Test Loss: 2.3074190616607666, Test Accuracy: 0.098\n",
      "Epoch 6,\n",
      " Train Loss: 2.3137218952178955, Train Accuracy: 0.10391666666666667,\n",
      " Test Loss: 2.308924436569214, Test Accuracy: 0.0982\n",
      "Epoch 7,\n",
      " Train Loss: 2.3138129711151123, Train Accuracy: 0.10203333333333334,\n",
      " Test Loss: 2.304291248321533, Test Accuracy: 0.0958\n",
      "Epoch 8,\n",
      " Train Loss: 2.314495325088501, Train Accuracy: 0.1011,\n",
      " Test Loss: 2.310403347015381, Test Accuracy: 0.1009\n",
      "Epoch 9,\n",
      " Train Loss: 2.3129289150238037, Train Accuracy: 0.10395,\n",
      " Test Loss: 2.3105597496032715, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3147313594818115, Train Accuracy: 0.10338333333333333,\n",
      " Test Loss: 2.3112382888793945, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 15.465666770935059, Train Accuracy: 0.10701666666666666,\n",
      " Test Loss: 2.3185548782348633, Test Accuracy: 0.1009\n",
      "Epoch 2,\n",
      " Train Loss: 2.3148460388183594, Train Accuracy: 0.10373333333333333,\n",
      " Test Loss: 2.3116116523742676, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.3150112628936768, Train Accuracy: 0.10291666666666667,\n",
      " Test Loss: 2.312743663787842, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.3146743774414062, Train Accuracy: 0.10306666666666667,\n",
      " Test Loss: 2.319546699523926, Test Accuracy: 0.1028\n",
      "Epoch 5,\n",
      " Train Loss: 2.314953565597534, Train Accuracy: 0.1028,\n",
      " Test Loss: 2.3185746669769287, Test Accuracy: 0.0982\n",
      "Epoch 6,\n",
      " Train Loss: 2.315354347229004, Train Accuracy: 0.10136666666666666,\n",
      " Test Loss: 2.3102340698242188, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3149404525756836, Train Accuracy: 0.10195,\n",
      " Test Loss: 2.309715509414673, Test Accuracy: 0.1028\n",
      "Epoch 8,\n",
      " Train Loss: 2.315431833267212, Train Accuracy: 0.10093333333333333,\n",
      " Test Loss: 2.312678575515747, Test Accuracy: 0.101\n",
      "Epoch 9,\n",
      " Train Loss: 2.3145368099212646, Train Accuracy: 0.10183333333333333,\n",
      " Test Loss: 2.331594944000244, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3146321773529053, Train Accuracy: 0.1034,\n",
      " Test Loss: 2.305968999862671, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.3822770118713379, Train Accuracy: 0.8779166666666667,\n",
      " Test Loss: 0.17986442148685455, Test Accuracy: 0.9439\n",
      "Epoch 2,\n",
      " Train Loss: 0.1580999493598938, Train Accuracy: 0.9519333333333333,\n",
      " Test Loss: 0.15049448609352112, Test Accuracy: 0.9555\n",
      "Epoch 3,\n",
      " Train Loss: 0.11789081990718842, Train Accuracy: 0.9642166666666667,\n",
      " Test Loss: 0.14729547500610352, Test Accuracy: 0.9554\n",
      "Epoch 4,\n",
      " Train Loss: 0.09852049499750137, Train Accuracy: 0.9696,\n",
      " Test Loss: 0.0960630550980568, Test Accuracy: 0.9719\n",
      "Epoch 5,\n",
      " Train Loss: 0.08215717226266861, Train Accuracy: 0.9746666666666667,\n",
      " Test Loss: 0.13614390790462494, Test Accuracy: 0.962\n",
      "Epoch 6,\n",
      " Train Loss: 0.0675482228398323, Train Accuracy: 0.9794333333333334,\n",
      " Test Loss: 0.1029333770275116, Test Accuracy: 0.9726\n",
      "Epoch 7,\n",
      " Train Loss: 0.06377539783716202, Train Accuracy: 0.98025,\n",
      " Test Loss: 0.0980820432305336, Test Accuracy: 0.9715\n",
      "Epoch 8,\n",
      " Train Loss: 0.057866308838129044, Train Accuracy: 0.981,\n",
      " Test Loss: 0.11305934190750122, Test Accuracy: 0.971\n",
      "Epoch 9,\n",
      " Train Loss: 0.05146237835288048, Train Accuracy: 0.9837166666666667,\n",
      " Test Loss: 0.09411351382732391, Test Accuracy: 0.9757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.04728059470653534, Train Accuracy: 0.9850333333333333,\n",
      " Test Loss: 0.12042374163866043, Test Accuracy: 0.9669\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 14.495162963867188, Train Accuracy: 0.1036,\n",
      " Test Loss: 2.308725357055664, Test Accuracy: 0.098\n",
      "Epoch 2,\n",
      " Train Loss: 2.3145294189453125, Train Accuracy: 0.10245,\n",
      " Test Loss: 2.320945978164673, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.313995838165283, Train Accuracy: 0.10211666666666666,\n",
      " Test Loss: 2.314931869506836, Test Accuracy: 0.1028\n",
      "Epoch 4,\n",
      " Train Loss: 2.313706159591675, Train Accuracy: 0.10186666666666666,\n",
      " Test Loss: 2.310185670852661, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.313375949859619, Train Accuracy: 0.1029,\n",
      " Test Loss: 2.3117778301239014, Test Accuracy: 0.0982\n",
      "Epoch 6,\n",
      " Train Loss: 2.313765525817871, Train Accuracy: 0.10443333333333334,\n",
      " Test Loss: 2.3123202323913574, Test Accuracy: 0.1009\n",
      "Epoch 7,\n",
      " Train Loss: 2.313654661178589, Train Accuracy: 0.10235,\n",
      " Test Loss: 2.310624122619629, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.3145041465759277, Train Accuracy: 0.10273333333333333,\n",
      " Test Loss: 2.3103160858154297, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.314706802368164, Train Accuracy: 0.10121666666666666,\n",
      " Test Loss: 2.3166158199310303, Test Accuracy: 0.0958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3146724700927734, Train Accuracy: 0.10261666666666666,\n",
      " Test Loss: 2.309568166732788, Test Accuracy: 0.0982\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 32, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 331.78814697265625, Train Accuracy: 0.1046,\n",
      " Test Loss: 2.3075995445251465, Test Accuracy: 0.101\n",
      "Epoch 2,\n",
      " Train Loss: 2.3150570392608643, Train Accuracy: 0.09973333333333333,\n",
      " Test Loss: 2.308157444000244, Test Accuracy: 0.1032\n",
      "Epoch 3,\n",
      " Train Loss: 2.3150546550750732, Train Accuracy: 0.10328333333333334,\n",
      " Test Loss: 2.3191826343536377, Test Accuracy: 0.098\n",
      "Epoch 4,\n",
      " Train Loss: 2.315115213394165, Train Accuracy: 0.10161666666666666,\n",
      " Test Loss: 2.30871844291687, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.315009832382202, Train Accuracy: 0.104,\n",
      " Test Loss: 2.3063080310821533, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.314664125442505, Train Accuracy: 0.10351666666666667,\n",
      " Test Loss: 2.3237481117248535, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3151791095733643, Train Accuracy: 0.10138333333333334,\n",
      " Test Loss: 2.311697244644165, Test Accuracy: 0.1028\n",
      "Epoch 8,\n",
      " Train Loss: 2.3141908645629883, Train Accuracy: 0.10241666666666667,\n",
      " Test Loss: 2.317214012145996, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3150219917297363, Train Accuracy: 0.10316666666666667,\n",
      " Test Loss: 2.3094301223754883, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.314413547515869, Train Accuracy: 0.1026,\n",
      " Test Loss: 2.3031187057495117, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.34336549043655396, Train Accuracy: 0.8923,\n",
      " Test Loss: 0.155170276761055, Test Accuracy: 0.9512\n",
      "Epoch 2,\n",
      " Train Loss: 0.1401616334915161, Train Accuracy: 0.9568,\n",
      " Test Loss: 0.12323740124702454, Test Accuracy: 0.9635\n",
      "Epoch 3,\n",
      " Train Loss: 0.10268411785364151, Train Accuracy: 0.9678166666666667,\n",
      " Test Loss: 0.13769975304603577, Test Accuracy: 0.955\n",
      "Epoch 4,\n",
      " Train Loss: 0.08124230057001114, Train Accuracy: 0.9746666666666667,\n",
      " Test Loss: 0.09436257183551788, Test Accuracy: 0.9705\n",
      "Epoch 5,\n",
      " Train Loss: 0.0675082877278328, Train Accuracy: 0.9778666666666667,\n",
      " Test Loss: 0.08783406764268875, Test Accuracy: 0.9732\n",
      "Epoch 6,\n",
      " Train Loss: 0.056649018079042435, Train Accuracy: 0.9823833333333334,\n",
      " Test Loss: 0.14669525623321533, Test Accuracy: 0.9588\n",
      "Epoch 7,\n",
      " Train Loss: 0.050279200077056885, Train Accuracy: 0.98315,\n",
      " Test Loss: 0.10570526123046875, Test Accuracy: 0.9713\n",
      "Epoch 8,\n",
      " Train Loss: 0.04422692209482193, Train Accuracy: 0.98565,\n",
      " Test Loss: 0.08095144480466843, Test Accuracy: 0.9767\n",
      "Epoch 9,\n",
      " Train Loss: 0.037057723850011826, Train Accuracy: 0.9875166666666667,\n",
      " Test Loss: 0.09080397337675095, Test Accuracy: 0.9732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.033098358660936356, Train Accuracy: 0.9886833333333334,\n",
      " Test Loss: 0.0774226188659668, Test Accuracy: 0.9799\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 6.522353649139404, Train Accuracy: 0.10106666666666667,\n",
      " Test Loss: 2.307816982269287, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.3095061779022217, Train Accuracy: 0.10156666666666667,\n",
      " Test Loss: 2.3112590312957764, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.310361862182617, Train Accuracy: 0.10345,\n",
      " Test Loss: 2.3138394355773926, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.3102049827575684, Train Accuracy: 0.1028,\n",
      " Test Loss: 2.303986072540283, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.3105828762054443, Train Accuracy: 0.10505,\n",
      " Test Loss: 2.3088109493255615, Test Accuracy: 0.1009\n",
      "Epoch 6,\n",
      " Train Loss: 2.310537815093994, Train Accuracy: 0.10481666666666667,\n",
      " Test Loss: 2.3079447746276855, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3109848499298096, Train Accuracy: 0.10418333333333334,\n",
      " Test Loss: 2.309049129486084, Test Accuracy: 0.101\n",
      "Epoch 8,\n",
      " Train Loss: 2.310281276702881, Train Accuracy: 0.1035,\n",
      " Test Loss: 2.3080763816833496, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3107731342315674, Train Accuracy: 0.1034,\n",
      " Test Loss: 2.3093390464782715, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.309755325317383, Train Accuracy: 0.10385,\n",
      " Test Loss: 2.309439182281494, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 83.87771606445312, Train Accuracy: 0.12508333333333332,\n",
      " Test Loss: 2.3218400478363037, Test Accuracy: 0.0982\n",
      "Epoch 2,\n",
      " Train Loss: 2.6762771606445312, Train Accuracy: 0.1112,\n",
      " Test Loss: 2.3129429817199707, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.3108744621276855, Train Accuracy: 0.10208333333333333,\n",
      " Test Loss: 2.3152029514312744, Test Accuracy: 0.1028\n",
      "Epoch 4,\n",
      " Train Loss: 2.3105382919311523, Train Accuracy: 0.10315,\n",
      " Test Loss: 2.312638282775879, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.311410427093506, Train Accuracy: 0.10166666666666667,\n",
      " Test Loss: 2.3125879764556885, Test Accuracy: 0.0982\n",
      "Epoch 6,\n",
      " Train Loss: 2.3108513355255127, Train Accuracy: 0.10228333333333334,\n",
      " Test Loss: 2.3136870861053467, Test Accuracy: 0.101\n",
      "Epoch 7,\n",
      " Train Loss: 2.310884475708008, Train Accuracy: 0.10153333333333334,\n",
      " Test Loss: 2.3253166675567627, Test Accuracy: 0.0982\n",
      "Epoch 8,\n",
      " Train Loss: 2.3110413551330566, Train Accuracy: 0.1036,\n",
      " Test Loss: 2.311504364013672, Test Accuracy: 0.1009\n",
      "Epoch 9,\n",
      " Train Loss: 2.3109350204467773, Train Accuracy: 0.10398333333333333,\n",
      " Test Loss: 2.3108959197998047, Test Accuracy: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3107240200042725, Train Accuracy: 0.1021,\n",
      " Test Loss: 2.3185105323791504, Test Accuracy: 0.1009\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.39307689666748047, Train Accuracy: 0.87445,\n",
      " Test Loss: 0.18910911679267883, Test Accuracy: 0.94\n",
      "Epoch 2,\n",
      " Train Loss: 0.17687135934829712, Train Accuracy: 0.94485,\n",
      " Test Loss: 0.15978588163852692, Test Accuracy: 0.9487\n",
      "Epoch 3,\n",
      " Train Loss: 0.13772453367710114, Train Accuracy: 0.9572833333333334,\n",
      " Test Loss: 0.15742582082748413, Test Accuracy: 0.9495\n",
      "Epoch 4,\n",
      " Train Loss: 0.11301317065954208, Train Accuracy: 0.9642666666666667,\n",
      " Test Loss: 0.13036395609378815, Test Accuracy: 0.9594\n",
      "Epoch 5,\n",
      " Train Loss: 0.09727290272712708, Train Accuracy: 0.9683333333333334,\n",
      " Test Loss: 0.11613940447568893, Test Accuracy: 0.9635\n",
      "Epoch 6,\n",
      " Train Loss: 0.08363118022680283, Train Accuracy: 0.9738833333333333,\n",
      " Test Loss: 0.15641866624355316, Test Accuracy: 0.9542\n",
      "Epoch 7,\n",
      " Train Loss: 0.07663941383361816, Train Accuracy: 0.9748166666666667,\n",
      " Test Loss: 0.1153564602136612, Test Accuracy: 0.964\n",
      "Epoch 8,\n",
      " Train Loss: 0.06980263441801071, Train Accuracy: 0.9773333333333334,\n",
      " Test Loss: 0.11719899624586105, Test Accuracy: 0.9645\n",
      "Epoch 9,\n",
      " Train Loss: 0.06450477242469788, Train Accuracy: 0.9786833333333333,\n",
      " Test Loss: 0.10005035251379013, Test Accuracy: 0.9709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.055971693247556686, Train Accuracy: 0.9818,\n",
      " Test Loss: 0.09688135981559753, Test Accuracy: 0.9708\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 4.070107460021973, Train Accuracy: 0.1189,\n",
      " Test Loss: 2.307185173034668, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.321625232696533, Train Accuracy: 0.10316666666666667,\n",
      " Test Loss: 2.3054358959198, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.290705919265747, Train Accuracy: 0.11358333333333333,\n",
      " Test Loss: 2.305865526199341, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.3108789920806885, Train Accuracy: 0.10278333333333334,\n",
      " Test Loss: 2.308612108230591, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.3104052543640137, Train Accuracy: 0.10135,\n",
      " Test Loss: 2.307772636413574, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.3108134269714355, Train Accuracy: 0.10135,\n",
      " Test Loss: 2.3118526935577393, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3104896545410156, Train Accuracy: 0.10301666666666667,\n",
      " Test Loss: 2.305786371231079, Test Accuracy: 0.1028\n",
      "Epoch 8,\n",
      " Train Loss: 2.3099288940429688, Train Accuracy: 0.10361666666666666,\n",
      " Test Loss: 2.312065362930298, Test Accuracy: 0.1009\n",
      "Epoch 9,\n",
      " Train Loss: 2.3101041316986084, Train Accuracy: 0.10306666666666667,\n",
      " Test Loss: 2.3080856800079346, Test Accuracy: 0.1009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.309600591659546, Train Accuracy: 0.10356666666666667,\n",
      " Test Loss: 2.3105947971343994, Test Accuracy: 0.0982\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 16.07457733154297, Train Accuracy: 0.10811666666666667,\n",
      " Test Loss: 2.31070613861084, Test Accuracy: 0.1028\n",
      "Epoch 2,\n",
      " Train Loss: 2.3111612796783447, Train Accuracy: 0.10301666666666667,\n",
      " Test Loss: 2.3111298084259033, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.3108508586883545, Train Accuracy: 0.1026,\n",
      " Test Loss: 2.3073909282684326, Test Accuracy: 0.101\n",
      "Epoch 4,\n",
      " Train Loss: 2.311218023300171, Train Accuracy: 0.10285,\n",
      " Test Loss: 2.308030366897583, Test Accuracy: 0.1032\n",
      "Epoch 5,\n",
      " Train Loss: 2.3105010986328125, Train Accuracy: 0.1045,\n",
      " Test Loss: 2.3070950508117676, Test Accuracy: 0.0974\n",
      "Epoch 6,\n",
      " Train Loss: 2.310706853866577, Train Accuracy: 0.10401666666666666,\n",
      " Test Loss: 2.308121681213379, Test Accuracy: 0.1009\n",
      "Epoch 7,\n",
      " Train Loss: 2.3111329078674316, Train Accuracy: 0.10506666666666667,\n",
      " Test Loss: 2.308148145675659, Test Accuracy: 0.1032\n",
      "Epoch 8,\n",
      " Train Loss: 2.3110408782958984, Train Accuracy: 0.10273333333333333,\n",
      " Test Loss: 2.315884590148926, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3111650943756104, Train Accuracy: 0.10206666666666667,\n",
      " Test Loss: 2.3141703605651855, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3103556632995605, Train Accuracy: 0.10371666666666667,\n",
      " Test Loss: 2.324172019958496, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.4137970209121704, Train Accuracy: 0.8694166666666666,\n",
      " Test Loss: 0.17517927289009094, Test Accuracy: 0.9443\n",
      "Epoch 2,\n",
      " Train Loss: 0.14664360880851746, Train Accuracy: 0.9543833333333334,\n",
      " Test Loss: 0.1438828408718109, Test Accuracy: 0.9539\n",
      "Epoch 3,\n",
      " Train Loss: 0.10685814917087555, Train Accuracy: 0.96725,\n",
      " Test Loss: 0.09611281752586365, Test Accuracy: 0.9703\n",
      "Epoch 4,\n",
      " Train Loss: 0.0852455198764801, Train Accuracy: 0.97335,\n",
      " Test Loss: 0.08560305833816528, Test Accuracy: 0.9739\n",
      "Epoch 5,\n",
      " Train Loss: 0.06894487142562866, Train Accuracy: 0.97765,\n",
      " Test Loss: 0.10030659288167953, Test Accuracy: 0.9688\n",
      "Epoch 6,\n",
      " Train Loss: 0.05860478803515434, Train Accuracy: 0.9813,\n",
      " Test Loss: 0.09959903359413147, Test Accuracy: 0.9705\n",
      "Epoch 7,\n",
      " Train Loss: 0.04926497861742973, Train Accuracy: 0.98445,\n",
      " Test Loss: 0.08707708865404129, Test Accuracy: 0.9736\n",
      "Epoch 8,\n",
      " Train Loss: 0.04370006173849106, Train Accuracy: 0.9851,\n",
      " Test Loss: 0.09228862076997757, Test Accuracy: 0.9738\n",
      "Epoch 9,\n",
      " Train Loss: 0.03843718394637108, Train Accuracy: 0.9873166666666666,\n",
      " Test Loss: 0.09062482416629791, Test Accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.03085852786898613, Train Accuracy: 0.9890666666666666,\n",
      " Test Loss: 0.0807701051235199, Test Accuracy: 0.9796\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 20.031782150268555, Train Accuracy: 0.10751666666666666,\n",
      " Test Loss: 2.305157423019409, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.3097598552703857, Train Accuracy: 0.10406666666666667,\n",
      " Test Loss: 2.307060956954956, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.309708595275879, Train Accuracy: 0.1029,\n",
      " Test Loss: 2.3095498085021973, Test Accuracy: 0.0982\n",
      "Epoch 4,\n",
      " Train Loss: 2.3094887733459473, Train Accuracy: 0.10288333333333333,\n",
      " Test Loss: 2.3050615787506104, Test Accuracy: 0.1009\n",
      "Epoch 5,\n",
      " Train Loss: 2.3103933334350586, Train Accuracy: 0.10418333333333334,\n",
      " Test Loss: 2.3087761402130127, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.3109867572784424, Train Accuracy: 0.10341666666666667,\n",
      " Test Loss: 2.3063478469848633, Test Accuracy: 0.098\n",
      "Epoch 7,\n",
      " Train Loss: 2.310197591781616, Train Accuracy: 0.10491666666666667,\n",
      " Test Loss: 2.313621997833252, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.3100342750549316, Train Accuracy: 0.10501666666666666,\n",
      " Test Loss: 2.3042657375335693, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3102779388427734, Train Accuracy: 0.10211666666666666,\n",
      " Test Loss: 2.3053300380706787, Test Accuracy: 0.0982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.309640407562256, Train Accuracy: 0.10413333333333333,\n",
      " Test Loss: 2.308960437774658, Test Accuracy: 0.1009\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 64, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1239.182861328125, Train Accuracy: 0.10901666666666666,\n",
      " Test Loss: 2.3189587593078613, Test Accuracy: 0.0958\n",
      "Epoch 2,\n",
      " Train Loss: 2.31118106842041, Train Accuracy: 0.10095,\n",
      " Test Loss: 2.309807777404785, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.3104817867279053, Train Accuracy: 0.10385,\n",
      " Test Loss: 2.3053667545318604, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.310746669769287, Train Accuracy: 0.10151666666666667,\n",
      " Test Loss: 2.308537721633911, Test Accuracy: 0.1028\n",
      "Epoch 5,\n",
      " Train Loss: 2.310915946960449, Train Accuracy: 0.1033,\n",
      " Test Loss: 2.324822187423706, Test Accuracy: 0.101\n",
      "Epoch 6,\n",
      " Train Loss: 2.3107361793518066, Train Accuracy: 0.1046,\n",
      " Test Loss: 2.32224440574646, Test Accuracy: 0.0958\n",
      "Epoch 7,\n",
      " Train Loss: 2.311244010925293, Train Accuracy: 0.10178333333333334,\n",
      " Test Loss: 2.3143885135650635, Test Accuracy: 0.0958\n",
      "Epoch 8,\n",
      " Train Loss: 2.310792922973633, Train Accuracy: 0.10301666666666667,\n",
      " Test Loss: 2.3101675510406494, Test Accuracy: 0.0982\n",
      "Epoch 9,\n",
      " Train Loss: 2.3107542991638184, Train Accuracy: 0.10436666666666666,\n",
      " Test Loss: 2.3076376914978027, Test Accuracy: 0.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.311208486557007, Train Accuracy: 0.10161666666666666,\n",
      " Test Loss: 2.3058977127075195, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.40539634227752686, Train Accuracy: 0.8729833333333333,\n",
      " Test Loss: 0.16714005172252655, Test Accuracy: 0.9479\n",
      "Epoch 2,\n",
      " Train Loss: 0.14863616228103638, Train Accuracy: 0.9546666666666667,\n",
      " Test Loss: 0.11862847954034805, Test Accuracy: 0.9633\n",
      "Epoch 3,\n",
      " Train Loss: 0.1083257868885994, Train Accuracy: 0.9665666666666667,\n",
      " Test Loss: 0.12681464850902557, Test Accuracy: 0.9579\n",
      "Epoch 4,\n",
      " Train Loss: 0.0861835926771164, Train Accuracy: 0.9734666666666667,\n",
      " Test Loss: 0.10615374147891998, Test Accuracy: 0.9681\n",
      "Epoch 5,\n",
      " Train Loss: 0.071324922144413, Train Accuracy: 0.9775333333333334,\n",
      " Test Loss: 0.09124527126550674, Test Accuracy: 0.9709\n",
      "Epoch 6,\n",
      " Train Loss: 0.06024784594774246, Train Accuracy: 0.9807333333333333,\n",
      " Test Loss: 0.08339334279298782, Test Accuracy: 0.9744\n",
      "Epoch 7,\n",
      " Train Loss: 0.05166931077837944, Train Accuracy: 0.9833666666666666,\n",
      " Test Loss: 0.07360697537660599, Test Accuracy: 0.9767\n",
      "Epoch 8,\n",
      " Train Loss: 0.043771762400865555, Train Accuracy: 0.98575,\n",
      " Test Loss: 0.09295864403247833, Test Accuracy: 0.9726\n",
      "Epoch 9,\n",
      " Train Loss: 0.03745828568935394, Train Accuracy: 0.9881166666666666,\n",
      " Test Loss: 0.07044786959886551, Test Accuracy: 0.9781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.03023442253470421, Train Accuracy: 0.9903666666666666,\n",
      " Test Loss: 0.06836014240980148, Test Accuracy: 0.9797\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 11.919622421264648, Train Accuracy: 0.10693333333333334,\n",
      " Test Loss: 2.302393913269043, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.305532932281494, Train Accuracy: 0.10426666666666666,\n",
      " Test Loss: 2.302969217300415, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.3064188957214355, Train Accuracy: 0.10615,\n",
      " Test Loss: 2.3045780658721924, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.3071610927581787, Train Accuracy: 0.1061,\n",
      " Test Loss: 2.302321672439575, Test Accuracy: 0.1009\n",
      "Epoch 5,\n",
      " Train Loss: 2.3065619468688965, Train Accuracy: 0.10341666666666667,\n",
      " Test Loss: 2.30302357673645, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.307140350341797, Train Accuracy: 0.10321666666666666,\n",
      " Test Loss: 2.3038291931152344, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3073501586914062, Train Accuracy: 0.10348333333333333,\n",
      " Test Loss: 2.304823637008667, Test Accuracy: 0.1009\n",
      "Epoch 8,\n",
      " Train Loss: 2.3077104091644287, Train Accuracy: 0.10411666666666666,\n",
      " Test Loss: 2.302107334136963, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3075175285339355, Train Accuracy: 0.10441666666666667,\n",
      " Test Loss: 2.3037314414978027, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.30751371383667, Train Accuracy: 0.1031,\n",
      " Test Loss: 2.303446054458618, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 148.14698791503906, Train Accuracy: 0.13716666666666666,\n",
      " Test Loss: 2.3058526515960693, Test Accuracy: 0.1139\n",
      "Epoch 2,\n",
      " Train Loss: 2.9222192764282227, Train Accuracy: 0.1177,\n",
      " Test Loss: 2.306574821472168, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.3117971420288086, Train Accuracy: 0.1025,\n",
      " Test Loss: 2.3060684204101562, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.308184862136841, Train Accuracy: 0.10355,\n",
      " Test Loss: 2.3100662231445312, Test Accuracy: 0.0982\n",
      "Epoch 5,\n",
      " Train Loss: 2.3084185123443604, Train Accuracy: 0.10336666666666666,\n",
      " Test Loss: 2.3122992515563965, Test Accuracy: 0.0982\n",
      "Epoch 6,\n",
      " Train Loss: 2.308786392211914, Train Accuracy: 0.10251666666666667,\n",
      " Test Loss: 2.3083255290985107, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3078973293304443, Train Accuracy: 0.10491666666666667,\n",
      " Test Loss: 2.306525945663452, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.3078858852386475, Train Accuracy: 0.10595,\n",
      " Test Loss: 2.3024744987487793, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3085317611694336, Train Accuracy: 0.10356666666666667,\n",
      " Test Loss: 2.3120996952056885, Test Accuracy: 0.101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3081133365631104, Train Accuracy: 0.10531666666666667,\n",
      " Test Loss: 2.307091474533081, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.45889952778816223, Train Accuracy: 0.8555666666666667,\n",
      " Test Loss: 0.21039888262748718, Test Accuracy: 0.9327\n",
      "Epoch 2,\n",
      " Train Loss: 0.17836035788059235, Train Accuracy: 0.94585,\n",
      " Test Loss: 0.1669965237379074, Test Accuracy: 0.9465\n",
      "Epoch 3,\n",
      " Train Loss: 0.13529865443706512, Train Accuracy: 0.9578666666666666,\n",
      " Test Loss: 0.11063151061534882, Test Accuracy: 0.9645\n",
      "Epoch 4,\n",
      " Train Loss: 0.11077943444252014, Train Accuracy: 0.9650833333333333,\n",
      " Test Loss: 0.11696229875087738, Test Accuracy: 0.9651\n",
      "Epoch 5,\n",
      " Train Loss: 0.09720251709222794, Train Accuracy: 0.9697,\n",
      " Test Loss: 0.10933443903923035, Test Accuracy: 0.9671\n",
      "Epoch 6,\n",
      " Train Loss: 0.0842965617775917, Train Accuracy: 0.9735333333333334,\n",
      " Test Loss: 0.12410596013069153, Test Accuracy: 0.9625\n",
      "Epoch 7,\n",
      " Train Loss: 0.07540672272443771, Train Accuracy: 0.9758666666666667,\n",
      " Test Loss: 0.1059110164642334, Test Accuracy: 0.967\n",
      "Epoch 8,\n",
      " Train Loss: 0.06715988367795944, Train Accuracy: 0.9782166666666666,\n",
      " Test Loss: 0.10901623964309692, Test Accuracy: 0.9673\n",
      "Epoch 9,\n",
      " Train Loss: 0.06113774701952934, Train Accuracy: 0.9797833333333333,\n",
      " Test Loss: 0.12305649369955063, Test Accuracy: 0.9634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.055269863456487656, Train Accuracy: 0.9821,\n",
      " Test Loss: 0.09571367502212524, Test Accuracy: 0.9725\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 4.938868999481201, Train Accuracy: 0.10653333333333333,\n",
      " Test Loss: 2.3059279918670654, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.3062329292297363, Train Accuracy: 0.10236666666666666,\n",
      " Test Loss: 2.3034842014312744, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.3062376976013184, Train Accuracy: 0.10336666666666666,\n",
      " Test Loss: 2.3041794300079346, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.307173252105713, Train Accuracy: 0.10408333333333333,\n",
      " Test Loss: 2.3038995265960693, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.307039737701416, Train Accuracy: 0.1043,\n",
      " Test Loss: 2.3047473430633545, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.3072397708892822, Train Accuracy: 0.1064,\n",
      " Test Loss: 2.3041954040527344, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.306976318359375, Train Accuracy: 0.1028,\n",
      " Test Loss: 2.304389715194702, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.306633949279785, Train Accuracy: 0.10351666666666667,\n",
      " Test Loss: 2.302298069000244, Test Accuracy: 0.0982\n",
      "Epoch 9,\n",
      " Train Loss: 2.3082854747772217, Train Accuracy: 0.10356666666666667,\n",
      " Test Loss: 2.3058643341064453, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.307586193084717, Train Accuracy: 0.10163333333333334,\n",
      " Test Loss: 2.3035991191864014, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [128, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 37.94639205932617, Train Accuracy: 0.1168,\n",
      " Test Loss: 2.3170132637023926, Test Accuracy: 0.0982\n",
      "Epoch 2,\n",
      " Train Loss: 2.3102939128875732, Train Accuracy: 0.10338333333333333,\n",
      " Test Loss: 2.3078770637512207, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.308192491531372, Train Accuracy: 0.10428333333333334,\n",
      " Test Loss: 2.3076751232147217, Test Accuracy: 0.1028\n",
      "Epoch 4,\n",
      " Train Loss: 2.3082447052001953, Train Accuracy: 0.1043,\n",
      " Test Loss: 2.3103866577148438, Test Accuracy: 0.1009\n",
      "Epoch 5,\n",
      " Train Loss: 2.3087761402130127, Train Accuracy: 0.10161666666666666,\n",
      " Test Loss: 2.305084705352783, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.308225154876709, Train Accuracy: 0.10528333333333334,\n",
      " Test Loss: 2.306016206741333, Test Accuracy: 0.1032\n",
      "Epoch 7,\n",
      " Train Loss: 2.308420181274414, Train Accuracy: 0.10188333333333334,\n",
      " Test Loss: 2.304673671722412, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.3084604740142822, Train Accuracy: 0.10418333333333334,\n",
      " Test Loss: 2.3133530616760254, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.308861255645752, Train Accuracy: 0.10343333333333334,\n",
      " Test Loss: 2.3099567890167236, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.308152675628662, Train Accuracy: 0.10631666666666667,\n",
      " Test Loss: 2.306065320968628, Test Accuracy: 0.1028\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.42170366644859314, Train Accuracy: 0.8659,\n",
      " Test Loss: 0.1557491421699524, Test Accuracy: 0.9509\n",
      "Epoch 2,\n",
      " Train Loss: 0.1426815241575241, Train Accuracy: 0.9557166666666667,\n",
      " Test Loss: 0.13139969110488892, Test Accuracy: 0.9579\n",
      "Epoch 3,\n",
      " Train Loss: 0.10412966459989548, Train Accuracy: 0.9674,\n",
      " Test Loss: 0.0929863378405571, Test Accuracy: 0.9698\n",
      "Epoch 4,\n",
      " Train Loss: 0.07990381866693497, Train Accuracy: 0.9747666666666667,\n",
      " Test Loss: 0.10370675474405289, Test Accuracy: 0.9675\n",
      "Epoch 5,\n",
      " Train Loss: 0.0650264322757721, Train Accuracy: 0.9798833333333333,\n",
      " Test Loss: 0.08247904479503632, Test Accuracy: 0.9753\n",
      "Epoch 6,\n",
      " Train Loss: 0.05123269930481911, Train Accuracy: 0.9834333333333334,\n",
      " Test Loss: 0.07766743749380112, Test Accuracy: 0.9773\n",
      "Epoch 7,\n",
      " Train Loss: 0.044480111449956894, Train Accuracy: 0.9858833333333333,\n",
      " Test Loss: 0.07751750200986862, Test Accuracy: 0.9767\n",
      "Epoch 8,\n",
      " Train Loss: 0.03704466670751572, Train Accuracy: 0.9881,\n",
      " Test Loss: 0.0766880065202713, Test Accuracy: 0.9773\n",
      "Epoch 9,\n",
      " Train Loss: 0.030664030462503433, Train Accuracy: 0.9896833333333334,\n",
      " Test Loss: 0.07893964648246765, Test Accuracy: 0.9796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.024480925872921944, Train Accuracy: 0.9921166666666666,\n",
      " Test Loss: 0.0760698989033699, Test Accuracy: 0.9806\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 36.14741897583008, Train Accuracy: 0.10551666666666666,\n",
      " Test Loss: 2.303699254989624, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.305389881134033, Train Accuracy: 0.10538333333333333,\n",
      " Test Loss: 2.301635265350342, Test Accuracy: 0.1009\n",
      "Epoch 3,\n",
      " Train Loss: 2.30543851852417, Train Accuracy: 0.10291666666666667,\n",
      " Test Loss: 2.303032159805298, Test Accuracy: 0.0958\n",
      "Epoch 4,\n",
      " Train Loss: 2.306833505630493, Train Accuracy: 0.10431666666666667,\n",
      " Test Loss: 2.3029191493988037, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.3066718578338623, Train Accuracy: 0.1051,\n",
      " Test Loss: 2.3038787841796875, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.307602643966675, Train Accuracy: 0.10406666666666667,\n",
      " Test Loss: 2.306964635848999, Test Accuracy: 0.1135\n",
      "Epoch 7,\n",
      " Train Loss: 2.3074469566345215, Train Accuracy: 0.10518333333333334,\n",
      " Test Loss: 2.3043737411499023, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.307340621948242, Train Accuracy: 0.10461666666666666,\n",
      " Test Loss: 2.305474281311035, Test Accuracy: 0.1028\n",
      "Epoch 9,\n",
      " Train Loss: 2.3076188564300537, Train Accuracy: 0.10451666666666666,\n",
      " Test Loss: 2.303799867630005, Test Accuracy: 0.0982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.307523488998413, Train Accuracy: 0.10426666666666666,\n",
      " Test Loss: 2.3071768283843994, Test Accuracy: 0.0982\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 128, Layer Sizes: [512, 256, 128], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 1673.5499267578125, Train Accuracy: 0.1202,\n",
      " Test Loss: 2.308006763458252, Test Accuracy: 0.1009\n",
      "Epoch 2,\n",
      " Train Loss: 2.3081932067871094, Train Accuracy: 0.10535,\n",
      " Test Loss: 2.3095862865448, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.3085744380950928, Train Accuracy: 0.10223333333333333,\n",
      " Test Loss: 2.307854175567627, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.308159589767456, Train Accuracy: 0.104,\n",
      " Test Loss: 2.308018684387207, Test Accuracy: 0.1009\n",
      "Epoch 5,\n",
      " Train Loss: 2.3083078861236572, Train Accuracy: 0.10633333333333334,\n",
      " Test Loss: 2.304676055908203, Test Accuracy: 0.1028\n",
      "Epoch 6,\n",
      " Train Loss: 2.3083856105804443, Train Accuracy: 0.10483333333333333,\n",
      " Test Loss: 2.3107123374938965, Test Accuracy: 0.0982\n",
      "Epoch 7,\n",
      " Train Loss: 2.308208703994751, Train Accuracy: 0.10366666666666667,\n",
      " Test Loss: 2.3049840927124023, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.308342933654785, Train Accuracy: 0.10415,\n",
      " Test Loss: 2.3079938888549805, Test Accuracy: 0.0982\n",
      "Epoch 9,\n",
      " Train Loss: 2.3080992698669434, Train Accuracy: 0.10506666666666667,\n",
      " Test Loss: 2.309715986251831, Test Accuracy: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3083131313323975, Train Accuracy: 0.10508333333333333,\n",
      " Test Loss: 2.307035207748413, Test Accuracy: 0.0982\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.4479057192802429, Train Accuracy: 0.8596166666666667,\n",
      " Test Loss: 0.2271939069032669, Test Accuracy: 0.9277\n",
      "Epoch 2,\n",
      " Train Loss: 0.17235615849494934, Train Accuracy: 0.9486,\n",
      " Test Loss: 0.15194295346736908, Test Accuracy: 0.9561\n",
      "Epoch 3,\n",
      " Train Loss: 0.1277988702058792, Train Accuracy: 0.9616166666666667,\n",
      " Test Loss: 0.11813957989215851, Test Accuracy: 0.9629\n",
      "Epoch 4,\n",
      " Train Loss: 0.09965173155069351, Train Accuracy: 0.9698666666666667,\n",
      " Test Loss: 0.15500546991825104, Test Accuracy: 0.951\n",
      "Epoch 5,\n",
      " Train Loss: 0.08420956134796143, Train Accuracy: 0.9741333333333333,\n",
      " Test Loss: 0.08602739870548248, Test Accuracy: 0.9725\n",
      "Epoch 6,\n",
      " Train Loss: 0.07123323529958725, Train Accuracy: 0.9785333333333334,\n",
      " Test Loss: 0.1028357744216919, Test Accuracy: 0.9715\n",
      "Epoch 7,\n",
      " Train Loss: 0.06129124388098717, Train Accuracy: 0.9810666666666666,\n",
      " Test Loss: 0.09447356313467026, Test Accuracy: 0.9708\n",
      "Epoch 8,\n",
      " Train Loss: 0.054457567632198334, Train Accuracy: 0.9834,\n",
      " Test Loss: 0.10632197558879852, Test Accuracy: 0.9658\n",
      "Epoch 9,\n",
      " Train Loss: 0.04778279364109039, Train Accuracy: 0.9855833333333334,\n",
      " Test Loss: 0.10419268906116486, Test Accuracy: 0.968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 0.04356551542878151, Train Accuracy: 0.9867666666666667,\n",
      " Test Loss: 0.09198726713657379, Test Accuracy: 0.9728\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: Adam\n",
      "Epoch 1,\n",
      " Train Loss: 17.54177474975586, Train Accuracy: 0.10678333333333333,\n",
      " Test Loss: 2.3028836250305176, Test Accuracy: 0.1135\n",
      "Epoch 2,\n",
      " Train Loss: 2.3034839630126953, Train Accuracy: 0.10813333333333333,\n",
      " Test Loss: 2.3037571907043457, Test Accuracy: 0.1028\n",
      "Epoch 3,\n",
      " Train Loss: 2.304652214050293, Train Accuracy: 0.10541666666666667,\n",
      " Test Loss: 2.3034377098083496, Test Accuracy: 0.1009\n",
      "Epoch 4,\n",
      " Train Loss: 2.3043999671936035, Train Accuracy: 0.10663333333333333,\n",
      " Test Loss: 2.303004741668701, Test Accuracy: 0.1135\n",
      "Epoch 5,\n",
      " Train Loss: 2.304647922515869, Train Accuracy: 0.10735,\n",
      " Test Loss: 2.3020832538604736, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.3045742511749268, Train Accuracy: 0.10783333333333334,\n",
      " Test Loss: 2.3030943870544434, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 14:43:52.633924: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:1856: Filling up shuffle buffer (this may take a while): 1 of 1024\n",
      "2023-11-21 14:43:52.648966: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7,\n",
      " Train Loss: 2.305079221725464, Train Accuracy: 0.10645,\n",
      " Test Loss: 2.303201913833618, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.305271863937378, Train Accuracy: 0.10515,\n",
      " Test Loss: 2.302154541015625, Test Accuracy: 0.1135\n",
      "Epoch 9,\n",
      " Train Loss: 2.3043935298919678, Train Accuracy: 0.10746666666666667,\n",
      " Test Loss: 2.303128480911255, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.RMSprop` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.RMSprop`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3043930530548096, Train Accuracy: 0.10805,\n",
      " Test Loss: 2.3031516075134277, Test Accuracy: 0.1028\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 256, Layer Sizes: [256, 256], Optimizer: RMSprop\n",
      "Epoch 1,\n",
      " Train Loss: 222.19284057617188, Train Accuracy: 0.15338333333333334,\n",
      " Test Loss: 2.3141255378723145, Test Accuracy: 0.1028\n",
      "Epoch 2,\n",
      " Train Loss: 2.3069024085998535, Train Accuracy: 0.1044,\n",
      " Test Loss: 2.3087997436523438, Test Accuracy: 0.1135\n",
      "Epoch 3,\n",
      " Train Loss: 2.306802988052368, Train Accuracy: 0.10383333333333333,\n",
      " Test Loss: 2.311448574066162, Test Accuracy: 0.1135\n",
      "Epoch 4,\n",
      " Train Loss: 2.306501865386963, Train Accuracy: 0.10473333333333333,\n",
      " Test Loss: 2.311145305633545, Test Accuracy: 0.1009\n",
      "Epoch 5,\n",
      " Train Loss: 2.306547164916992, Train Accuracy: 0.10406666666666667,\n",
      " Test Loss: 2.305515766143799, Test Accuracy: 0.1135\n",
      "Epoch 6,\n",
      " Train Loss: 2.3066203594207764, Train Accuracy: 0.10268333333333333,\n",
      " Test Loss: 2.3072807788848877, Test Accuracy: 0.101\n",
      "Epoch 7,\n",
      " Train Loss: 2.3065576553344727, Train Accuracy: 0.10345,\n",
      " Test Loss: 2.3069140911102295, Test Accuracy: 0.1135\n",
      "Epoch 8,\n",
      " Train Loss: 2.3068323135375977, Train Accuracy: 0.10591666666666667,\n",
      " Test Loss: 2.312075138092041, Test Accuracy: 0.0892\n",
      "Epoch 9,\n",
      " Train Loss: 2.3064422607421875, Train Accuracy: 0.10361666666666666,\n",
      " Test Loss: 2.3086161613464355, Test Accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.SGD` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.SGD`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10,\n",
      " Train Loss: 2.3066318035125732, Train Accuracy: 0.1037,\n",
      " Test Loss: 2.3204801082611084, Test Accuracy: 0.1135\n",
      "Running experiment with LR: 0.1, Momentum: 0.5, Batch Size: 256, Layer Sizes: [128, 128], Optimizer: SGD\n",
      "Epoch 1,\n",
      " Train Loss: 0.5353261828422546, Train Accuracy: 0.8327166666666667,\n",
      " Test Loss: 0.2411889135837555, Test Accuracy: 0.927\n",
      "Epoch 2,\n",
      " Train Loss: 0.20020723342895508, Train Accuracy: 0.9393,\n",
      " Test Loss: 0.15801754593849182, Test Accuracy: 0.9512\n",
      "Epoch 3,\n",
      " Train Loss: 0.15027306973934174, Train Accuracy: 0.9538666666666666,\n",
      " Test Loss: 0.17079219222068787, Test Accuracy: 0.9445\n",
      "Epoch 4,\n",
      " Train Loss: 0.12224161624908447, Train Accuracy: 0.9624333333333334,\n",
      " Test Loss: 0.11810402572154999, Test Accuracy: 0.9617\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/fc/wq_gjk3n0dx253618rmsgxy00000gn/T/ipykernel_49174/3706863801.py\u001B[0m in \u001B[0;36m?\u001B[0;34m()\u001B[0m\n\u001B[1;32m     53\u001B[0m                         \u001B[0moptimizer\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlearning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mlr\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m                     \u001B[0mcce\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeras\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlosses\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCategoricalCrossentropy\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 57\u001B[0;31m                     train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n\u001B[0m\u001B[1;32m     58\u001B[0m                         \u001B[0mEPOCHS\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtest_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcce\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m                     )\n\u001B[1;32m     60\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/var/folders/fc/wq_gjk3n0dx253618rmsgxy00000gn/T/ipykernel_49174/2464798920.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(num_epochs, model, train_dataset, test_dataset, loss_function, optimizer)\u001B[0m\n\u001B[1;32m     88\u001B[0m         \u001B[0;31m# Training phase\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     89\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mx_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtarget_train\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mGradientTape\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mtape\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     91\u001B[0m                 \u001B[0;31m# Forward pass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 92\u001B[0;31m                 \u001B[0mpred_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     93\u001B[0m                 \u001B[0;31m# Calculate the training loss\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     94\u001B[0m                 \u001B[0mloss_train\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mloss_function\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtarget_train\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpred_train\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     95\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/engine/training.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    586\u001B[0m                 \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mcopied_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mcopied_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    587\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    588\u001B[0m             \u001B[0mlayout_map_lib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_map_subclass_model_variable\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_layout_map\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    589\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 590\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/engine/base_layer.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001B[1;32m   1147\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1148\u001B[0m                 ):\n\u001B[0;32m-> 1149\u001B[0;31m                     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1151\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1152\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    154\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    157\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 158\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/var/folders/fc/wq_gjk3n0dx253618rmsgxy00000gn/T/ipykernel_49174/2464798920.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     57\u001B[0m         \u001B[0;31m# Forward pass through hidden layers\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mmlp_layer\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmlp_layers\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     59\u001B[0m             \u001B[0mx\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmlp_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     60\u001B[0m         \u001B[0;31m# Forward pass through the output layer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 61\u001B[0;31m         \u001B[0my\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moutput_layer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     62\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0;31m# To get the full stack trace, call:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m             \u001B[0;31m# `tf.debugging.disable_traceback_filtering()`\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     70\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     71\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 72\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/engine/base_layer.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1145\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1146\u001B[0m                 with autocast_variable.enable_auto_cast_variables(\n\u001B[1;32m   1147\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_compute_dtype_object\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1148\u001B[0m                 ):\n\u001B[0;32m-> 1149\u001B[0;31m                     \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcall_fn\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1150\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1151\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_activity_regularizer\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1152\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_handle_activity_regularization\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moutputs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    154\u001B[0m                 \u001B[0mnew_e\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    155\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mnew_e\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    156\u001B[0m         \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    157\u001B[0m             \u001B[0;32mdel\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 158\u001B[0;31m             \u001B[0;32mdel\u001B[0m \u001B[0mbound_signature\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/keras/src/layers/core/dense.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(self, inputs)\u001B[0m\n\u001B[1;32m    237\u001B[0m                 outputs = tf.nn.embedding_lookup_sparse(\n\u001B[1;32m    238\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mids\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mweights\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcombiner\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"sum\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    239\u001B[0m                 )\n\u001B[1;32m    240\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 241\u001B[0;31m                 \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmatmul\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ma\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mb\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    242\u001B[0m         \u001B[0;31m# Broadcast kernel to inputs.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    243\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    244\u001B[0m             \u001B[0moutputs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtf\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtensordot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0minputs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkernel\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mrank\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/tensorflow/python/ops/weak_tensor_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    140\u001B[0m   \u001B[0;32mdef\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    141\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_auto_dtype_conversion_enabled\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 142\u001B[0;31m       \u001B[0;32mreturn\u001B[0m \u001B[0mop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    143\u001B[0m     \u001B[0mbound_arguments\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msignature\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    144\u001B[0m     \u001B[0mbound_arguments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapply_defaults\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    145\u001B[0m     \u001B[0mbound_kwargs\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbound_arguments\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marguments\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/tensorflow/python/util/traceback_utils.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    151\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m       \u001B[0mfiltered_tb\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_process_traceback_frames\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m__traceback__\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    153\u001B[0m       \u001B[0;32mraise\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwith_traceback\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfiltered_tb\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    154\u001B[0m     \u001B[0;32mfinally\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 155\u001B[0;31m       \u001B[0;32mdel\u001B[0m \u001B[0mfiltered_tb\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/tensorflow/python/util/dispatch.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1257\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1258\u001B[0m       \u001B[0;31m# Fallback dispatch system (dispatch v1):\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1259\u001B[0m       \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1260\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mdispatch_target\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1261\u001B[0;31m       \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1262\u001B[0m         \u001B[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1263\u001B[0m         \u001B[0;31m# TypeError, when given unexpected types.  So we need to catch both.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1264\u001B[0m         \u001B[0mresult\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mop_dispatch_handler\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/tensorflow/python/ops/math_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, name)\u001B[0m\n\u001B[1;32m   3838\u001B[0m         \u001B[0madjoint_b\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0madjoint_b\u001B[0m \u001B[0;32mor\u001B[0m \u001B[0mtranspose_b\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3839\u001B[0m         return gen_math_ops.batch_mat_mul_v3(\n\u001B[1;32m   3840\u001B[0m             a, b, adj_x=adjoint_a, adj_y=adjoint_b, Tout=output_type, name=name)\n\u001B[1;32m   3841\u001B[0m       \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3842\u001B[0;31m         return gen_math_ops.mat_mul(\n\u001B[0m\u001B[1;32m   3843\u001B[0m             a, b, transpose_a=transpose_a, transpose_b=transpose_b, name=name)\n",
      "\u001B[0;32m~/tf/env/lib/python3.10/site-packages/tensorflow/python/ops/gen_math_ops.py\u001B[0m in \u001B[0;36m?\u001B[0;34m(a, b, transpose_a, transpose_b, name)\u001B[0m\n\u001B[1;32m   6173\u001B[0m         transpose_b)\n\u001B[1;32m   6174\u001B[0m       \u001B[0;32mreturn\u001B[0m \u001B[0m_result\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6175\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_NotOkStatusException\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0me\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6176\u001B[0m       \u001B[0m_ops\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mraise_from_not_ok_status\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0me\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mname\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 6177\u001B[0;31m     \u001B[0;32mexcept\u001B[0m \u001B[0m_core\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_FallbackException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   6178\u001B[0m       \u001B[0;32mpass\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6179\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   6180\u001B[0m       return mat_mul_eager_fallback(\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# 2. 5 Visualization\n",
    "def visualization(train_losses , train_accuracies , test_losses , test_accuracies):\n",
    "    \"\"\"\n",
    "    Visualizes accuracy and loss for training and test data using the mean of each epoch.\n",
    "    Loss is displayed in a regular line, accuracy in a dotted line.\n",
    "    Training data is displayed in blue, test data in red. Parameters\n",
    "    ----------\n",
    "    train_losses : numpy.ndarray\n",
    "    training losses train_accuracies : numpy.ndarray\n",
    "    training accuracies test_losses : numpy.ndarray\n",
    "    test losses\n",
    "    test_accuracies : numpy.ndarray\n",
    "    test accuracies\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(train_losses, \"b-\")\n",
    "    line2, = plt.plot(test_losses, \"r-\")\n",
    "    line3, = plt.plot(train_accuracies, \"b:\")\n",
    "    line4, = plt.plot(test_accuracies, \"r:\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualization(train_losses , train_accuracies , test_losses , test_accuracies)\n",
    "\n",
    "# hyperparameters\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "momentums = [0.5, 0.9]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "layer_configurations = [[256, 256], [128, 128], [512, 256, 128]]\n",
    "optimizers_list = [optimizers.SGD, optimizers.Adam, optimizers.RMSprop]\n",
    "\n",
    "# Record results\n",
    "experiment_results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentums:\n",
    "        for batch_size in batch_sizes:\n",
    "            for layer_sizes in layer_configurations:\n",
    "                for opt in optimizers_list:\n",
    "                    print(f\"Running experiment with LR: {lr}, Momentum: {momentum}, Batch Size: {batch_size}, Layer Sizes: {layer_sizes}, Optimizer: {opt.__name__}\")\n",
    "\n",
    "                    train_dataset = data_pipeline(train_ds, batch_size)\n",
    "                    test_dataset = data_pipeline(test_ds, batch_size)\n",
    "\n",
    "                    model = MLPModel(layer_sizes)\n",
    "\n",
    "                    if opt == optimizers.SGD:\n",
    "                        optimizer = opt(learning_rate=lr, momentum=momentum)\n",
    "                    else:\n",
    "                        optimizer = opt(learning_rate=lr)\n",
    "\n",
    "                    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "                    train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "                        EPOCHS, model, train_dataset, test_dataset, cce, optimizer\n",
    "                    )\n",
    "\n",
    "                    experiment_results.append({\n",
    "                        'lr': lr,\n",
    "                        'momentum': momentum,\n",
    "                        'batch_size': batch_size,\n",
    "                        'layer_sizes': layer_sizes,\n",
    "                        'optimizer': opt.__name__,\n",
    "                        'train_losses': train_losses,\n",
    "                        'test_losses': test_losses,\n",
    "                        'train_accuracies': train_accuracies,\n",
    "                        'test_accuracies': test_accuracies\n",
    "                    })\n",
    "\n",
    "\n",
    "# Assuming experiment_results is a list of dictionaries\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "print(\"Summary Statistics:\")\n",
    "print(results_df[['train_losses', 'test_losses', 'train_accuracies', 'test_accuracies']].describe())\n",
    "# Best accuracy\n",
    "best_accuracy = results_df.sort_values(by='test_accuracies', ascending=False).head(1)\n",
    "print(\"\\nBest Accuracy Configuration:\")\n",
    "print(best_accuracy)\n",
    "\n",
    "# Lowest loss\n",
    "lowest_loss = results_df.sort_values(by='test_losses').head(1)\n",
    "print(\"\\nLowest Loss Configuration:\")\n",
    "print(lowest_loss)\n",
    "# Plotting Test Losses for different configurations\n",
    "plt.figure(figsize=(12, 6))\n",
    "for index, row in results_df.iterrows():\n",
    "    label = f\"LR: {row['lr']}, Momentum: {row['momentum']}, Batch: {row['batch_size']}, Layers: {len(row['layers'])}\"\n",
    "    plt.plot(row['test_losses'], label=label)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss per Epoch for Different Configurations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Similarly, you can plot for accuracies\n",
    "correlation_matrix = results_df[['lr', 'momentum', 'batch_size', 'train_losses', 'test_losses', 'train_accuracies', 'test_accuracies']].corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T13:44:31.341942Z",
     "start_time": "2023-11-21T11:50:30.492691Z"
    }
   },
   "id": "455df1f445f6a87c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_hyper(hyperpapram, method):\n",
    "    unique_val = results_df[hyperparam].unique()\n",
    "    plt.figure(figsize(10,5))\n",
    "    for i in unique_val:\n",
    "        subset = results_df[results_df[hyperparam] == value]\n",
    "        mean_metric = subset.groupby('epoch')[metric].mean()\n",
    "        plot.plot(mean_metric, label = f\"{hyperparam}:{value}\")\n",
    "    \n",
    "    plt.ylabel(method)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Effect of Adjusting the hyperparameters of our model with different hyperparametrs and methods')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "plot_hyper('lr','test_accuracies')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5441e903f4a4eb6f"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "d0cbd61506dc15e1"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
