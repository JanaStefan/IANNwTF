{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN5UWRa4KLJhzH/vFVFoKpV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanaStefan/IANNwTF_g22/blob/main/cnn_jana.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import keras\n",
        "from keras import layers, losses, optimizers\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "IN5I_9X2TOwh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_cifar(train_data, test_data, batch_size, shuffle_buffer_size):\n",
        "  def preprocessing_func(img, label):\n",
        "    img = tf.cast(img, tf.float32)\n",
        "    img = (img/128) - 1 #// normalized\n",
        "    label = tf.one_hot(label, depth=10)\n",
        "    return img, label\n",
        "  train_data  = train_data.map(lambda img, label: preprocessing_func(img, label))\n",
        "  train_data = train_data.shuffle(shuffle_buffer_size).batch(batch_size).prefetch(2)\n",
        "  test_data = test_data.map(lambda img, label: preprocessing_func(img, label))\n",
        "  test_data = test_data.batch(1)\n",
        "  return train_data, test_data"
      ],
      "metadata": {
        "id": "vv9I51CXTNE9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "NwbdSZRej_JR"
      },
      "outputs": [],
      "source": [
        "class CNN():\n",
        "    def __init__(self, input_size=(32, 32, 3), num_filters=[32, 64, 128], kernel_size=[(3,3), (3,3), (3,3)], strides=[(1,1), (1,1), (1,1)], conv_activation=['relu', 'relu', 'relu'], cnn_pool_type=[\"max_pool\", \"max_pool\", \"max_pool\"],\n",
        "                 padding=[\"valid\", \"valid\", \"valid\"], use_bias=[False, False, False], dense_activation=['relu'], dense_sizes=[256], num_classes=10, flatten_type=\"global_max\", name=\"CNN_model\"):\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.num_filters = num_filters\n",
        "        self.kernel_size = kernel_size\n",
        "        self.strides = strides\n",
        "        self.conv_activation = conv_activation\n",
        "        self.cnn_pool_type = cnn_pool_type\n",
        "        self.padding = padding\n",
        "        self.use_bias = use_bias\n",
        "        self.dense_activation = dense_activation\n",
        "        self.dense_sizes = dense_sizes\n",
        "        self.num_classes = num_classes\n",
        "        self.flatten_type = flatten_type\n",
        "        self.name = name\n",
        "\n",
        "        self.model = None\n",
        "        self.create_cnn()\n",
        "        #self.compile_cnn()\n",
        "\n",
        "\n",
        "\n",
        "    def create_cnn(self):\n",
        "        inputs = layers.Input(shape=(self.input_size), dtype=tf.float32)\n",
        "        x = inputs\n",
        "\n",
        "        # Create CNN part with pooling layers\n",
        "        for i, num_filter in enumerate(self.num_filters):\n",
        "            x = layers.Conv2D(num_filter, self.kernel_size[i], self.strides[i], self.padding[i], activation=self.conv_activation[i], use_bias=self.use_bias[i])(x)\n",
        "            x = layers.BatchNormalization()(x)\n",
        "            if self.cnn_pool_type == \"max_pool\":\n",
        "                x = layers.MaxPool2D()(x) #TODO: make the pool size adjustable too?\n",
        "            else:\n",
        "                x = layers.AveragePooling2D()(x) #TODO: make the pool size adjustable too?\n",
        "            #x = layers.Dropout(0.2)(x)\n",
        "\n",
        "        # Flatten the output of the CNN part for it to fit into the MLP part\n",
        "        if self.flatten_type == \"global_max\":\n",
        "            x = layers.GlobalMaxPool2D()(x)\n",
        "        else:\n",
        "            x = layers.GlobalAveragePooling2D()(x)\n",
        "\n",
        "        # Create the MLP part\n",
        "        for i, dense_size in enumerate(self.dense_sizes):\n",
        "            x = layers.Dense(dense_size, activation=self.dense_activation[i])(x)\n",
        "\n",
        "        # Create the output part\n",
        "        y = layers.Dense(units=self.num_classes, activation='softmax')(x)\n",
        "\n",
        "        self.model = keras.Model(inputs=inputs, outputs=y, name=self.name)\n",
        "        self.model.summary()\n",
        "\n",
        "    # not used:\n",
        "    def compile_cnn(self):\n",
        "        self.model.compile(loss=losses.SparseCategoricalCrossentropy(), optimizer=optimizers.Adam(), metrics=[\"accuracy\"])\n",
        "\n",
        "\n",
        "    def train(self, train_data, test_data, batch_size=64, shuffle_buffer_size=1000, epochs=15, learning_rate=0.01, optimizer='adam'):\n",
        "        train_data, test_data = prep_cifar(train_data, test_data, batch_size, shuffle_buffer_size)\n",
        "        if optimizer == 'adam':\n",
        "          optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=learning_rate)\n",
        "        else:\n",
        "          optimizer = tf.keras.optimizers.RMSprop(learning_rate=learning_rate)\n",
        "        loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
        "        for epoch in range(epochs):\n",
        "          loss_list = []\n",
        "          correct_predictions = 0\n",
        "          total_samples = 0\n",
        "          print(\"Epoch: \" + str(epoch))\n",
        "          for image, label in train_data:\n",
        "            with tf.GradientTape() as tape:\n",
        "              pred = self.model(image)\n",
        "              loss = loss_f(label, pred)\n",
        "              loss_list.append(loss)\n",
        "            correct_predictions += np.sum(np.argmax(label, axis=1) == np.argmax(pred, axis=1))\n",
        "            total_samples += len(train_data)\n",
        "            gradients = tape.gradient(loss, self.model.trainable_variables)\n",
        "            optimizer.apply_gradients(zip(gradients, self.model.trainable_variables))\n",
        "          print(\"Loss: \" + str(np.average(loss_list)))\n",
        "          print(\"Accuracy: \" + str(correct_predictions / total_samples))\n",
        "          print(\"correct pred: \" + str(correct_predictions))\n",
        "          print(\"num data: \" + str(total_samples))\n",
        "\n",
        "        test_loss, test_accuracy = self.evaluate(test_data)\n",
        "        return test_loss, test_accuracy\n",
        "\n",
        "\n",
        "    def evaluate(self, test_data):\n",
        "      loss_f = tf.keras.losses.CategoricalCrossentropy()\n",
        "      loss_list = []\n",
        "      correct_predictions = 0\n",
        "      for image, label in test_data:\n",
        "          pred = self.model(image)\n",
        "          loss = loss_f(label, pred)\n",
        "          loss_list.append(loss)\n",
        "          if np.argmax(label, axis=-1) == np.argmax(pred, axis=-1):\n",
        "              correct_predictions += 1\n",
        "      return np.average(loss_list), correct_predictions/len(test_data)\n",
        "\n",
        "\n",
        "    def get_model(self):\n",
        "        return self.model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "train_data = tfds.load('cifar10', split='train', as_supervised=True)\n",
        "test_data = tfds.load('cifar10', split='test', as_supervised=True)\n",
        "#test_data = prep_cifar(test_data, 1, 1000)"
      ],
      "metadata": {
        "id": "aYDzh_elqmw6"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Default Set\n",
        "\n",
        "cnn_model = CNN()\n",
        "av_loss = cnn_model.train(train_data, test_data, epochs=1)\n",
        "#av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7LfktBUxkGAc",
        "outputId": "f0923909-6807-46f2-d371-a8af697a9f46"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"CNN_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_18 (InputLayer)       [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_51 (Conv2D)          (None, 30, 30, 32)        864       \n",
            "                                                                 \n",
            " batch_normalization_51 (Ba  (None, 30, 30, 32)        128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " average_pooling2d_51 (Aver  (None, 15, 15, 32)        0         \n",
            " agePooling2D)                                                   \n",
            "                                                                 \n",
            " conv2d_52 (Conv2D)          (None, 13, 13, 64)        18432     \n",
            "                                                                 \n",
            " batch_normalization_52 (Ba  (None, 13, 13, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " average_pooling2d_52 (Aver  (None, 6, 6, 64)          0         \n",
            " agePooling2D)                                                   \n",
            "                                                                 \n",
            " conv2d_53 (Conv2D)          (None, 4, 4, 128)         73728     \n",
            "                                                                 \n",
            " batch_normalization_53 (Ba  (None, 4, 4, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " average_pooling2d_53 (Aver  (None, 2, 2, 128)         0         \n",
            " agePooling2D)                                                   \n",
            "                                                                 \n",
            " global_max_pooling2d_17 (G  (None, 128)               0         \n",
            " lobalMaxPooling2D)                                              \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 256)               33024     \n",
            "                                                                 \n",
            " dense_35 (Dense)            (None, 10)                2570      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 129514 (505.91 KB)\n",
            "Trainable params: 129066 (504.16 KB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch: 0\n",
            "Loss: 1.8287157\n",
            "Accuracy: 0.025320347198147578\n",
            "correct pred: 15484\n",
            "num data: 611524\n",
            "Test Loss: (1.6754947, 0.3866)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "wbaoDh25a0qJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 1:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[32, 64, 128],\n",
        "    kernel_size=[(3,3), (3,3), (3,3)],\n",
        "    strides=[(1,1), (1,1), (1,1)],\n",
        "    conv_activation=['elu', 'elu', 'elu'],\n",
        "    cnn_pool_type=['max_pool', 'max_pool', 'max_pool'],\n",
        "    padding=['valid', 'valid', 'valid'],\n",
        "    use_bias=[False, False, False],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='global_max',\n",
        "    name=\"Set_1\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=32, shuffle_buffer_size=500, epochs=15, learning_rate=0.0001, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5kcmSSOaf_x",
        "outputId": "d1ba8715-0f09-422a-8fac-ee0758caab3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Set_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, 32, 32, 3)]       0         \n",
            "                                                                 \n",
            " conv2d_19 (Conv2D)          (None, 30, 30, 32)        864       \n",
            "                                                                 \n",
            " batch_normalization_18 (Ba  (None, 30, 30, 32)        128       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " average_pooling2d_18 (Aver  (None, 15, 15, 32)        0         \n",
            " agePooling2D)                                                   \n",
            "                                                                 \n",
            " conv2d_20 (Conv2D)          (None, 13, 13, 64)        18432     \n",
            "                                                                 \n",
            " batch_normalization_19 (Ba  (None, 13, 13, 64)        256       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " average_pooling2d_19 (Aver  (None, 6, 6, 64)          0         \n",
            " agePooling2D)                                                   \n",
            "                                                                 \n",
            " conv2d_21 (Conv2D)          (None, 4, 4, 128)         73728     \n",
            "                                                                 \n",
            " batch_normalization_20 (Ba  (None, 4, 4, 128)         512       \n",
            " tchNormalization)                                               \n",
            "                                                                 \n",
            " average_pooling2d_20 (Aver  (None, 2, 2, 128)         0         \n",
            " agePooling2D)                                                   \n",
            "                                                                 \n",
            " global_max_pooling2d_6 (Gl  (None, 128)               0         \n",
            " obalMaxPooling2D)                                               \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 128)               16512     \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 10)                1290      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 111722 (436.41 KB)\n",
            "Trainable params: 111274 (434.66 KB)\n",
            "Non-trainable params: 448 (1.75 KB)\n",
            "_________________________________________________________________\n",
            "Epoch: 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7e0534cc0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7e0534cc0280> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 2.2718532\n",
            "Epoch: 1\n",
            "Loss: 2.1984518\n",
            "Epoch: 2\n",
            "Loss: 2.1519659\n",
            "Epoch: 3\n",
            "Loss: 2.1097505\n",
            "Epoch: 4\n",
            "Loss: 2.069687\n",
            "Epoch: 5\n",
            "Loss: 2.0316186\n",
            "Epoch: 6\n",
            "Loss: 1.9940733\n",
            "Epoch: 7\n",
            "Loss: 1.9570284\n",
            "Epoch: 8\n",
            "Loss: 1.9213948\n",
            "Epoch: 9\n",
            "Loss: 1.8863839\n",
            "Epoch: 10\n",
            "Loss: 1.8518654\n",
            "Epoch: 11\n",
            "Loss: 1.8191652\n",
            "Epoch: 12\n",
            "Loss: 1.7874949\n",
            "Epoch: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 2:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[32, 64, 128],\n",
        "    kernel_size=[(4,4), (3,3), (2,2)],\n",
        "    strides=[(2,2), (1,1), (1,1)],\n",
        "    conv_activation=['elu', 'elu', 'elu'],\n",
        "    cnn_pool_type=['max_pool', 'max_pool', 'max_pool'],\n",
        "    padding=['valid', 'valid', 'valid'],\n",
        "    use_bias=[False, False, False],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='global_max',\n",
        "    name=\"Set_2\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=32, shuffle_buffer_size=1000, epochs=15, learning_rate=0.01, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "o3ynExLyg-wZ",
        "outputId": "5d6f69cd-4669-4658-b59f-bb625d1eedf9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-396f733643fd>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Set 2:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m cnn_model = CNN(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mnum_filters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m128\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'CNN' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 3:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[64, 128, 128],\n",
        "    kernel_size=[(4,4), (3,3), (2,2)],\n",
        "    strides=[(2,2), (1,1), (1,1)],\n",
        "    conv_activation=['relu', 'relu', 'relu'],\n",
        "    cnn_pool_type=['max_pool', 'max_pool', 'max_pool'],\n",
        "    padding=['valid', 'valid', 'valid'],\n",
        "    use_bias=[False, True, True],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='global_max',\n",
        "    name=\"Set_3\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=32, shuffle_buffer_size=1000, epochs=15, learning_rate=0.0001, optimizer='adam')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "id": "EVsgzQLShV6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 4:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[64, 128, 128],\n",
        "    kernel_size=[(4,4), (3,3), (2,2)],\n",
        "    strides=[(2,2), (1,1), (1,1)],\n",
        "    conv_activation=['relu', 'relu', 'relu'],\n",
        "    cnn_pool_type=['max_pool', 'max_pool', 'max_pool'],\n",
        "    padding=['valid', 'valid', 'valid'],\n",
        "    use_bias=[False, True, True],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='global_max',\n",
        "    name=\"Set_4\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=64, shuffle_buffer_size=500, epochs=15, learning_rate=0.01, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "id": "Qtzq_NxghqN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 5:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[64, 64],\n",
        "    kernel_size=[(3,3), (3,3)],\n",
        "    strides=[(2,2), (2,2)],\n",
        "    conv_activation=['relu', 'relu'],\n",
        "    cnn_pool_type=['max_pool', 'max_pool'],\n",
        "    padding=['valid', 'valid'],\n",
        "    use_bias=[False, True],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='global_max',\n",
        "    name=\"Set_5\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=64, shuffle_buffer_size=500, epochs=15, learning_rate=0.01, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "id": "sPwqHeq8i8iv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 6:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[64, 128],\n",
        "    kernel_size=[(3,3), (3,3)],\n",
        "    strides=[(2,2), (2,2)],\n",
        "    conv_activation=['relu', 'relu'],\n",
        "    cnn_pool_type=['av_pool', 'av_pool'],\n",
        "    padding=['valid', 'valid'],\n",
        "    use_bias=[False, True],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='av_max',\n",
        "    name=\"Set_6\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=64, shuffle_buffer_size=500, epochs=15, learning_rate=0.01, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "id": "3F_Kzx8JjRYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 7:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[64, 128],\n",
        "    kernel_size=[(3,3), (3,3)],\n",
        "    strides=[(1,1), (1,1)],\n",
        "    conv_activation=['relu', 'relu'],\n",
        "    cnn_pool_type=['av_pool', 'av_pool'],\n",
        "    padding=['valid', 'valid'],\n",
        "    use_bias=[False, False],\n",
        "    dense_activation=['tanh'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='av_max',\n",
        "    name=\"Set_7\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=32, shuffle_buffer_size=1000, epochs=15, learning_rate=0.01, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "id": "snNRxlhdjfC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set 8:\n",
        "cnn_model = CNN(\n",
        "    num_filters=[32, 64, 128, 128],\n",
        "    kernel_size=[(4,4), (3,3), (2,2), (2,2)],\n",
        "    strides=[(2,2), (1,1), (1,1), (1,1)],\n",
        "    conv_activation=['elu', 'elu', 'elu', 'elu'],\n",
        "    cnn_pool_type=['max_pool', 'max_pool', 'max_pool', 'max_pool'],\n",
        "    padding=['valid', 'valid', 'valid', 'valid'],\n",
        "    use_bias=[False, False, False, False],\n",
        "    dense_activation=['softmax'],\n",
        "    dense_sizes=[128],\n",
        "    flatten_type='global_max',\n",
        "    name=\"Set_8\",\n",
        ")\n",
        "cnn_model.train(train_data, test_data, batch_size=32, shuffle_buffer_size=1000, epochs=15, learning_rate=0.01, optimizer='rmsprop')\n",
        "av_loss = cnn_model.evaluate(test_data)\n",
        "print(\"Test Loss: \" + str(av_loss))"
      ],
      "metadata": {
        "id": "1XnpfX3NjzDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}