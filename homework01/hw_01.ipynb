{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "992e2cfce3820f19",
   "metadata": {},
   "source": [
    "# 2 Assignment: Multi-Layer Perceptron\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1110bd6ae6f7bad5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 1000x300 with 10 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "dataset = load_digits()\n",
    "\n",
    "# Step 1: Extract data into (input, target) tuples\n",
    "data = [(image, target) for image, target in zip(dataset.images, dataset.target)]\n",
    "\n",
    "# Step 2: Plot the images to check if they are correct\n",
    "def plot_images(images, targets, num_images=10):\n",
    "    fig, axes = plt.subplots(1, num_images, figsize=(10, 3))\n",
    "    for i in range(num_images):\n",
    "        axes[i].imshow(images[i].reshape(8, 8), cmap='gray')\n",
    "        axes[i].set_title(f\"Digit {targets[i]}\")\n",
    "        axes[i].axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "# Plot the first 5 images\n",
    "# Assuming you have a list of 5 tuples in the 'data' variable\n",
    "data_subset = data[:10]\n",
    "# Unpack the tuples into separate lists using zip and *\n",
    "unpacked_lists = zip(*data_subset)\n",
    "\n",
    "# Pass the unpacked lists to the plot_images function\n",
    "plot_images(*unpacked_lists)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:17:06.860530Z",
     "start_time": "2023-11-07T13:17:05.560811Z"
    }
   },
   "id": "b2398b993c6d5e7c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
      "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
      "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
      "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
      "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.]), 0), (array([ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.,  0.,  0.,  0., 11., 16.,\n",
      "        9.,  0.,  0.,  0.,  0.,  3., 15., 16.,  6.,  0.,  0.,  0.,  7.,\n",
      "       15., 16., 16.,  2.,  0.,  0.,  0.,  0.,  1., 16., 16.,  3.,  0.,\n",
      "        0.,  0.,  0.,  1., 16., 16.,  6.,  0.,  0.,  0.,  0.,  1., 16.,\n",
      "       16.,  6.,  0.,  0.,  0.,  0.,  0., 11., 16., 10.,  0.,  0.]), 1), (array([ 0.,  0.,  0.,  4., 15., 12.,  0.,  0.,  0.,  0.,  3., 16., 15.,\n",
      "       14.,  0.,  0.,  0.,  0.,  8., 13.,  8., 16.,  0.,  0.,  0.,  0.,\n",
      "        1.,  6., 15., 11.,  0.,  0.,  0.,  1.,  8., 13., 15.,  1.,  0.,\n",
      "        0.,  0.,  9., 16., 16.,  5.,  0.,  0.,  0.,  0.,  3., 13., 16.,\n",
      "       16., 11.,  5.,  0.,  0.,  0.,  0.,  3., 11., 16.,  9.,  0.]), 2), (array([ 0.,  0.,  7., 15., 13.,  1.,  0.,  0.,  0.,  8., 13.,  6., 15.,\n",
      "        4.,  0.,  0.,  0.,  2.,  1., 13., 13.,  0.,  0.,  0.,  0.,  0.,\n",
      "        2., 15., 11.,  1.,  0.,  0.,  0.,  0.,  0.,  1., 12., 12.,  1.,\n",
      "        0.,  0.,  0.,  0.,  0.,  1., 10.,  8.,  0.,  0.,  0.,  8.,  4.,\n",
      "        5., 14.,  9.,  0.,  0.,  0.,  7., 13., 13.,  9.,  0.,  0.]), 3), (array([ 0.,  0.,  0.,  1., 11.,  0.,  0.,  0.,  0.,  0.,  0.,  7.,  8.,\n",
      "        0.,  0.,  0.,  0.,  0.,  1., 13.,  6.,  2.,  2.,  0.,  0.,  0.,\n",
      "        7., 15.,  0.,  9.,  8.,  0.,  0.,  5., 16., 10.,  0., 16.,  6.,\n",
      "        0.,  0.,  4., 15., 16., 13., 16.,  1.,  0.,  0.,  0.,  0.,  3.,\n",
      "       15., 10.,  0.,  0.,  0.,  0.,  0.,  2., 16.,  4.,  0.,  0.]), 4)]\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Reshape images into vectors\n",
    "# using flatten() function to flat the image data into a vertor \n",
    "data = [(image.flatten(), target) for image, target in data]\n",
    "print(data[:5])\n",
    "# Step 4: Rescale images to [0, 1] range\n",
    "data = [(image.astype(np.float32) / 255.0, target) for image, target in data]\n",
    "\n",
    "# Step 5: One-hot encode the target digits\n",
    "encoder = OneHotEncoder(sparse_output=False, categories='auto')\n",
    "targets = np.array([target for _, target in data]).reshape(-1, 1)\n",
    "onehot_targets = encoder.fit_transform(targets)\n",
    "\n",
    "# Step 6: Write a generator function\n",
    "def data_generator(data, batch_size):\n",
    "    while True:\n",
    "        np.random.shuffle(data)\n",
    "        for i in range(0, len(data), batch_size):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            batch_inputs, batch_targets = zip(*batch_data)\n",
    "            yield np.array(batch_inputs), np.array(batch_targets)\n",
    "\n",
    "# Example usage of the generator\n",
    "batch_size = 32\n",
    "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "train_generator = data_generator(train_data, batch_size)\n",
    "val_generator = data_generator(val_data, batch_size)\n",
    "\n",
    "# You can now use train_generator and val_generator in your training loop\n",
    "# For example:\n",
    "# for epoch in range(num_epochs):\n",
    "#     for batch_inputs, batch_targets in train_generator:\n",
    "#         # Your training logic here"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-07T13:17:06.887351Z",
     "start_time": "2023-11-07T13:17:06.861275Z"
    }
   },
   "id": "e9a4a069f4b89046"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bf63aa1951a71cf0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66e8f7d8f317947e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.2 Sigmoid Activation Function\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14280642857a2877"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "\n",
    "class Sigmoid:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Compute the sigmoid activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (minibatch_size, num_units)\n",
    "          Input array.\n",
    "\n",
    "        Returns:\n",
    "        - ndarray, shape (minibatch_size, num_units)\n",
    "          Output array after applying the sigmoid activation function element-wise.\n",
    "        \"\"\"\n",
    "        return 1 / (1 + np.exp(-x))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-06T20:40:46.758420Z",
     "start_time": "2023-11-06T20:40:46.753284Z"
    }
   },
   "id": "6e53a3fb0ac95753"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2.3 Softmax activation function\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e8f5288013f8874"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SoftmaxActivation:\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Compute the softmax activation function.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (minibatch_size, 10)\n",
    "          Input array.\n",
    "\n",
    "        Returns:\n",
    "        - ndarray, shape (minibatch_size, 10)\n",
    "          Output array after applying the softmax activation function element-wise.\n",
    "        \"\"\"\n",
    "        exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        softmax_output = exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
    "        return softmax_output\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c1f3eb38c7ea34d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MLPLayer:\n",
    "    def __init__(self, activation_function, num_units, input_size):\n",
    "        \"\"\"\n",
    "        Initialize the MLP layer.\n",
    "\n",
    "        Parameters:\n",
    "        - activation_function: object\n",
    "          An instance of the activation function (SigmoidActivation or SoftmaxActivation).\n",
    "        - num_units: int\n",
    "          Number of units (perceptrons) in the layer.\n",
    "        - input_size: int\n",
    "          Number of units in the preceding layer.\n",
    "        \"\"\"\n",
    "        self.activation_function = activation_function\n",
    "        self.num_units = num_units\n",
    "        self.input_size = input_size\n",
    "\n",
    "        # Initialize weights with small random values and bias with zeros\n",
    "        self.weights = np.random.normal(loc=0.0, scale=0.2, size=(input_size, num_units))\n",
    "        self.bias = np.zeros((1, num_units))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Perform the forward pass of the MLP layer.\n",
    "\n",
    "        Parameters:\n",
    "        - x: ndarray, shape (minibatch_size, input_size)\n",
    "          Input array.\n",
    "\n",
    "        Returns:\n",
    "        - ndarray, shape (minibatch_size, num_units)\n",
    "          Output array after applying the weight matrix, bias, and activation function.\n",
    "        \"\"\"\n",
    "        # Calculate pre-activations\n",
    "        pre_activations = np.dot(x, self.weights) + self.bias\n",
    "\n",
    "        # Apply activation function\n",
    "        output = self.activation_function(pre_activations)\n",
    "\n",
    "        return output\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "59fcce061571395c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CategoricalCrossEntropyLoss:\n",
    "    def __call__(self, y_true, y_pred):\n",
    "        \"\"\"\n",
    "        Compute the Categorical Cross-Entropy Loss.\n",
    "\n",
    "        Parameters:\n",
    "        - y_true: ndarray, shape (minibatch_size, num_classes)\n",
    "          True labels (one-hot encoded).\n",
    "        - y_pred: ndarray, shape (minibatch_size, num_classes)\n",
    "          Predicted probabilities.\n",
    "\n",
    "        Returns:\n",
    "        - float\n",
    "          Categorical Cross-Entropy Loss.\n",
    "        \"\"\"\n",
    "        # Avoid numerical instability by adding a small epsilon\n",
    "        epsilon = 1e-15\n",
    "        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "\n",
    "        # Compute the cross-entropy loss\n",
    "        loss = -np.sum(y_true * np.log(y_pred)) / y_true.shape[0]\n",
    "\n",
    "        return loss\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "30a967b464609044"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
