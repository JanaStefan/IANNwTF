{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.1 Loading the MNIST dataset\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tensorflow.keras import optimizers\n",
    "\n",
    "\n",
    "(train_ds, test_ds), ds_info = tfds.load('mnist', split=['train', 'test'], as_supervised=True, with_info=True)\n",
    "#tfds.show_examples(train_ds , ds_info)\n",
    "\n",
    "# 2.2 Setting up the data pipeline\n",
    "\n",
    "def data_pipeline(input, batch_size=64, prefetch_buffer_size=4):\n",
    "    # Map the dataset to extract images and labels\n",
    "    input =input.map(lambda image, label: (image, label))\n",
    "    # Reshape each image to a flat vector\n",
    "    input = input.map(lambda image, label: (tf.reshape(image, (-1,)), label))\n",
    "    # Normalize(Scale) image values to be in the range [-1, 1]\n",
    "    input = input.map(lambda image, label: ((tf.cast(image, tf.float32) / 128) - 1, label))\n",
    "    # One-hot encode the labels\n",
    "    input = input.map(lambda image, label: (image, tf.one_hot(label, depth=10)))\n",
    "    # Shuffle the dataset and create batches of size 4\n",
    "    input = input.shuffle(1024).batch(batch_size)\n",
    "    # Prefetch the dataset to improve pipeline performance\n",
    "    input = input.prefetch(prefetch_buffer_size)\n",
    "    return input\n",
    "\n",
    "# Save the datasets after applying the data pipeline\n",
    "train_dataset = data_pipeline(train_ds)\n",
    "test_dataset = data_pipeline(test_ds)\n",
    "\n",
    "for elem in train_dataset.take(1):\n",
    "    print(elem)\n",
    "\n",
    "for elem in test_dataset.take(1):\n",
    "    print(elem)\n",
    "\n",
    "\n",
    "# 2.3 Building a deep neural network with TensorFlow\n",
    "class MLPModel(tf.keras.Model):\n",
    "    def __init__(self, layer_sizes, output_size=10):\n",
    "        super().__init__()\n",
    "        self.mlp_layers = []  # create a list to store hidden layers\n",
    "\n",
    "        # Create hidden layers with ReLU activation\n",
    "        for layer_size in layer_sizes:\n",
    "            new_layer = layers.Dense(units=layer_size, activation='relu')\n",
    "            self.mlp_layers.append(new_layer)\n",
    "        # Output layer with softmax activation for classification\n",
    "        self.output_layer = layers.Dense(units=output_size, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        # Forward pass through hidden layers\n",
    "        for mlp_layer in self.mlp_layers:\n",
    "            x = mlp_layer(x)\n",
    "        # Forward pass through the output layer\n",
    "        y = self.output_layer(x)\n",
    "        return y\n",
    "\n",
    "# 2.4 Training the network\n",
    "\"\"\"\n",
    "Define a training loop function which receives\n",
    "• The number of epochs\n",
    "• The model object\n",
    "• The training dataset\n",
    "• The test dataset\n",
    "• The loss function\n",
    "• The optimizer\n",
    "• Different arrays for the different values you want to track for visualization\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def train_model(num_epochs, model, train_dataset, test_dataset, loss_function, optimizer):\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    train_accuracies = []  # Track training accuracy\n",
    "    test_accuracies = []   # Track testing accuracy\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_train_losses = []\n",
    "        correct_train_predictions = 0  # Counter for correct training predictions\n",
    "        total_train_samples = 0\n",
    "\n",
    "        # Training phase\n",
    "        for x_train, target_train in train_dataset:\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass\n",
    "                pred_train = model(x_train)\n",
    "                # Calculate the training loss\n",
    "                loss_train = loss_function(target_train, pred_train)\n",
    "\n",
    "            # Calculate gradients\n",
    "            gradients_train = tape.gradient(loss_train, model.trainable_variables)\n",
    "\n",
    "            # Update weights using optimizer\n",
    "            optimizer.apply_gradients(zip(gradients_train, model.trainable_variables))\n",
    "\n",
    "            # Append the training loss to the list\n",
    "            epoch_train_losses.append(loss_train.numpy())\n",
    "\n",
    "            # Calculate training accuracy\n",
    "            correct_train_predictions += np.sum(np.argmax(target_train, axis=1) == np.argmax(pred_train, axis=1))\n",
    "            total_train_samples += len(x_train)\n",
    "\n",
    "        train_accuracy = correct_train_predictions / total_train_samples\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Testing phase\n",
    "        epoch_test_losses = []\n",
    "        correct_test_predictions = 0  # Counter for correct testing predictions\n",
    "        total_test_samples = 0\n",
    "\n",
    "        for x_test, target_test in test_dataset:\n",
    "            # Forward pass\n",
    "            pred_test = model(x_test)\n",
    "            # Calculate the testing loss\n",
    "            loss_test = loss_function(target_test, pred_test)\n",
    "            epoch_test_losses.append(loss_test.numpy())\n",
    "\n",
    "            # Calculate testing accuracy\n",
    "            correct_test_predictions += np.sum(np.argmax(target_test, axis=1) == np.argmax(pred_test, axis=1))\n",
    "            total_test_samples += len(x_test)\n",
    "\n",
    "        test_accuracy = correct_test_predictions / total_test_samples\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Print the mean training and testing loss and accuracy for the epoch\n",
    "        mean_train_loss = np.mean(epoch_train_losses)\n",
    "        mean_test_loss = np.mean(epoch_test_losses)\n",
    "        print(f\"Epoch {epoch + 1},\\n Train Loss: {mean_train_loss}, Train Accuracy: {train_accuracy},\\n Test Loss: {mean_test_loss}, Test Accuracy: {test_accuracy}\")\n",
    "\n",
    "        # Append losses to the lists for visualization\n",
    "        train_losses.append(mean_train_loss)\n",
    "        test_losses.append(mean_test_loss)\n",
    "\n",
    "    return train_losses, test_losses, train_accuracies, test_accuracies\n",
    "\n",
    "# Example usage\n",
    "EPOCHS = 10\n",
    "model = MLPModel(layer_sizes=[256, 256], output_size=10)\n",
    "\n",
    "# Loss function and optimizer\n",
    "cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "sgd = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)\n",
    "\n",
    "# Assuming train_dataset and test_dataset are your TensorFlow datasets\n",
    "train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    EPOCHS, model, train_dataset, test_dataset, cce, sgd\n",
    ")\n",
    "\n",
    "\n",
    "# 2. 5 Visualization\n",
    "def visualization(train_losses , train_accuracies , test_losses , test_accuracies):\n",
    "    \"\"\"\n",
    "    Visualizes accuracy and loss for training and test data using the mean of each epoch.\n",
    "    Loss is displayed in a regular line, accuracy in a dotted line.\n",
    "    Training data is displayed in blue, test data in red. Parameters\n",
    "    ----------\n",
    "    train_losses : numpy.ndarray\n",
    "    training losses train_accuracies : numpy.ndarray\n",
    "    training accuracies test_losses : numpy.ndarray\n",
    "    test losses\n",
    "    test_accuracies : numpy.ndarray\n",
    "    test accuracies\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    line1, = plt.plot(train_losses, \"b-\")\n",
    "    line2, = plt.plot(test_losses, \"r-\")\n",
    "    line3, = plt.plot(train_accuracies, \"b:\")\n",
    "    line4, = plt.plot(test_accuracies, \"r:\")\n",
    "    plt.xlabel(\"Training steps\")\n",
    "    plt.ylabel(\"Loss/Accuracy\")\n",
    "    plt.legend((line1, line2, line3, line4), (\"training loss\", \"test loss\", \"train accuracy\", \"test accuracy\"))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "visualization(train_losses , train_accuracies , test_losses , test_accuracies)\n",
    "\n",
    "# hyperparameters\n",
    "learning_rates = [0.01, 0.1, 0.5]\n",
    "momentums = [0.5, 0.9]\n",
    "batch_sizes = [32, 64, 128, 256]\n",
    "layer_configurations = [[256, 256], [128, 128], [512, 256, 128]]\n",
    "optimizers_list = [optimizers.SGD, optimizers.Adam, optimizers.RMSprop]\n",
    "\n",
    "# Record results\n",
    "experiment_results = []\n",
    "\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for momentum in momentums:\n",
    "        for batch_size in batch_sizes:\n",
    "            for layer_sizes in layer_configurations:\n",
    "                for opt in optimizers_list:\n",
    "                    print(f\"Running experiment with LR: {lr}, Momentum: {momentum}, Batch Size: {batch_size}, Layer Sizes: {layer_sizes}, Optimizer: {opt.__name__}\")\n",
    "\n",
    "                    train_dataset = data_pipeline(train_ds, batch_size)\n",
    "                    test_dataset = data_pipeline(test_ds, batch_size)\n",
    "\n",
    "                    model = MLPModel(layer_sizes)\n",
    "\n",
    "                    if opt == optimizers.SGD:\n",
    "                        optimizer = opt(learning_rate=lr, momentum=momentum)\n",
    "                    else:\n",
    "                        optimizer = opt(learning_rate=lr)\n",
    "\n",
    "                    cce = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "                    train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "                        EPOCHS, model, train_dataset, test_dataset, cce, optimizer\n",
    "                    )\n",
    "\n",
    "                    experiment_results.append({\n",
    "                        'lr': lr,\n",
    "                        'momentum': momentum,\n",
    "                        'batch_size': batch_size,\n",
    "                        'layer_sizes': layer_sizes,\n",
    "                        'optimizer': opt.__name__,\n",
    "                        'train_losses': train_losses,\n",
    "                        'test_losses': test_losses,\n",
    "                        'train_accuracies': train_accuracies,\n",
    "                        'test_accuracies': test_accuracies\n",
    "                    })\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "\n",
    "def plot_perhyper(hyperparam, method):\n",
    "    unique_vals = results_df[hyperparam].unique()\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    for i in unique_vals:\n",
    "        subset = results_df[results_df[hyperparam] == i] \n",
    "        mean_metric = subset.groupby('epoch')[method].mean()  \n",
    "        plt.plot(mean_metric, label=f\"{hyperparam}:{i}\")  \n",
    "        \n",
    "    plt.ylabel(method)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.title('Effect of Adjusting the hyperparameters of our model with different hyperparameters and methods')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "plot_perhyper('lr','test_accuracies')\n",
    "methods = ['train_losses', 'test_losses', 'train_accuracies', 'test_accuracies']\n",
    "for i in methods:\n",
    "    plot_perhyper('lr', i)\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(experiment_results)\n",
    "print(\"Summary Statistics:\")\n",
    "print(results_df[['train_losses', 'test_losses', 'train_accuracies', 'test_accuracies']].describe())\n",
    "# Best accuracy\n",
    "best_accuracy = results_df.sort_values(by='test_accuracies', ascending=False).head(1)\n",
    "print(\"\\nBest Accuracy Configuration:\")\n",
    "print(best_accuracy)\n",
    "\n",
    "# Lowest loss\n",
    "lowest_loss = results_df.sort_values(by='test_losses').head(1)\n",
    "print(\"\\nLowest Loss Configuration:\")\n",
    "print(lowest_loss)\n",
    "# Plotting Test Losses for different configurations\n",
    "plt.figure(figsize=(12, 6))\n",
    "for index, row in results_df.iterrows():\n",
    "    label = f\"LR: {row['lr']}, Momentum: {row['momentum']}, Batch: {row['batch_size']}, Layer Sizes: {row['layer_sizes']}\"\n",
    "    plt.plot(row['test_losses'], label=label)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Test Loss')\n",
    "plt.title('Test Loss per Epoch for Different Configurations')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Similarly, you can plot for accuracies\n",
    "correlation_matrix = results_df[['lr', 'momentum', 'batch_size', 'train_losses', 'test_losses', 'train_accuracies', 'test_accuracies']].corr()\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
